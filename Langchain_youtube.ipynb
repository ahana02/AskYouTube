{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN37sQfYHScXyJdIX98+KZ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahana02/AskYouTube/blob/main/Langchain_youtube.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Use GPU runtime"
      ],
      "metadata": {
        "id": "MtKoKnp63RqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyzEXeRkYGIb",
        "outputId": "02600596-786e-4d19-cb7e-1fc1e8a98ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m51.2/57.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/whisper.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEuh12CQYcyi",
        "outputId": "8750d9ce-5547-4693-dea4-a6996da3600e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import pytube"
      ],
      "metadata": {
        "id": "iqQXWexQYgBx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the taken Youtube link\n",
        "url = \"https://www.youtube.com/watch?v=QDX-1M5Nj7s\"\n",
        "video = pytube.YouTube(url,use_oauth=False,\n",
        "        allow_oauth_cache=True)\n",
        "video.streams.get_highest_resolution().filesize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FUhWKz1Y0HB",
        "outputId": "a914ff91-7a98-48ba-b0d4-72e8572fbe18"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93449491"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting and downloading as 'MP4' file\n",
        "audio = video.streams.get_audio_only()\n",
        "fn = audio.download(output_path=\"tmp.mp3\")"
      ],
      "metadata": {
        "id": "CAQzzx3VY6Mp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the 'base' model of Whisper\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "q-fKBf3PaG4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8c4328d-9128-4641-c5fa-5ef91ef382a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:02<00:00, 63.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = model.transcribe(\"/content/tmp.mp3/MIT Introduction to Deep Learning  6S191.mp4\")"
      ],
      "metadata": {
        "id": "YkKvgw6eZ5HW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = transcription['segments']\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_jyeEqJaRiI",
        "outputId": "e70a1e4d-efba-4f47-bf5d-40099aacd136"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 0, 'seek': 0, 'start': 0.0, 'end': 14.64, 'text': ' Good afternoon, everyone. Thank you all for joining today. My name is Alexander Amini,', 'tokens': [50364, 2205, 6499, 11, 1518, 13, 1044, 291, 439, 337, 5549, 965, 13, 1222, 1315, 307, 14845, 2012, 3812, 11, 51096], 'temperature': 0.0, 'avg_logprob': -0.2659889050384066, 'compression_ratio': 1.4126984126984128, 'no_speech_prob': 0.10993754118680954}, {'id': 1, 'seek': 0, 'start': 14.64, 'end': 19.88, 'text': \" and I'll be one of your course organizers this year, along with Ava. And together, we're\", 'tokens': [51096, 293, 286, 603, 312, 472, 295, 428, 1164, 35071, 341, 1064, 11, 2051, 365, 316, 2757, 13, 400, 1214, 11, 321, 434, 51358], 'temperature': 0.0, 'avg_logprob': -0.2659889050384066, 'compression_ratio': 1.4126984126984128, 'no_speech_prob': 0.10993754118680954}, {'id': 2, 'seek': 0, 'start': 19.88, 'end': 26.16, 'text': ' super excited to introduce you all to introduction to deep learning. Now, MIT Inter to Deep', 'tokens': [51358, 1687, 2919, 281, 5366, 291, 439, 281, 9339, 281, 2452, 2539, 13, 823, 11, 13100, 5751, 281, 14895, 51672], 'temperature': 0.0, 'avg_logprob': -0.2659889050384066, 'compression_ratio': 1.4126984126984128, 'no_speech_prob': 0.10993754118680954}, {'id': 3, 'seek': 2616, 'start': 26.16, 'end': 32.0, 'text': ' Learning is a really, really fun, exciting, and fast-paced program here at MIT. And let', 'tokens': [50364, 15205, 307, 257, 534, 11, 534, 1019, 11, 4670, 11, 293, 2370, 12, 47038, 1461, 510, 412, 13100, 13, 400, 718, 50656], 'temperature': 0.0, 'avg_logprob': -0.13181934533295808, 'compression_ratio': 1.6804511278195489, 'no_speech_prob': 0.008171971887350082}, {'id': 4, 'seek': 2616, 'start': 32.0, 'end': 36.4, 'text': ' me start by just, first of all, giving you a little bit of background into what we do', 'tokens': [50656, 385, 722, 538, 445, 11, 700, 295, 439, 11, 2902, 291, 257, 707, 857, 295, 3678, 666, 437, 321, 360, 50876], 'temperature': 0.0, 'avg_logprob': -0.13181934533295808, 'compression_ratio': 1.6804511278195489, 'no_speech_prob': 0.008171971887350082}, {'id': 5, 'seek': 2616, 'start': 36.4, 'end': 41.6, 'text': \" and what you're going to learn about this year. So this week of Intro to Deep Learning,\", 'tokens': [50876, 293, 437, 291, 434, 516, 281, 1466, 466, 341, 1064, 13, 407, 341, 1243, 295, 47406, 281, 14895, 15205, 11, 51136], 'temperature': 0.0, 'avg_logprob': -0.13181934533295808, 'compression_ratio': 1.6804511278195489, 'no_speech_prob': 0.008171971887350082}, {'id': 6, 'seek': 2616, 'start': 41.6, 'end': 45.879999999999995, 'text': \" we're going to cover a ton of material in just one week. You'll learn the foundations\", 'tokens': [51136, 321, 434, 516, 281, 2060, 257, 2952, 295, 2527, 294, 445, 472, 1243, 13, 509, 603, 1466, 264, 22467, 51350], 'temperature': 0.0, 'avg_logprob': -0.13181934533295808, 'compression_ratio': 1.6804511278195489, 'no_speech_prob': 0.008171971887350082}, {'id': 7, 'seek': 2616, 'start': 45.879999999999995, 'end': 52.28, 'text': ' of this really, really fascinating and exciting field of deep learning and artificial intelligence.', 'tokens': [51350, 295, 341, 534, 11, 534, 10343, 293, 4670, 2519, 295, 2452, 2539, 293, 11677, 7599, 13, 51670], 'temperature': 0.0, 'avg_logprob': -0.13181934533295808, 'compression_ratio': 1.6804511278195489, 'no_speech_prob': 0.008171971887350082}, {'id': 8, 'seek': 5228, 'start': 52.28, 'end': 57.0, 'text': \" And more importantly, you're going to get hands-on experience actually reinforcing what\", 'tokens': [50364, 400, 544, 8906, 11, 291, 434, 516, 281, 483, 2377, 12, 266, 1752, 767, 48262, 437, 50600], 'temperature': 0.0, 'avg_logprob': -0.1480939501807803, 'compression_ratio': 1.7312252964426877, 'no_speech_prob': 0.0015963433543220162}, {'id': 9, 'seek': 5228, 'start': 57.0, 'end': 63.08, 'text': ' you learn in the lectures as part of hands-on software labs. Now, over the past decade,', 'tokens': [50600, 291, 1466, 294, 264, 16564, 382, 644, 295, 2377, 12, 266, 4722, 20339, 13, 823, 11, 670, 264, 1791, 10378, 11, 50904], 'temperature': 0.0, 'avg_logprob': -0.1480939501807803, 'compression_ratio': 1.7312252964426877, 'no_speech_prob': 0.0015963433543220162}, {'id': 10, 'seek': 5228, 'start': 63.08, 'end': 68.8, 'text': ' AI and deep learning have really had a huge resurgence and had incredible successes. And', 'tokens': [50904, 7318, 293, 2452, 2539, 362, 534, 632, 257, 2603, 725, 44607, 293, 632, 4651, 26101, 13, 400, 51190], 'temperature': 0.0, 'avg_logprob': -0.1480939501807803, 'compression_ratio': 1.7312252964426877, 'no_speech_prob': 0.0015963433543220162}, {'id': 11, 'seek': 5228, 'start': 68.8, 'end': 73.56, 'text': \" a lot of problems that even just a decade ago, we thought we're not really even solvable\", 'tokens': [51190, 257, 688, 295, 2740, 300, 754, 445, 257, 10378, 2057, 11, 321, 1194, 321, 434, 406, 534, 754, 1404, 17915, 51428], 'temperature': 0.0, 'avg_logprob': -0.1480939501807803, 'compression_ratio': 1.7312252964426877, 'no_speech_prob': 0.0015963433543220162}, {'id': 12, 'seek': 5228, 'start': 73.56, 'end': 78.84, 'text': \" in the near future. Now, we're solving with deep learning with incredible ease. Now,\", 'tokens': [51428, 294, 264, 2651, 2027, 13, 823, 11, 321, 434, 12606, 365, 2452, 2539, 365, 4651, 12708, 13, 823, 11, 51692], 'temperature': 0.0, 'avg_logprob': -0.1480939501807803, 'compression_ratio': 1.7312252964426877, 'no_speech_prob': 0.0015963433543220162}, {'id': 13, 'seek': 7884, 'start': 78.84, 'end': 84.52000000000001, 'text': ' this past year in particular of 2022 has been an incredible year for deep learning progress.', 'tokens': [50364, 341, 1791, 1064, 294, 1729, 295, 20229, 575, 668, 364, 4651, 1064, 337, 2452, 2539, 4205, 13, 50648], 'temperature': 0.0, 'avg_logprob': -0.10867924265342184, 'compression_ratio': 1.860082304526749, 'no_speech_prob': 0.0283696036785841}, {'id': 14, 'seek': 7884, 'start': 84.52000000000001, 'end': 88.92, 'text': \" And I'd like to say that actually this past year in particular has been the year of\", 'tokens': [50648, 400, 286, 1116, 411, 281, 584, 300, 767, 341, 1791, 1064, 294, 1729, 575, 668, 264, 1064, 295, 50868], 'temperature': 0.0, 'avg_logprob': -0.10867924265342184, 'compression_ratio': 1.860082304526749, 'no_speech_prob': 0.0283696036785841}, {'id': 15, 'seek': 7884, 'start': 88.92, 'end': 93.80000000000001, 'text': ' generative deep learning, using deep learning to generate brand new types of data that have', 'tokens': [50868, 1337, 1166, 2452, 2539, 11, 1228, 2452, 2539, 281, 8460, 3360, 777, 3467, 295, 1412, 300, 362, 51112], 'temperature': 0.0, 'avg_logprob': -0.10867924265342184, 'compression_ratio': 1.860082304526749, 'no_speech_prob': 0.0283696036785841}, {'id': 16, 'seek': 7884, 'start': 93.80000000000001, 'end': 99.72, 'text': ' never been seen before and never existed in reality. In fact, I want to start this class by', 'tokens': [51112, 1128, 668, 1612, 949, 293, 1128, 13135, 294, 4103, 13, 682, 1186, 11, 286, 528, 281, 722, 341, 1508, 538, 51408], 'temperature': 0.0, 'avg_logprob': -0.10867924265342184, 'compression_ratio': 1.860082304526749, 'no_speech_prob': 0.0283696036785841}, {'id': 17, 'seek': 7884, 'start': 99.72, 'end': 104.36, 'text': ' actually showing you how we started this class several years ago, which was by playing this', 'tokens': [51408, 767, 4099, 291, 577, 321, 1409, 341, 1508, 2940, 924, 2057, 11, 597, 390, 538, 2433, 341, 51640], 'temperature': 0.0, 'avg_logprob': -0.10867924265342184, 'compression_ratio': 1.860082304526749, 'no_speech_prob': 0.0283696036785841}, {'id': 18, 'seek': 10436, 'start': 104.36, 'end': 110.44, 'text': \" video that I'll play in a second. Now, this video actually was an introductory video for the class.\", 'tokens': [50364, 960, 300, 286, 603, 862, 294, 257, 1150, 13, 823, 11, 341, 960, 767, 390, 364, 39048, 960, 337, 264, 1508, 13, 50668], 'temperature': 0.0, 'avg_logprob': -0.24182507763170216, 'compression_ratio': 1.4148936170212767, 'no_speech_prob': 0.0027540349401533604}, {'id': 19, 'seek': 10436, 'start': 110.44, 'end': 116.6, 'text': \" It was kind of exemplifies this idea that I'm talking about. So let me just stop there and play\", 'tokens': [50668, 467, 390, 733, 295, 24112, 11221, 341, 1558, 300, 286, 478, 1417, 466, 13, 407, 718, 385, 445, 1590, 456, 293, 862, 50976], 'temperature': 0.0, 'avg_logprob': -0.24182507763170216, 'compression_ratio': 1.4148936170212767, 'no_speech_prob': 0.0027540349401533604}, {'id': 20, 'seek': 10436, 'start': 116.6, 'end': 127.16, 'text': ' this video first of all. Hi, everybody. And welcome to MIT 6S-191. The', 'tokens': [50976, 341, 960, 700, 295, 439, 13, 2421, 11, 2201, 13, 400, 2928, 281, 13100, 1386, 50, 12, 3405, 16, 13, 440, 51504], 'temperature': 0.0, 'avg_logprob': -0.24182507763170216, 'compression_ratio': 1.4148936170212767, 'no_speech_prob': 0.0027540349401533604}, {'id': 21, 'seek': 12716, 'start': 127.16, 'end': 132.6, 'text': ' official introductory course on deep learning is called here at MIT.', 'tokens': [50364, 4783, 39048, 1164, 322, 2452, 2539, 307, 1219, 510, 412, 13100, 13, 50636], 'temperature': 0.0, 'avg_logprob': -0.4681854248046875, 'compression_ratio': 1.5058823529411764, 'no_speech_prob': 0.4720129668712616}, {'id': 22, 'seek': 12716, 'start': 133.96, 'end': 142.04, 'text': ' Deep learning is revolutionizing so many things from the bodies, the medicine, and everything in', 'tokens': [50704, 14895, 2539, 307, 8894, 3319, 370, 867, 721, 490, 264, 7510, 11, 264, 7195, 11, 293, 1203, 294, 51108], 'temperature': 0.0, 'avg_logprob': -0.4681854248046875, 'compression_ratio': 1.5058823529411764, 'no_speech_prob': 0.4720129668712616}, {'id': 23, 'seek': 12716, 'start': 142.04, 'end': 150.04, 'text': \" the field. You'll learn the fundamentals of this field and how you can build some of these\", 'tokens': [51108, 264, 2519, 13, 509, 603, 1466, 264, 29505, 295, 341, 2519, 293, 577, 291, 393, 1322, 512, 295, 613, 51508], 'temperature': 0.0, 'avg_logprob': -0.4681854248046875, 'compression_ratio': 1.5058823529411764, 'no_speech_prob': 0.4720129668712616}, {'id': 24, 'seek': 15004, 'start': 150.04, 'end': 159.32, 'text': \" incredible alternatives. In fact, this is where our students and video are not real and we're\", 'tokens': [50364, 4651, 20478, 13, 682, 1186, 11, 341, 307, 689, 527, 1731, 293, 960, 366, 406, 957, 293, 321, 434, 50828], 'temperature': 0.0, 'avg_logprob': -0.4238045801881884, 'compression_ratio': 1.4913294797687862, 'no_speech_prob': 0.005983508657664061}, {'id': 25, 'seek': 15004, 'start': 159.32, 'end': 167.79999999999998, 'text': \" creating deep learning and our visual intelligence. And in this class, you'll learn how\", 'tokens': [50828, 4084, 2452, 2539, 293, 527, 5056, 7599, 13, 400, 294, 341, 1508, 11, 291, 603, 1466, 577, 51252], 'temperature': 0.0, 'avg_logprob': -0.4238045801881884, 'compression_ratio': 1.4913294797687862, 'no_speech_prob': 0.005983508657664061}, {'id': 26, 'seek': 15004, 'start': 168.84, 'end': 173.56, 'text': ' and have been honored to be with me today and I hope you enjoyed the course.', 'tokens': [51304, 293, 362, 668, 14556, 281, 312, 365, 385, 965, 293, 286, 1454, 291, 4626, 264, 1164, 13, 51540], 'temperature': 0.0, 'avg_logprob': -0.4238045801881884, 'compression_ratio': 1.4913294797687862, 'no_speech_prob': 0.005983508657664061}, {'id': 27, 'seek': 17356, 'start': 173.88, 'end': 183.32, 'text': \" So in case you couldn't tell, this video and its entire audio was actually not real. It was\", 'tokens': [50380, 407, 294, 1389, 291, 2809, 380, 980, 11, 341, 960, 293, 1080, 2302, 6278, 390, 767, 406, 957, 13, 467, 390, 50852], 'temperature': 0.0, 'avg_logprob': -0.15093616909450955, 'compression_ratio': 1.7256637168141593, 'no_speech_prob': 0.015609080903232098}, {'id': 28, 'seek': 17356, 'start': 183.32, 'end': 188.76, 'text': ' synthetically generated by a deep learning algorithm. And when we introduced this class a few years', 'tokens': [50852, 10657, 22652, 10833, 538, 257, 2452, 2539, 9284, 13, 400, 562, 321, 7268, 341, 1508, 257, 1326, 924, 51124], 'temperature': 0.0, 'avg_logprob': -0.15093616909450955, 'compression_ratio': 1.7256637168141593, 'no_speech_prob': 0.015609080903232098}, {'id': 29, 'seek': 17356, 'start': 188.76, 'end': 194.92000000000002, 'text': ' ago, this video was created several years ago. But even several years ago, when we introduced this', 'tokens': [51124, 2057, 11, 341, 960, 390, 2942, 2940, 924, 2057, 13, 583, 754, 2940, 924, 2057, 11, 562, 321, 7268, 341, 51432], 'temperature': 0.0, 'avg_logprob': -0.15093616909450955, 'compression_ratio': 1.7256637168141593, 'no_speech_prob': 0.015609080903232098}, {'id': 30, 'seek': 17356, 'start': 194.92000000000002, 'end': 200.04, 'text': ' and put it on YouTube, when some were viral, people really loved this video. They were intrigued by', 'tokens': [51432, 293, 829, 309, 322, 3088, 11, 562, 512, 645, 16132, 11, 561, 534, 4333, 341, 960, 13, 814, 645, 35140, 538, 51688], 'temperature': 0.0, 'avg_logprob': -0.15093616909450955, 'compression_ratio': 1.7256637168141593, 'no_speech_prob': 0.015609080903232098}, {'id': 31, 'seek': 20004, 'start': 200.12, 'end': 207.4, 'text': ' how real the video and audio felt and looked entirely generated by an algorithm, by a computer.', 'tokens': [50368, 577, 957, 264, 960, 293, 6278, 2762, 293, 2956, 7696, 10833, 538, 364, 9284, 11, 538, 257, 3820, 13, 50732], 'temperature': 0.0, 'avg_logprob': -0.12395307990942109, 'compression_ratio': 1.6607929515418502, 'no_speech_prob': 0.00033518372219987214}, {'id': 32, 'seek': 20004, 'start': 208.04, 'end': 212.6, 'text': ' And people were shocked with the power and the realism of these types of approaches. And this', 'tokens': [50764, 400, 561, 645, 12763, 365, 264, 1347, 293, 264, 38484, 295, 613, 3467, 295, 11587, 13, 400, 341, 50992], 'temperature': 0.0, 'avg_logprob': -0.12395307990942109, 'compression_ratio': 1.6607929515418502, 'no_speech_prob': 0.00033518372219987214}, {'id': 33, 'seek': 20004, 'start': 212.6, 'end': 218.84, 'text': ' was a few years ago. Now, fast forward to today and the state of deep learning today, we have', 'tokens': [50992, 390, 257, 1326, 924, 2057, 13, 823, 11, 2370, 2128, 281, 965, 293, 264, 1785, 295, 2452, 2539, 965, 11, 321, 362, 51304], 'temperature': 0.0, 'avg_logprob': -0.12395307990942109, 'compression_ratio': 1.6607929515418502, 'no_speech_prob': 0.00033518372219987214}, {'id': 34, 'seek': 20004, 'start': 220.51999999999998, 'end': 226.92, 'text': \" seen deep learning accelerating at a rate faster than we've ever seen before. In fact, we can\", 'tokens': [51388, 1612, 2452, 2539, 34391, 412, 257, 3314, 4663, 813, 321, 600, 1562, 1612, 949, 13, 682, 1186, 11, 321, 393, 51708], 'temperature': 0.0, 'avg_logprob': -0.12395307990942109, 'compression_ratio': 1.6607929515418502, 'no_speech_prob': 0.00033518372219987214}, {'id': 35, 'seek': 22692, 'start': 226.92, 'end': 233.79999999999998, 'text': ' use deep learning now to generate not just images of faces, but generate full synthetic environments', 'tokens': [50364, 764, 2452, 2539, 586, 281, 8460, 406, 445, 5267, 295, 8475, 11, 457, 8460, 1577, 23420, 12388, 50708], 'temperature': 0.0, 'avg_logprob': -0.08891028224831761, 'compression_ratio': 1.7275985663082438, 'no_speech_prob': 0.0011495796497911215}, {'id': 36, 'seek': 22692, 'start': 233.79999999999998, 'end': 239.39999999999998, 'text': ' where we can train autonomous vehicles entirely in simulation and deploy them on full-scale vehicles', 'tokens': [50708, 689, 321, 393, 3847, 23797, 8948, 7696, 294, 16575, 293, 7274, 552, 322, 1577, 12, 20033, 8948, 50988], 'temperature': 0.0, 'avg_logprob': -0.08891028224831761, 'compression_ratio': 1.7275985663082438, 'no_speech_prob': 0.0011495796497911215}, {'id': 37, 'seek': 22692, 'start': 239.39999999999998, 'end': 244.35999999999999, 'text': ' in the real world seamlessly. The videos here you see are actually from a data-driven simulator', 'tokens': [50988, 294, 264, 957, 1002, 38083, 13, 440, 2145, 510, 291, 536, 366, 767, 490, 257, 1412, 12, 25456, 32974, 51236], 'temperature': 0.0, 'avg_logprob': -0.08891028224831761, 'compression_ratio': 1.7275985663082438, 'no_speech_prob': 0.0011495796497911215}, {'id': 38, 'seek': 22692, 'start': 244.35999999999999, 'end': 250.2, 'text': ' from neural networks generated called Vista that we actually built here at MIT and have open', 'tokens': [51236, 490, 18161, 9590, 10833, 1219, 691, 5236, 300, 321, 767, 3094, 510, 412, 13100, 293, 362, 1269, 51528], 'temperature': 0.0, 'avg_logprob': -0.08891028224831761, 'compression_ratio': 1.7275985663082438, 'no_speech_prob': 0.0011495796497911215}, {'id': 39, 'seek': 22692, 'start': 250.2, 'end': 254.67999999999998, 'text': ' source to the public. So all of you can actually train and build the future of autonomy and', 'tokens': [51528, 4009, 281, 264, 1908, 13, 407, 439, 295, 291, 393, 767, 3847, 293, 1322, 264, 2027, 295, 27278, 293, 51752], 'temperature': 0.0, 'avg_logprob': -0.08891028224831761, 'compression_ratio': 1.7275985663082438, 'no_speech_prob': 0.0011495796497911215}, {'id': 40, 'seek': 25468, 'start': 254.68, 'end': 259.8, 'text': ' self-driving cars. And of course, it goes far beyond this as well. Deep learning can be used to', 'tokens': [50364, 2698, 12, 47094, 5163, 13, 400, 295, 1164, 11, 309, 1709, 1400, 4399, 341, 382, 731, 13, 14895, 2539, 393, 312, 1143, 281, 50620], 'temperature': 0.0, 'avg_logprob': -0.06202629710851091, 'compression_ratio': 1.6883116883116882, 'no_speech_prob': 0.0007663086289539933}, {'id': 41, 'seek': 25468, 'start': 259.8, 'end': 266.2, 'text': ' generate content directly from how we speak and the language that we convey to it from prompts', 'tokens': [50620, 8460, 2701, 3838, 490, 577, 321, 1710, 293, 264, 2856, 300, 321, 16965, 281, 309, 490, 41095, 50940], 'temperature': 0.0, 'avg_logprob': -0.06202629710851091, 'compression_ratio': 1.6883116883116882, 'no_speech_prob': 0.0007663086289539933}, {'id': 42, 'seek': 25468, 'start': 266.2, 'end': 272.2, 'text': ' that we say. Deep learning can reason about the prompts in natural language and English, for example,', 'tokens': [50940, 300, 321, 584, 13, 14895, 2539, 393, 1778, 466, 264, 41095, 294, 3303, 2856, 293, 3669, 11, 337, 1365, 11, 51240], 'temperature': 0.0, 'avg_logprob': -0.06202629710851091, 'compression_ratio': 1.6883116883116882, 'no_speech_prob': 0.0007663086289539933}, {'id': 43, 'seek': 25468, 'start': 272.2, 'end': 279.56, 'text': \" and then guide and control what is generated according to what we specify. We've seen examples of\", 'tokens': [51240, 293, 550, 5934, 293, 1969, 437, 307, 10833, 4650, 281, 437, 321, 16500, 13, 492, 600, 1612, 5110, 295, 51608], 'temperature': 0.0, 'avg_logprob': -0.06202629710851091, 'compression_ratio': 1.6883116883116882, 'no_speech_prob': 0.0007663086289539933}, {'id': 44, 'seek': 27956, 'start': 279.64, 'end': 285.16, 'text': ' where we can generate, for example, things that, again, have never existed in reality. We can ask a', 'tokens': [50368, 689, 321, 393, 8460, 11, 337, 1365, 11, 721, 300, 11, 797, 11, 362, 1128, 13135, 294, 4103, 13, 492, 393, 1029, 257, 50644], 'temperature': 0.0, 'avg_logprob': -0.10940542867628195, 'compression_ratio': 1.8198529411764706, 'no_speech_prob': 0.009385990910232067}, {'id': 45, 'seek': 27956, 'start': 285.16, 'end': 291.72, 'text': ' neural network to generate a photo of an astronaut riding a horse. And it actually can imagine,', 'tokens': [50644, 18161, 3209, 281, 8460, 257, 5052, 295, 364, 18516, 9546, 257, 6832, 13, 400, 309, 767, 393, 3811, 11, 50972], 'temperature': 0.0, 'avg_logprob': -0.10940542867628195, 'compression_ratio': 1.8198529411764706, 'no_speech_prob': 0.009385990910232067}, {'id': 46, 'seek': 27956, 'start': 291.72, 'end': 297.32, 'text': ' hallucinate what this might look like, even though, of course, this photo, not only this photo has', 'tokens': [50972, 35212, 13923, 437, 341, 1062, 574, 411, 11, 754, 1673, 11, 295, 1164, 11, 341, 5052, 11, 406, 787, 341, 5052, 575, 51252], 'temperature': 0.0, 'avg_logprob': -0.10940542867628195, 'compression_ratio': 1.8198529411764706, 'no_speech_prob': 0.009385990910232067}, {'id': 47, 'seek': 27956, 'start': 297.32, 'end': 301.48, 'text': \" never occurred before, but I don't think any photo of an astronaut riding a horse has ever occurred\", 'tokens': [51252, 1128, 11068, 949, 11, 457, 286, 500, 380, 519, 604, 5052, 295, 364, 18516, 9546, 257, 6832, 575, 1562, 11068, 51460], 'temperature': 0.0, 'avg_logprob': -0.10940542867628195, 'compression_ratio': 1.8198529411764706, 'no_speech_prob': 0.009385990910232067}, {'id': 48, 'seek': 27956, 'start': 301.48, 'end': 306.2, 'text': \" before. So there's not really even training data that you could go off in this case. And my personal\", 'tokens': [51460, 949, 13, 407, 456, 311, 406, 534, 754, 3097, 1412, 300, 291, 727, 352, 766, 294, 341, 1389, 13, 400, 452, 2973, 51696], 'temperature': 0.0, 'avg_logprob': -0.10940542867628195, 'compression_ratio': 1.8198529411764706, 'no_speech_prob': 0.009385990910232067}, {'id': 49, 'seek': 30620, 'start': 306.2, 'end': 311.0, 'text': ' favorite is actually how we can not only build software that can generate images and videos,', 'tokens': [50364, 2954, 307, 767, 577, 321, 393, 406, 787, 1322, 4722, 300, 393, 8460, 5267, 293, 2145, 11, 50604], 'temperature': 0.0, 'avg_logprob': -0.10203559443635761, 'compression_ratio': 1.864864864864865, 'no_speech_prob': 0.002356697339564562}, {'id': 50, 'seek': 30620, 'start': 311.0, 'end': 316.92, 'text': ' but build software that can generate software as well. We can also have algorithms that can take', 'tokens': [50604, 457, 1322, 4722, 300, 393, 8460, 4722, 382, 731, 13, 492, 393, 611, 362, 14642, 300, 393, 747, 50900], 'temperature': 0.0, 'avg_logprob': -0.10203559443635761, 'compression_ratio': 1.864864864864865, 'no_speech_prob': 0.002356697339564562}, {'id': 51, 'seek': 30620, 'start': 316.92, 'end': 323.88, 'text': ' language prompts, for example, a prompt like this, write code in TensorFlow to train a neural network.', 'tokens': [50900, 2856, 41095, 11, 337, 1365, 11, 257, 12391, 411, 341, 11, 2464, 3089, 294, 37624, 281, 3847, 257, 18161, 3209, 13, 51248], 'temperature': 0.0, 'avg_logprob': -0.10203559443635761, 'compression_ratio': 1.864864864864865, 'no_speech_prob': 0.002356697339564562}, {'id': 52, 'seek': 30620, 'start': 323.88, 'end': 330.03999999999996, 'text': ' And not only will it write the code and create that neural network, but it will have the ability', 'tokens': [51248, 400, 406, 787, 486, 309, 2464, 264, 3089, 293, 1884, 300, 18161, 3209, 11, 457, 309, 486, 362, 264, 3485, 51556], 'temperature': 0.0, 'avg_logprob': -0.10203559443635761, 'compression_ratio': 1.864864864864865, 'no_speech_prob': 0.002356697339564562}, {'id': 53, 'seek': 30620, 'start': 330.03999999999996, 'end': 334.59999999999997, 'text': \" to reason about the code that it's generated and walk you through step by step explaining the\", 'tokens': [51556, 281, 1778, 466, 264, 3089, 300, 309, 311, 10833, 293, 1792, 291, 807, 1823, 538, 1823, 13468, 264, 51784], 'temperature': 0.0, 'avg_logprob': -0.10203559443635761, 'compression_ratio': 1.864864864864865, 'no_speech_prob': 0.002356697339564562}, {'id': 54, 'seek': 33460, 'start': 334.6, 'end': 339.32000000000005, 'text': ' process and procedure all the way from the ground up to you so that you can actually learn how to', 'tokens': [50364, 1399, 293, 10747, 439, 264, 636, 490, 264, 2727, 493, 281, 291, 370, 300, 291, 393, 767, 1466, 577, 281, 50600], 'temperature': 0.0, 'avg_logprob': -0.06095991222136611, 'compression_ratio': 1.7107142857142856, 'no_speech_prob': 0.0021811879705637693}, {'id': 55, 'seek': 33460, 'start': 339.32000000000005, 'end': 345.96000000000004, 'text': ' do this process as well. Now, I think some of these examples really just highlight how far', 'tokens': [50600, 360, 341, 1399, 382, 731, 13, 823, 11, 286, 519, 512, 295, 613, 5110, 534, 445, 5078, 577, 1400, 50932], 'temperature': 0.0, 'avg_logprob': -0.06095991222136611, 'compression_ratio': 1.7107142857142856, 'no_speech_prob': 0.0021811879705637693}, {'id': 56, 'seek': 33460, 'start': 345.96000000000004, 'end': 350.44, 'text': ' deep learning and these methods have come in the past six years since we started this course. And', 'tokens': [50932, 2452, 2539, 293, 613, 7150, 362, 808, 294, 264, 1791, 2309, 924, 1670, 321, 1409, 341, 1164, 13, 400, 51156], 'temperature': 0.0, 'avg_logprob': -0.06095991222136611, 'compression_ratio': 1.7107142857142856, 'no_speech_prob': 0.0021811879705637693}, {'id': 57, 'seek': 33460, 'start': 350.44, 'end': 355.56, 'text': \" you saw that example just a few years ago from that introductory video. But now we're seeing such\", 'tokens': [51156, 291, 1866, 300, 1365, 445, 257, 1326, 924, 2057, 490, 300, 39048, 960, 13, 583, 586, 321, 434, 2577, 1270, 51412], 'temperature': 0.0, 'avg_logprob': -0.06095991222136611, 'compression_ratio': 1.7107142857142856, 'no_speech_prob': 0.0021811879705637693}, {'id': 58, 'seek': 33460, 'start': 355.56, 'end': 360.68, 'text': ' incredible advances. And the most amazing part of this course, in my opinion, is actually that', 'tokens': [51412, 4651, 25297, 13, 400, 264, 881, 2243, 644, 295, 341, 1164, 11, 294, 452, 4800, 11, 307, 767, 300, 51668], 'temperature': 0.0, 'avg_logprob': -0.06095991222136611, 'compression_ratio': 1.7107142857142856, 'no_speech_prob': 0.0021811879705637693}, {'id': 59, 'seek': 36068, 'start': 360.68, 'end': 365.48, 'text': \" within this one week, we're going to take you through from the ground up starting from today,\", 'tokens': [50364, 1951, 341, 472, 1243, 11, 321, 434, 516, 281, 747, 291, 807, 490, 264, 2727, 493, 2891, 490, 965, 11, 50604], 'temperature': 0.0, 'avg_logprob': -0.10254402160644531, 'compression_ratio': 1.6989247311827957, 'no_speech_prob': 0.0016460503684356809}, {'id': 60, 'seek': 36068, 'start': 365.48, 'end': 371.24, 'text': ' all of that foundational building blocks that will allow you to understand and make all of this', 'tokens': [50604, 439, 295, 300, 32195, 2390, 8474, 300, 486, 2089, 291, 281, 1223, 293, 652, 439, 295, 341, 50892], 'temperature': 0.0, 'avg_logprob': -0.10254402160644531, 'compression_ratio': 1.6989247311827957, 'no_speech_prob': 0.0016460503684356809}, {'id': 61, 'seek': 36068, 'start': 371.24, 'end': 377.56, 'text': \" amazing advances possible. So with that, hopefully now you're all super excited about what this\", 'tokens': [50892, 2243, 25297, 1944, 13, 407, 365, 300, 11, 4696, 586, 291, 434, 439, 1687, 2919, 466, 437, 341, 51208], 'temperature': 0.0, 'avg_logprob': -0.10254402160644531, 'compression_ratio': 1.6989247311827957, 'no_speech_prob': 0.0016460503684356809}, {'id': 62, 'seek': 36068, 'start': 377.56, 'end': 383.32, 'text': ' class will teach. And I want to basically now just start by taking a step back and introducing', 'tokens': [51208, 1508, 486, 2924, 13, 400, 286, 528, 281, 1936, 586, 445, 722, 538, 1940, 257, 1823, 646, 293, 15424, 51496], 'temperature': 0.0, 'avg_logprob': -0.10254402160644531, 'compression_ratio': 1.6989247311827957, 'no_speech_prob': 0.0016460503684356809}, {'id': 63, 'seek': 36068, 'start': 383.32, 'end': 387.8, 'text': \" some of these terminologies that I've kind of been throwing around so far with deep learning,\", 'tokens': [51496, 512, 295, 613, 10761, 6204, 300, 286, 600, 733, 295, 668, 10238, 926, 370, 1400, 365, 2452, 2539, 11, 51720], 'temperature': 0.0, 'avg_logprob': -0.10254402160644531, 'compression_ratio': 1.6989247311827957, 'no_speech_prob': 0.0016460503684356809}, {'id': 64, 'seek': 38780, 'start': 387.8, 'end': 392.6, 'text': ' artificial intelligence, what do these things actually mean? So first of all, I want to maybe', 'tokens': [50364, 11677, 7599, 11, 437, 360, 613, 721, 767, 914, 30, 407, 700, 295, 439, 11, 286, 528, 281, 1310, 50604], 'temperature': 0.0, 'avg_logprob': -0.07010384968348912, 'compression_ratio': 1.740909090909091, 'no_speech_prob': 0.0008291005506180227}, {'id': 65, 'seek': 38780, 'start': 392.6, 'end': 399.08, 'text': ' just take a second to speak a little bit about intelligence and what intelligence means at its', 'tokens': [50604, 445, 747, 257, 1150, 281, 1710, 257, 707, 857, 466, 7599, 293, 437, 7599, 1355, 412, 1080, 50928], 'temperature': 0.0, 'avg_logprob': -0.07010384968348912, 'compression_ratio': 1.740909090909091, 'no_speech_prob': 0.0008291005506180227}, {'id': 66, 'seek': 38780, 'start': 399.08, 'end': 405.48, 'text': ' core. So to me, intelligence is simply the ability to process information such that we can use it', 'tokens': [50928, 4965, 13, 407, 281, 385, 11, 7599, 307, 2935, 264, 3485, 281, 1399, 1589, 1270, 300, 321, 393, 764, 309, 51248], 'temperature': 0.0, 'avg_logprob': -0.07010384968348912, 'compression_ratio': 1.740909090909091, 'no_speech_prob': 0.0008291005506180227}, {'id': 67, 'seek': 38780, 'start': 405.48, 'end': 411.96000000000004, 'text': ' to inform some future decision or action that we take. Now, the field of artificial intelligence', 'tokens': [51248, 281, 1356, 512, 2027, 3537, 420, 3069, 300, 321, 747, 13, 823, 11, 264, 2519, 295, 11677, 7599, 51572], 'temperature': 0.0, 'avg_logprob': -0.07010384968348912, 'compression_ratio': 1.740909090909091, 'no_speech_prob': 0.0008291005506180227}, {'id': 68, 'seek': 41196, 'start': 411.96, 'end': 417.96, 'text': ' is simply the ability for us to build algorithms, artificial algorithms that can do exactly this,', 'tokens': [50364, 307, 2935, 264, 3485, 337, 505, 281, 1322, 14642, 11, 11677, 14642, 300, 393, 360, 2293, 341, 11, 50664], 'temperature': 0.0, 'avg_logprob': -0.09253543277956405, 'compression_ratio': 1.855513307984791, 'no_speech_prob': 0.0010156923672184348}, {'id': 69, 'seek': 41196, 'start': 417.96, 'end': 423.96, 'text': ' process information to inform some future decision. Now, machine learning is simply a subset of AI,', 'tokens': [50664, 1399, 1589, 281, 1356, 512, 2027, 3537, 13, 823, 11, 3479, 2539, 307, 2935, 257, 25993, 295, 7318, 11, 50964], 'temperature': 0.0, 'avg_logprob': -0.09253543277956405, 'compression_ratio': 1.855513307984791, 'no_speech_prob': 0.0010156923672184348}, {'id': 70, 'seek': 41196, 'start': 423.96, 'end': 431.64, 'text': ' which focuses specifically on how we can build a machine to or teach a machine how to do this from', 'tokens': [50964, 597, 16109, 4682, 322, 577, 321, 393, 1322, 257, 3479, 281, 420, 2924, 257, 3479, 577, 281, 360, 341, 490, 51348], 'temperature': 0.0, 'avg_logprob': -0.09253543277956405, 'compression_ratio': 1.855513307984791, 'no_speech_prob': 0.0010156923672184348}, {'id': 71, 'seek': 41196, 'start': 431.64, 'end': 437.64, 'text': ' some experiences or data, for example. Now, deep learning goes one step beyond this and is a', 'tokens': [51348, 512, 5235, 420, 1412, 11, 337, 1365, 13, 823, 11, 2452, 2539, 1709, 472, 1823, 4399, 341, 293, 307, 257, 51648], 'temperature': 0.0, 'avg_logprob': -0.09253543277956405, 'compression_ratio': 1.855513307984791, 'no_speech_prob': 0.0010156923672184348}, {'id': 72, 'seek': 41196, 'start': 437.64, 'end': 441.79999999999995, 'text': ' subset of machine learning, which focuses explicitly on what are called neural networks and how we', 'tokens': [51648, 25993, 295, 3479, 2539, 11, 597, 16109, 20803, 322, 437, 366, 1219, 18161, 9590, 293, 577, 321, 51856], 'temperature': 0.0, 'avg_logprob': -0.09253543277956405, 'compression_ratio': 1.855513307984791, 'no_speech_prob': 0.0010156923672184348}, {'id': 73, 'seek': 44180, 'start': 441.8, 'end': 446.04, 'text': ' can build neural networks that can extract features in the data. These are basically what you can', 'tokens': [50364, 393, 1322, 18161, 9590, 300, 393, 8947, 4122, 294, 264, 1412, 13, 1981, 366, 1936, 437, 291, 393, 50576], 'temperature': 0.0, 'avg_logprob': -0.05873976187272505, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00011409405124140903}, {'id': 74, 'seek': 44180, 'start': 446.04, 'end': 451.24, 'text': ' think of as patterns that occur within the data so that it can learn to complete these tasks as well.', 'tokens': [50576, 519, 295, 382, 8294, 300, 5160, 1951, 264, 1412, 370, 300, 309, 393, 1466, 281, 3566, 613, 9608, 382, 731, 13, 50836], 'temperature': 0.0, 'avg_logprob': -0.05873976187272505, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00011409405124140903}, {'id': 75, 'seek': 44180, 'start': 452.76, 'end': 457.8, 'text': \" Now, that's exactly what this class is really all about at its core. We're going to try and teach\", 'tokens': [50912, 823, 11, 300, 311, 2293, 437, 341, 1508, 307, 534, 439, 466, 412, 1080, 4965, 13, 492, 434, 516, 281, 853, 293, 2924, 51164], 'temperature': 0.0, 'avg_logprob': -0.05873976187272505, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00011409405124140903}, {'id': 76, 'seek': 44180, 'start': 457.8, 'end': 463.64, 'text': ' you and give you the foundational understanding and how we can build and teach computers to learn', 'tokens': [51164, 291, 293, 976, 291, 264, 32195, 3701, 293, 577, 321, 393, 1322, 293, 2924, 10807, 281, 1466, 51456], 'temperature': 0.0, 'avg_logprob': -0.05873976187272505, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00011409405124140903}, {'id': 77, 'seek': 44180, 'start': 463.64, 'end': 468.92, 'text': \" tasks, many different types of tasks directly from raw data. And that's really what this class\", 'tokens': [51456, 9608, 11, 867, 819, 3467, 295, 9608, 3838, 490, 8936, 1412, 13, 400, 300, 311, 534, 437, 341, 1508, 51720], 'temperature': 0.0, 'avg_logprob': -0.05873976187272505, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00011409405124140903}, {'id': 78, 'seek': 46892, 'start': 468.92, 'end': 475.24, 'text': \" boils down to at its most simple form. And we'll provide a very solid foundation for you both on\", 'tokens': [50364, 35049, 760, 281, 412, 1080, 881, 2199, 1254, 13, 400, 321, 603, 2893, 257, 588, 5100, 7030, 337, 291, 1293, 322, 50680], 'temperature': 0.0, 'avg_logprob': -0.14686046026449287, 'compression_ratio': 1.8211678832116789, 'no_speech_prob': 0.0017800972564145923}, {'id': 79, 'seek': 46892, 'start': 475.24, 'end': 480.12, 'text': ' the technical side through the lectures, which will have been in two parts throughout the class,', 'tokens': [50680, 264, 6191, 1252, 807, 264, 16564, 11, 597, 486, 362, 668, 294, 732, 3166, 3710, 264, 1508, 11, 50924], 'temperature': 0.0, 'avg_logprob': -0.14686046026449287, 'compression_ratio': 1.8211678832116789, 'no_speech_prob': 0.0017800972564145923}, {'id': 80, 'seek': 46892, 'start': 480.12, 'end': 485.0, 'text': ' the first lecture and the second lecture, each one about one hour long, followed by a software lab,', 'tokens': [50924, 264, 700, 7991, 293, 264, 1150, 7991, 11, 1184, 472, 466, 472, 1773, 938, 11, 6263, 538, 257, 4722, 2715, 11, 51168], 'temperature': 0.0, 'avg_logprob': -0.14686046026449287, 'compression_ratio': 1.8211678832116789, 'no_speech_prob': 0.0017800972564145923}, {'id': 81, 'seek': 46892, 'start': 485.0, 'end': 489.32, 'text': ' which will immediately follow the lectures, which will try to reinforce a lot of what we cover in the', 'tokens': [51168, 597, 486, 4258, 1524, 264, 16564, 11, 597, 486, 853, 281, 22634, 257, 688, 295, 437, 321, 2060, 294, 264, 51384], 'temperature': 0.0, 'avg_logprob': -0.14686046026449287, 'compression_ratio': 1.8211678832116789, 'no_speech_prob': 0.0017800972564145923}, {'id': 82, 'seek': 46892, 'start': 489.32, 'end': 495.48, 'text': ' in the in the technical part of the class and you know, give you hands-on experience implementing those', 'tokens': [51384, 294, 264, 294, 264, 6191, 644, 295, 264, 1508, 293, 291, 458, 11, 976, 291, 2377, 12, 266, 1752, 18114, 729, 51692], 'temperature': 0.0, 'avg_logprob': -0.14686046026449287, 'compression_ratio': 1.8211678832116789, 'no_speech_prob': 0.0017800972564145923}, {'id': 83, 'seek': 49548, 'start': 496.44, 'end': 501.16, 'text': ' ideas. So this program is split between these two pieces, the technical lectures and the software', 'tokens': [50412, 3487, 13, 407, 341, 1461, 307, 7472, 1296, 613, 732, 3755, 11, 264, 6191, 16564, 293, 264, 4722, 50648], 'temperature': 0.0, 'avg_logprob': -0.07055375645461592, 'compression_ratio': 1.698581560283688, 'no_speech_prob': 0.0006549489335156977}, {'id': 84, 'seek': 49548, 'start': 501.16, 'end': 506.36, 'text': ' labs. We have several new updates this year in specific, especially in many of the later lectures.', 'tokens': [50648, 20339, 13, 492, 362, 2940, 777, 9205, 341, 1064, 294, 2685, 11, 2318, 294, 867, 295, 264, 1780, 16564, 13, 50908], 'temperature': 0.0, 'avg_logprob': -0.07055375645461592, 'compression_ratio': 1.698581560283688, 'no_speech_prob': 0.0006549489335156977}, {'id': 85, 'seek': 49548, 'start': 507.0, 'end': 511.0, 'text': ' The first lecture will cover the foundations of deep learning, which is going to be right now.', 'tokens': [50940, 440, 700, 7991, 486, 2060, 264, 22467, 295, 2452, 2539, 11, 597, 307, 516, 281, 312, 558, 586, 13, 51140], 'temperature': 0.0, 'avg_logprob': -0.07055375645461592, 'compression_ratio': 1.698581560283688, 'no_speech_prob': 0.0006549489335156977}, {'id': 86, 'seek': 49548, 'start': 512.12, 'end': 517.5600000000001, 'text': \" And finally, we'll conclude the course with some very exciting guest lectures from both academia\", 'tokens': [51196, 400, 2721, 11, 321, 603, 16886, 264, 1164, 365, 512, 588, 4670, 8341, 16564, 490, 1293, 28937, 51468], 'temperature': 0.0, 'avg_logprob': -0.07055375645461592, 'compression_ratio': 1.698581560283688, 'no_speech_prob': 0.0006549489335156977}, {'id': 87, 'seek': 49548, 'start': 517.5600000000001, 'end': 523.16, 'text': ' and industry who are really leading and driving forward the state of AI and deep learning.', 'tokens': [51468, 293, 3518, 567, 366, 534, 5775, 293, 4840, 2128, 264, 1785, 295, 7318, 293, 2452, 2539, 13, 51748], 'temperature': 0.0, 'avg_logprob': -0.07055375645461592, 'compression_ratio': 1.698581560283688, 'no_speech_prob': 0.0006549489335156977}, {'id': 88, 'seek': 52316, 'start': 523.16, 'end': 529.56, 'text': ' And of course, we have many awesome prizes that go with all of the software labs and the project', 'tokens': [50364, 400, 295, 1164, 11, 321, 362, 867, 3476, 27350, 300, 352, 365, 439, 295, 264, 4722, 20339, 293, 264, 1716, 50684], 'temperature': 0.0, 'avg_logprob': -0.10836294899999568, 'compression_ratio': 1.76, 'no_speech_prob': 0.0008132141083478928}, {'id': 89, 'seek': 52316, 'start': 529.56, 'end': 534.4399999999999, 'text': ' competition at the end of the course. So maybe quickly to go through these each day, like I said,', 'tokens': [50684, 6211, 412, 264, 917, 295, 264, 1164, 13, 407, 1310, 2661, 281, 352, 807, 613, 1184, 786, 11, 411, 286, 848, 11, 50928], 'temperature': 0.0, 'avg_logprob': -0.10836294899999568, 'compression_ratio': 1.76, 'no_speech_prob': 0.0008132141083478928}, {'id': 90, 'seek': 52316, 'start': 534.4399999999999, 'end': 540.04, 'text': \" we'll have dedicated software labs that couple with the lectures. Starting today with Lab 1,\", 'tokens': [50928, 321, 603, 362, 8374, 4722, 20339, 300, 1916, 365, 264, 16564, 13, 16217, 965, 365, 10137, 502, 11, 51208], 'temperature': 0.0, 'avg_logprob': -0.10836294899999568, 'compression_ratio': 1.76, 'no_speech_prob': 0.0008132141083478928}, {'id': 91, 'seek': 52316, 'start': 540.04, 'end': 544.36, 'text': \" you'll actually build a neural network, keeping with the steam of generative AI, you'll build a\", 'tokens': [51208, 291, 603, 767, 1322, 257, 18161, 3209, 11, 5145, 365, 264, 11952, 295, 1337, 1166, 7318, 11, 291, 603, 1322, 257, 51424], 'temperature': 0.0, 'avg_logprob': -0.10836294899999568, 'compression_ratio': 1.76, 'no_speech_prob': 0.0008132141083478928}, {'id': 92, 'seek': 52316, 'start': 544.36, 'end': 549.48, 'text': ' neural network that can learn, listen to a lot of music and actually learn how to generate brand new', 'tokens': [51424, 18161, 3209, 300, 393, 1466, 11, 2140, 281, 257, 688, 295, 1318, 293, 767, 1466, 577, 281, 8460, 3360, 777, 51680], 'temperature': 0.0, 'avg_logprob': -0.10836294899999568, 'compression_ratio': 1.76, 'no_speech_prob': 0.0008132141083478928}, {'id': 93, 'seek': 54948, 'start': 549.48, 'end': 556.2, 'text': \" songs in that genre of music. At the end, at the next level of the class on Friday, we'll host a\", 'tokens': [50364, 5781, 294, 300, 11022, 295, 1318, 13, 1711, 264, 917, 11, 412, 264, 958, 1496, 295, 264, 1508, 322, 6984, 11, 321, 603, 3975, 257, 50700], 'temperature': 0.0, 'avg_logprob': -0.1051861842473348, 'compression_ratio': 1.5622489959839359, 'no_speech_prob': 0.007064318750053644}, {'id': 94, 'seek': 54948, 'start': 556.2, 'end': 562.28, 'text': ' project pitch competition where either you individually or as part of a group can participate and', 'tokens': [50700, 1716, 7293, 6211, 689, 2139, 291, 16652, 420, 382, 644, 295, 257, 1594, 393, 8197, 293, 51004], 'temperature': 0.0, 'avg_logprob': -0.1051861842473348, 'compression_ratio': 1.5622489959839359, 'no_speech_prob': 0.007064318750053644}, {'id': 95, 'seek': 54948, 'start': 562.28, 'end': 569.32, 'text': \" present an idea, a novel deep learning idea to all of us. It'll be roughly three minutes in length.\", 'tokens': [51004, 1974, 364, 1558, 11, 257, 7613, 2452, 2539, 1558, 281, 439, 295, 505, 13, 467, 603, 312, 9810, 1045, 2077, 294, 4641, 13, 51356], 'temperature': 0.0, 'avg_logprob': -0.1051861842473348, 'compression_ratio': 1.5622489959839359, 'no_speech_prob': 0.007064318750053644}, {'id': 96, 'seek': 54948, 'start': 569.88, 'end': 575.5600000000001, 'text': ' And we will focus not as much because this is a one week program. We are not going to focus so', 'tokens': [51384, 400, 321, 486, 1879, 406, 382, 709, 570, 341, 307, 257, 472, 1243, 1461, 13, 492, 366, 406, 516, 281, 1879, 370, 51668], 'temperature': 0.0, 'avg_logprob': -0.1051861842473348, 'compression_ratio': 1.5622489959839359, 'no_speech_prob': 0.007064318750053644}, {'id': 97, 'seek': 57556, 'start': 575.56, 'end': 580.52, 'text': ' much on the results of your pitch, but rather the invasion and the idea and the novelty of what', 'tokens': [50364, 709, 322, 264, 3542, 295, 428, 7293, 11, 457, 2831, 264, 21575, 293, 264, 1558, 293, 264, 44805, 295, 437, 50612], 'temperature': 0.0, 'avg_logprob': -0.10939051586648692, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.0012244204990565777}, {'id': 98, 'seek': 57556, 'start': 580.52, 'end': 585.64, 'text': \" you're trying to propose. The prizes here are quite significant already. Where first prize is going\", 'tokens': [50612, 291, 434, 1382, 281, 17421, 13, 440, 27350, 510, 366, 1596, 4776, 1217, 13, 2305, 700, 12818, 307, 516, 50868], 'temperature': 0.0, 'avg_logprob': -0.10939051586648692, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.0012244204990565777}, {'id': 99, 'seek': 57556, 'start': 585.64, 'end': 591.2399999999999, 'text': ' to get an Nvidia GPU, which is really a key piece of hardware that is instrumental. If you want to', 'tokens': [50868, 281, 483, 364, 46284, 18407, 11, 597, 307, 534, 257, 2141, 2522, 295, 8837, 300, 307, 17388, 13, 759, 291, 528, 281, 51148], 'temperature': 0.0, 'avg_logprob': -0.10939051586648692, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.0012244204990565777}, {'id': 100, 'seek': 57556, 'start': 591.2399999999999, 'end': 595.4, 'text': ' actually build a deep learning project and train these neural networks, which can be very large and', 'tokens': [51148, 767, 1322, 257, 2452, 2539, 1716, 293, 3847, 613, 18161, 9590, 11, 597, 393, 312, 588, 2416, 293, 51356], 'temperature': 0.0, 'avg_logprob': -0.10939051586648692, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.0012244204990565777}, {'id': 101, 'seek': 57556, 'start': 595.4, 'end': 600.8399999999999, 'text': ' require a lot of compute, these prizes will give you the compute to do so. And finally, this year will be', 'tokens': [51356, 3651, 257, 688, 295, 14722, 11, 613, 27350, 486, 976, 291, 264, 14722, 281, 360, 370, 13, 400, 2721, 11, 341, 1064, 486, 312, 51628], 'temperature': 0.0, 'avg_logprob': -0.10939051586648692, 'compression_ratio': 1.6666666666666667, 'no_speech_prob': 0.0012244204990565777}, {'id': 102, 'seek': 60084, 'start': 600.84, 'end': 606.36, 'text': ' awarding a grand prize for labs two and three combined, which will occur on Tuesday and Wednesday,', 'tokens': [50364, 7130, 278, 257, 2697, 12818, 337, 20339, 732, 293, 1045, 9354, 11, 597, 486, 5160, 322, 10017, 293, 10579, 11, 50640], 'temperature': 0.0, 'avg_logprob': -0.0907968282699585, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.004603169392794371}, {'id': 103, 'seek': 60084, 'start': 606.36, 'end': 611.5600000000001, 'text': ' focused on what I believe is actually solving some of the most exciting problems in this field of', 'tokens': [50640, 5178, 322, 437, 286, 1697, 307, 767, 12606, 512, 295, 264, 881, 4670, 2740, 294, 341, 2519, 295, 50900], 'temperature': 0.0, 'avg_logprob': -0.0907968282699585, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.004603169392794371}, {'id': 104, 'seek': 60084, 'start': 611.5600000000001, 'end': 617.88, 'text': ' deep learning and how specifically how we can build models that can be robust, not only accurate,', 'tokens': [50900, 2452, 2539, 293, 577, 4682, 577, 321, 393, 1322, 5245, 300, 393, 312, 13956, 11, 406, 787, 8559, 11, 51216], 'temperature': 0.0, 'avg_logprob': -0.0907968282699585, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.004603169392794371}, {'id': 105, 'seek': 60084, 'start': 617.88, 'end': 622.52, 'text': \" but robust and trustworthy and safe when they're deployed as well. And you'll actually get\", 'tokens': [51216, 457, 13956, 293, 39714, 293, 3273, 562, 436, 434, 17826, 382, 731, 13, 400, 291, 603, 767, 483, 51448], 'temperature': 0.0, 'avg_logprob': -0.0907968282699585, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.004603169392794371}, {'id': 106, 'seek': 60084, 'start': 622.52, 'end': 627.72, 'text': ' experienced developing those types of solutions that can actually advance the state of VR and AI.', 'tokens': [51448, 6751, 6416, 729, 3467, 295, 6547, 300, 393, 767, 7295, 264, 1785, 295, 13722, 293, 7318, 13, 51708], 'temperature': 0.0, 'avg_logprob': -0.0907968282699585, 'compression_ratio': 1.6428571428571428, 'no_speech_prob': 0.004603169392794371}, {'id': 107, 'seek': 62772, 'start': 628.6, 'end': 633.0, 'text': ' Now, all of these labs that I mentioned and competitions here are going to be', 'tokens': [50408, 823, 11, 439, 295, 613, 20339, 300, 286, 2835, 293, 26185, 510, 366, 516, 281, 312, 50628], 'temperature': 0.0, 'avg_logprob': -0.11588404728816105, 'compression_ratio': 1.7388059701492538, 'no_speech_prob': 0.009395715780556202}, {'id': 108, 'seek': 62772, 'start': 633.88, 'end': 639.64, 'text': \" due on Thursday night at 11 p.m. right before the last day of class. And we'll be helping you all along\", 'tokens': [50672, 3462, 322, 10383, 1818, 412, 2975, 280, 13, 76, 13, 558, 949, 264, 1036, 786, 295, 1508, 13, 400, 321, 603, 312, 4315, 291, 439, 2051, 50960], 'temperature': 0.0, 'avg_logprob': -0.11588404728816105, 'compression_ratio': 1.7388059701492538, 'no_speech_prob': 0.009395715780556202}, {'id': 109, 'seek': 62772, 'start': 639.64, 'end': 645.64, 'text': ' the way. This prize or this competition in particular has very significant prizes. So I encourage', 'tokens': [50960, 264, 636, 13, 639, 12818, 420, 341, 6211, 294, 1729, 575, 588, 4776, 27350, 13, 407, 286, 5373, 51260], 'temperature': 0.0, 'avg_logprob': -0.11588404728816105, 'compression_ratio': 1.7388059701492538, 'no_speech_prob': 0.009395715780556202}, {'id': 110, 'seek': 62772, 'start': 645.64, 'end': 651.0, 'text': ' all of you to really enter this prize and try to try to give a chance to win the prize.', 'tokens': [51260, 439, 295, 291, 281, 534, 3242, 341, 12818, 293, 853, 281, 853, 281, 976, 257, 2931, 281, 1942, 264, 12818, 13, 51528], 'temperature': 0.0, 'avg_logprob': -0.11588404728816105, 'compression_ratio': 1.7388059701492538, 'no_speech_prob': 0.009395715780556202}, {'id': 111, 'seek': 62772, 'start': 652.28, 'end': 656.52, 'text': \" And of course, like I said, we're going to be helping you all along the way. We are many available\", 'tokens': [51592, 400, 295, 1164, 11, 411, 286, 848, 11, 321, 434, 516, 281, 312, 4315, 291, 439, 2051, 264, 636, 13, 492, 366, 867, 2435, 51804], 'temperature': 0.0, 'avg_logprob': -0.11588404728816105, 'compression_ratio': 1.7388059701492538, 'no_speech_prob': 0.009395715780556202}, {'id': 112, 'seek': 65652, 'start': 656.52, 'end': 661.8, 'text': ' resources throughout this class to help you achieve this. Please post a piata if you have any', 'tokens': [50364, 3593, 3710, 341, 1508, 281, 854, 291, 4584, 341, 13, 2555, 2183, 257, 3895, 3274, 498, 291, 362, 604, 50628], 'temperature': 0.0, 'avg_logprob': -0.1194135992376654, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0035217558033764362}, {'id': 113, 'seek': 65652, 'start': 661.8, 'end': 667.0, 'text': ' questions. And of course, this program has an incredible team that you can reach out to at any', 'tokens': [50628, 1651, 13, 400, 295, 1164, 11, 341, 1461, 575, 364, 4651, 1469, 300, 291, 393, 2524, 484, 281, 412, 604, 50888], 'temperature': 0.0, 'avg_logprob': -0.1194135992376654, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0035217558033764362}, {'id': 114, 'seek': 65652, 'start': 667.0, 'end': 672.92, 'text': ' point in case you have any issues or questions on the materials myself and Ava will be your two main', 'tokens': [50888, 935, 294, 1389, 291, 362, 604, 2663, 420, 1651, 322, 264, 5319, 2059, 293, 316, 2757, 486, 312, 428, 732, 2135, 51184], 'temperature': 0.0, 'avg_logprob': -0.1194135992376654, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0035217558033764362}, {'id': 115, 'seek': 65652, 'start': 672.92, 'end': 677.8, 'text': \" lectures for the first part of the class. We'll also be hearing like I said in the later part of\", 'tokens': [51184, 16564, 337, 264, 700, 644, 295, 264, 1508, 13, 492, 603, 611, 312, 4763, 411, 286, 848, 294, 264, 1780, 644, 295, 51428], 'temperature': 0.0, 'avg_logprob': -0.1194135992376654, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0035217558033764362}, {'id': 116, 'seek': 65652, 'start': 677.8, 'end': 682.28, 'text': ' the class from some guest lectures who will share some really cutting edge state of the art', 'tokens': [51428, 264, 1508, 490, 512, 8341, 16564, 567, 486, 2073, 512, 534, 6492, 4691, 1785, 295, 264, 1523, 51652], 'temperature': 0.0, 'avg_logprob': -0.1194135992376654, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0035217558033764362}, {'id': 117, 'seek': 68228, 'start': 682.28, 'end': 686.6, 'text': ' developments and deep learning. And of course, I want to give a huge shout out and thanks to all', 'tokens': [50364, 20862, 293, 2452, 2539, 13, 400, 295, 1164, 11, 286, 528, 281, 976, 257, 2603, 8043, 484, 293, 3231, 281, 439, 50580], 'temperature': 0.0, 'avg_logprob': -0.15764803635446648, 'compression_ratio': 1.7153024911032029, 'no_speech_prob': 0.008805569261312485}, {'id': 118, 'seek': 68228, 'start': 686.6, 'end': 691.9599999999999, 'text': \" of our sponsors who without their support, this program wouldn't have been possible for yet again\", 'tokens': [50580, 295, 527, 22593, 567, 1553, 641, 1406, 11, 341, 1461, 2759, 380, 362, 668, 1944, 337, 1939, 797, 50848], 'temperature': 0.0, 'avg_logprob': -0.15764803635446648, 'compression_ratio': 1.7153024911032029, 'no_speech_prob': 0.008805569261312485}, {'id': 119, 'seek': 68228, 'start': 691.9599999999999, 'end': 698.36, 'text': \" another year. So thank you all. Okay, so now with that, let's really dive into the really fun\", 'tokens': [50848, 1071, 1064, 13, 407, 1309, 291, 439, 13, 1033, 11, 370, 586, 365, 300, 11, 718, 311, 534, 9192, 666, 264, 534, 1019, 51168], 'temperature': 0.0, 'avg_logprob': -0.15764803635446648, 'compression_ratio': 1.7153024911032029, 'no_speech_prob': 0.008805569261312485}, {'id': 120, 'seek': 68228, 'start': 698.36, 'end': 705.0, 'text': \" stuff of today's lecture, which is the technical part. And I think I want to start this part by asking\", 'tokens': [51168, 1507, 295, 965, 311, 7991, 11, 597, 307, 264, 6191, 644, 13, 400, 286, 519, 286, 528, 281, 722, 341, 644, 538, 3365, 51500], 'temperature': 0.0, 'avg_logprob': -0.15764803635446648, 'compression_ratio': 1.7153024911032029, 'no_speech_prob': 0.008805569261312485}, {'id': 121, 'seek': 68228, 'start': 706.1999999999999, 'end': 710.76, 'text': ' all of you and having yourselves ask yourself, having you ask yourselves this question of,', 'tokens': [51560, 439, 295, 291, 293, 1419, 14791, 1029, 1803, 11, 1419, 291, 1029, 14791, 341, 1168, 295, 11, 51788], 'temperature': 0.0, 'avg_logprob': -0.15764803635446648, 'compression_ratio': 1.7153024911032029, 'no_speech_prob': 0.008805569261312485}, {'id': 122, 'seek': 71076, 'start': 711.16, 'end': 716.28, 'text': ' you know, why are all of you here? First of all, why do you care about this topic in the first place?', 'tokens': [50384, 291, 458, 11, 983, 366, 439, 295, 291, 510, 30, 2386, 295, 439, 11, 983, 360, 291, 1127, 466, 341, 4829, 294, 264, 700, 1081, 30, 50640], 'temperature': 0.0, 'avg_logprob': -0.08579947208536082, 'compression_ratio': 1.93359375, 'no_speech_prob': 0.0007911373977549374}, {'id': 123, 'seek': 71076, 'start': 716.28, 'end': 721.8, 'text': ' Now, I think to answer this question, we have to take a step back and think about, you know,', 'tokens': [50640, 823, 11, 286, 519, 281, 1867, 341, 1168, 11, 321, 362, 281, 747, 257, 1823, 646, 293, 519, 466, 11, 291, 458, 11, 50916], 'temperature': 0.0, 'avg_logprob': -0.08579947208536082, 'compression_ratio': 1.93359375, 'no_speech_prob': 0.0007911373977549374}, {'id': 124, 'seek': 71076, 'start': 721.8, 'end': 726.52, 'text': ' the history of machine learning and what machine learning is and what deep learning brings to the', 'tokens': [50916, 264, 2503, 295, 3479, 2539, 293, 437, 3479, 2539, 307, 293, 437, 2452, 2539, 5607, 281, 264, 51152], 'temperature': 0.0, 'avg_logprob': -0.08579947208536082, 'compression_ratio': 1.93359375, 'no_speech_prob': 0.0007911373977549374}, {'id': 125, 'seek': 71076, 'start': 726.52, 'end': 732.12, 'text': ' table on top of machine learning. Now, traditional machine learning algorithms typically define what are', 'tokens': [51152, 3199, 322, 1192, 295, 3479, 2539, 13, 823, 11, 5164, 3479, 2539, 14642, 5850, 6964, 437, 366, 51432], 'temperature': 0.0, 'avg_logprob': -0.08579947208536082, 'compression_ratio': 1.93359375, 'no_speech_prob': 0.0007911373977549374}, {'id': 126, 'seek': 71076, 'start': 732.12, 'end': 737.24, 'text': ' called these set of features in the data. You can think of these as certain patterns in the data.', 'tokens': [51432, 1219, 613, 992, 295, 4122, 294, 264, 1412, 13, 509, 393, 519, 295, 613, 382, 1629, 8294, 294, 264, 1412, 13, 51688], 'temperature': 0.0, 'avg_logprob': -0.08579947208536082, 'compression_ratio': 1.93359375, 'no_speech_prob': 0.0007911373977549374}, {'id': 127, 'seek': 73724, 'start': 737.24, 'end': 741.0, 'text': ' And then usually these features are hand engineered. So probably a human will come into the', 'tokens': [50364, 400, 550, 2673, 613, 4122, 366, 1011, 38648, 13, 407, 1391, 257, 1952, 486, 808, 666, 264, 50552], 'temperature': 0.0, 'avg_logprob': -0.09852949489246715, 'compression_ratio': 1.8104089219330854, 'no_speech_prob': 0.0010813081171363592}, {'id': 128, 'seek': 73724, 'start': 741.0, 'end': 746.6, 'text': ' data set and with a lot of domain knowledge and experience can try to uncover what these features', 'tokens': [50552, 1412, 992, 293, 365, 257, 688, 295, 9274, 3601, 293, 1752, 393, 853, 281, 21694, 437, 613, 4122, 50832], 'temperature': 0.0, 'avg_logprob': -0.09852949489246715, 'compression_ratio': 1.8104089219330854, 'no_speech_prob': 0.0010813081171363592}, {'id': 129, 'seek': 73724, 'start': 746.6, 'end': 751.96, 'text': ' might be. Now, the key idea of deep learning and this is really central to this class is that instead', 'tokens': [50832, 1062, 312, 13, 823, 11, 264, 2141, 1558, 295, 2452, 2539, 293, 341, 307, 534, 5777, 281, 341, 1508, 307, 300, 2602, 51100], 'temperature': 0.0, 'avg_logprob': -0.09852949489246715, 'compression_ratio': 1.8104089219330854, 'no_speech_prob': 0.0010813081171363592}, {'id': 130, 'seek': 73724, 'start': 751.96, 'end': 757.64, 'text': ' of having a human define these features, what if we could have a machine look at all of this data', 'tokens': [51100, 295, 1419, 257, 1952, 6964, 613, 4122, 11, 437, 498, 321, 727, 362, 257, 3479, 574, 412, 439, 295, 341, 1412, 51384], 'temperature': 0.0, 'avg_logprob': -0.09852949489246715, 'compression_ratio': 1.8104089219330854, 'no_speech_prob': 0.0010813081171363592}, {'id': 131, 'seek': 73724, 'start': 757.64, 'end': 763.08, 'text': ' and actually try to extract and uncover what are the core patterns in the data so that it can use', 'tokens': [51384, 293, 767, 853, 281, 8947, 293, 21694, 437, 366, 264, 4965, 8294, 294, 264, 1412, 370, 300, 309, 393, 764, 51656], 'temperature': 0.0, 'avg_logprob': -0.09852949489246715, 'compression_ratio': 1.8104089219330854, 'no_speech_prob': 0.0010813081171363592}, {'id': 132, 'seek': 76308, 'start': 763.08, 'end': 768.6, 'text': ' those when it sees new data to make some decisions. So for example, if we wanted to detect faces in', 'tokens': [50364, 729, 562, 309, 8194, 777, 1412, 281, 652, 512, 5327, 13, 407, 337, 1365, 11, 498, 321, 1415, 281, 5531, 8475, 294, 50640], 'temperature': 0.0, 'avg_logprob': -0.1102421305201075, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 0.002588922856375575}, {'id': 133, 'seek': 76308, 'start': 768.6, 'end': 773.72, 'text': ' an image, a deep neural network algorithm might actually learn that in order to detect a face,', 'tokens': [50640, 364, 3256, 11, 257, 2452, 18161, 3209, 9284, 1062, 767, 1466, 300, 294, 1668, 281, 5531, 257, 1851, 11, 50896], 'temperature': 0.0, 'avg_logprob': -0.1102421305201075, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 0.002588922856375575}, {'id': 134, 'seek': 76308, 'start': 773.72, 'end': 779.0, 'text': ' it first has to detect things like edges in the image, lines and edges. And when you combine those', 'tokens': [50896, 309, 700, 575, 281, 5531, 721, 411, 8819, 294, 264, 3256, 11, 3876, 293, 8819, 13, 400, 562, 291, 10432, 729, 51160], 'temperature': 0.0, 'avg_logprob': -0.1102421305201075, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 0.002588922856375575}, {'id': 135, 'seek': 76308, 'start': 779.0, 'end': 784.84, 'text': ' lines and edges, you can actually create compositions of features like corners and curves, which when', 'tokens': [51160, 3876, 293, 8819, 11, 291, 393, 767, 1884, 43401, 295, 4122, 411, 12413, 293, 19490, 11, 597, 562, 51452], 'temperature': 0.0, 'avg_logprob': -0.1102421305201075, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 0.002588922856375575}, {'id': 136, 'seek': 76308, 'start': 784.84, 'end': 788.84, 'text': ' you create those when you combine those, you can create more high level features. For example,', 'tokens': [51452, 291, 1884, 729, 562, 291, 10432, 729, 11, 291, 393, 1884, 544, 1090, 1496, 4122, 13, 1171, 1365, 11, 51652], 'temperature': 0.0, 'avg_logprob': -0.1102421305201075, 'compression_ratio': 1.8918918918918919, 'no_speech_prob': 0.002588922856375575}, {'id': 137, 'seek': 78884, 'start': 788.84, 'end': 794.44, 'text': ' eyes and noses and ears. And then those are the features that allow you to ultimately detect what', 'tokens': [50364, 2575, 293, 3269, 279, 293, 8798, 13, 400, 550, 729, 366, 264, 4122, 300, 2089, 291, 281, 6284, 5531, 437, 50644], 'temperature': 0.0, 'avg_logprob': -0.09573843768823927, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00020021782256662846}, {'id': 138, 'seek': 78884, 'start': 794.44, 'end': 798.12, 'text': ' you care about detecting, which is the face. But all of these come from what are called kind of a', 'tokens': [50644, 291, 1127, 466, 40237, 11, 597, 307, 264, 1851, 13, 583, 439, 295, 613, 808, 490, 437, 366, 1219, 733, 295, 257, 50828], 'temperature': 0.0, 'avg_logprob': -0.09573843768823927, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00020021782256662846}, {'id': 139, 'seek': 78884, 'start': 798.12, 'end': 803.1600000000001, 'text': ' hierarchical learning of features. And you can actually see some examples of these. These are real', 'tokens': [50828, 35250, 804, 2539, 295, 4122, 13, 400, 291, 393, 767, 536, 512, 5110, 295, 613, 13, 1981, 366, 957, 51080], 'temperature': 0.0, 'avg_logprob': -0.09573843768823927, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00020021782256662846}, {'id': 140, 'seek': 78884, 'start': 803.1600000000001, 'end': 807.96, 'text': \" features learned by a neural network and how they're combined defines this progression of information.\", 'tokens': [51080, 4122, 3264, 538, 257, 18161, 3209, 293, 577, 436, 434, 9354, 23122, 341, 18733, 295, 1589, 13, 51320], 'temperature': 0.0, 'avg_logprob': -0.09573843768823927, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00020021782256662846}, {'id': 141, 'seek': 78884, 'start': 809.32, 'end': 814.12, 'text': ' But in fact, what I just described, this underlying and fundamental building block of neural', 'tokens': [51388, 583, 294, 1186, 11, 437, 286, 445, 7619, 11, 341, 14217, 293, 8088, 2390, 3461, 295, 18161, 51628], 'temperature': 0.0, 'avg_logprob': -0.09573843768823927, 'compression_ratio': 1.7562724014336917, 'no_speech_prob': 0.00020021782256662846}, {'id': 142, 'seek': 81412, 'start': 814.12, 'end': 820.44, 'text': ' networks and deep learning have actually existed for decades. Now, why are we studying all of this now?', 'tokens': [50364, 9590, 293, 2452, 2539, 362, 767, 13135, 337, 7878, 13, 823, 11, 983, 366, 321, 7601, 439, 295, 341, 586, 30, 50680], 'temperature': 0.0, 'avg_logprob': -0.09749538020083778, 'compression_ratio': 1.6270491803278688, 'no_speech_prob': 0.0038819112814962864}, {'id': 143, 'seek': 81412, 'start': 820.44, 'end': 825.48, 'text': ' And today in this class, with all this great enthusiasm to learn this, right? Well, for one,', 'tokens': [50680, 400, 965, 294, 341, 1508, 11, 365, 439, 341, 869, 23417, 281, 1466, 341, 11, 558, 30, 1042, 11, 337, 472, 11, 50932], 'temperature': 0.0, 'avg_logprob': -0.09749538020083778, 'compression_ratio': 1.6270491803278688, 'no_speech_prob': 0.0038819112814962864}, {'id': 144, 'seek': 81412, 'start': 826.04, 'end': 831.96, 'text': ' there have been several key advances that have occurred in the past decade. Number one is that data', 'tokens': [50960, 456, 362, 668, 2940, 2141, 25297, 300, 362, 11068, 294, 264, 1791, 10378, 13, 5118, 472, 307, 300, 1412, 51256], 'temperature': 0.0, 'avg_logprob': -0.09749538020083778, 'compression_ratio': 1.6270491803278688, 'no_speech_prob': 0.0038819112814962864}, {'id': 145, 'seek': 81412, 'start': 831.96, 'end': 838.2, 'text': ' is so much more pervasive than it has ever been before in our lifetimes. These models are hungry for', 'tokens': [51256, 307, 370, 709, 544, 680, 39211, 813, 309, 575, 1562, 668, 949, 294, 527, 4545, 302, 1532, 13, 1981, 5245, 366, 8067, 337, 51568], 'temperature': 0.0, 'avg_logprob': -0.09749538020083778, 'compression_ratio': 1.6270491803278688, 'no_speech_prob': 0.0038819112814962864}, {'id': 146, 'seek': 83820, 'start': 838.2, 'end': 844.84, 'text': \" more data. And we're living in the age of big data. More data is available to these models\", 'tokens': [50364, 544, 1412, 13, 400, 321, 434, 2647, 294, 264, 3205, 295, 955, 1412, 13, 5048, 1412, 307, 2435, 281, 613, 5245, 50696], 'temperature': 0.0, 'avg_logprob': -0.09315523637079559, 'compression_ratio': 1.7508896797153024, 'no_speech_prob': 0.0027121123857796192}, {'id': 147, 'seek': 83820, 'start': 844.84, 'end': 851.48, 'text': ' than ever before and they thrive off of that. Secondly, these algorithms are massively parallelizable.', 'tokens': [50696, 813, 1562, 949, 293, 436, 21233, 766, 295, 300, 13, 19483, 11, 613, 14642, 366, 29379, 8952, 22395, 13, 51028], 'temperature': 0.0, 'avg_logprob': -0.09315523637079559, 'compression_ratio': 1.7508896797153024, 'no_speech_prob': 0.0027121123857796192}, {'id': 148, 'seek': 83820, 'start': 851.48, 'end': 857.8000000000001, 'text': \" They require a lot of compute. And we're also at a unique time in history where we have the ability\", 'tokens': [51028, 814, 3651, 257, 688, 295, 14722, 13, 400, 321, 434, 611, 412, 257, 3845, 565, 294, 2503, 689, 321, 362, 264, 3485, 51344], 'temperature': 0.0, 'avg_logprob': -0.09315523637079559, 'compression_ratio': 1.7508896797153024, 'no_speech_prob': 0.0027121123857796192}, {'id': 149, 'seek': 83820, 'start': 857.8000000000001, 'end': 862.9200000000001, 'text': ' to train these extremely large scale algorithms and techniques that have existed for a very long time.', 'tokens': [51344, 281, 3847, 613, 4664, 2416, 4373, 14642, 293, 7512, 300, 362, 13135, 337, 257, 588, 938, 565, 13, 51600], 'temperature': 0.0, 'avg_logprob': -0.09315523637079559, 'compression_ratio': 1.7508896797153024, 'no_speech_prob': 0.0027121123857796192}, {'id': 150, 'seek': 83820, 'start': 862.9200000000001, 'end': 867.4000000000001, 'text': ' But we can now train them due to the hardware advances that have been made. And finally, due to', 'tokens': [51600, 583, 321, 393, 586, 3847, 552, 3462, 281, 264, 8837, 25297, 300, 362, 668, 1027, 13, 400, 2721, 11, 3462, 281, 51824], 'temperature': 0.0, 'avg_logprob': -0.09315523637079559, 'compression_ratio': 1.7508896797153024, 'no_speech_prob': 0.0027121123857796192}, {'id': 151, 'seek': 86740, 'start': 867.4, 'end': 872.4399999999999, 'text': ' open source toolboxes and software platforms like TensorFlow, for example, which all of you will', 'tokens': [50364, 1269, 4009, 44593, 279, 293, 4722, 9473, 411, 37624, 11, 337, 1365, 11, 597, 439, 295, 291, 486, 50616], 'temperature': 0.0, 'avg_logprob': -0.07511376880464099, 'compression_ratio': 1.7246376811594204, 'no_speech_prob': 0.0001419764885213226}, {'id': 152, 'seek': 86740, 'start': 872.4399999999999, 'end': 878.52, 'text': ' get a lot of experience on in this class, training and building the code for these neural networks has', 'tokens': [50616, 483, 257, 688, 295, 1752, 322, 294, 341, 1508, 11, 3097, 293, 2390, 264, 3089, 337, 613, 18161, 9590, 575, 50920], 'temperature': 0.0, 'avg_logprob': -0.07511376880464099, 'compression_ratio': 1.7246376811594204, 'no_speech_prob': 0.0001419764885213226}, {'id': 153, 'seek': 86740, 'start': 878.52, 'end': 883.0, 'text': ' never been easier. So from the software point of view as well, there have been incredible advances to', 'tokens': [50920, 1128, 668, 3571, 13, 407, 490, 264, 4722, 935, 295, 1910, 382, 731, 11, 456, 362, 668, 4651, 25297, 281, 51144], 'temperature': 0.0, 'avg_logprob': -0.07511376880464099, 'compression_ratio': 1.7246376811594204, 'no_speech_prob': 0.0001419764885213226}, {'id': 154, 'seek': 86740, 'start': 883.0, 'end': 887.56, 'text': \" open source, you know, the underlying fundamentals of what you're going to learn.\", 'tokens': [51144, 1269, 4009, 11, 291, 458, 11, 264, 14217, 29505, 295, 437, 291, 434, 516, 281, 1466, 13, 51372], 'temperature': 0.0, 'avg_logprob': -0.07511376880464099, 'compression_ratio': 1.7246376811594204, 'no_speech_prob': 0.0001419764885213226}, {'id': 155, 'seek': 86740, 'start': 888.52, 'end': 893.72, 'text': ' So let me start now with just building up from the ground up, the fundamental building block', 'tokens': [51420, 407, 718, 385, 722, 586, 365, 445, 2390, 493, 490, 264, 2727, 493, 11, 264, 8088, 2390, 3461, 51680], 'temperature': 0.0, 'avg_logprob': -0.07511376880464099, 'compression_ratio': 1.7246376811594204, 'no_speech_prob': 0.0001419764885213226}, {'id': 156, 'seek': 89372, 'start': 893.72, 'end': 898.12, 'text': \" of every single neural network that you're going to learn in this class. And that's going to be\", 'tokens': [50364, 295, 633, 2167, 18161, 3209, 300, 291, 434, 516, 281, 1466, 294, 341, 1508, 13, 400, 300, 311, 516, 281, 312, 50584], 'temperature': 0.0, 'avg_logprob': -0.07985643417604509, 'compression_ratio': 1.9123505976095618, 'no_speech_prob': 0.0038201415445655584}, {'id': 157, 'seek': 89372, 'start': 898.12, 'end': 903.88, 'text': ' just a single neuron, right? And in neural network language, a single neuron is called a perceptron.', 'tokens': [50584, 445, 257, 2167, 34090, 11, 558, 30, 400, 294, 18161, 3209, 2856, 11, 257, 2167, 34090, 307, 1219, 257, 43276, 2044, 13, 50872], 'temperature': 0.0, 'avg_logprob': -0.07985643417604509, 'compression_ratio': 1.9123505976095618, 'no_speech_prob': 0.0038201415445655584}, {'id': 158, 'seek': 89372, 'start': 905.1600000000001, 'end': 912.0400000000001, 'text': \" So what is a perceptron? A perceptron is, like I said, a single neuron. And it's actually, I'm\", 'tokens': [50936, 407, 437, 307, 257, 43276, 2044, 30, 316, 43276, 2044, 307, 11, 411, 286, 848, 11, 257, 2167, 34090, 13, 400, 309, 311, 767, 11, 286, 478, 51280], 'temperature': 0.0, 'avg_logprob': -0.07985643417604509, 'compression_ratio': 1.9123505976095618, 'no_speech_prob': 0.0038201415445655584}, {'id': 159, 'seek': 89372, 'start': 912.0400000000001, 'end': 915.96, 'text': \" going to say it's very, very simple idea. So I want to make sure that everyone in the audience\", 'tokens': [51280, 516, 281, 584, 309, 311, 588, 11, 588, 2199, 1558, 13, 407, 286, 528, 281, 652, 988, 300, 1518, 294, 264, 4034, 51476], 'temperature': 0.0, 'avg_logprob': -0.07985643417604509, 'compression_ratio': 1.9123505976095618, 'no_speech_prob': 0.0038201415445655584}, {'id': 160, 'seek': 89372, 'start': 915.96, 'end': 921.4, 'text': \" understands exactly what a perceptron is and how it works. So let's start by first defining a\", 'tokens': [51476, 15146, 2293, 437, 257, 43276, 2044, 307, 293, 577, 309, 1985, 13, 407, 718, 311, 722, 538, 700, 17827, 257, 51748], 'temperature': 0.0, 'avg_logprob': -0.07985643417604509, 'compression_ratio': 1.9123505976095618, 'no_speech_prob': 0.0038201415445655584}, {'id': 161, 'seek': 92140, 'start': 921.4, 'end': 927.4, 'text': ' perceptron as taking as input a set of inputs, right? So on the left hand side, you can see this', 'tokens': [50364, 43276, 2044, 382, 1940, 382, 4846, 257, 992, 295, 15743, 11, 558, 30, 407, 322, 264, 1411, 1011, 1252, 11, 291, 393, 536, 341, 50664], 'temperature': 0.0, 'avg_logprob': -0.11333922431582497, 'compression_ratio': 1.6952789699570816, 'no_speech_prob': 0.00026526208966970444}, {'id': 162, 'seek': 92140, 'start': 927.4, 'end': 933.88, 'text': \" perceptron takes M different inputs, one to M, right? These are the blue circles. We're denoting\", 'tokens': [50664, 43276, 2044, 2516, 376, 819, 15743, 11, 472, 281, 376, 11, 558, 30, 1981, 366, 264, 3344, 13040, 13, 492, 434, 1441, 17001, 50988], 'temperature': 0.0, 'avg_logprob': -0.11333922431582497, 'compression_ratio': 1.6952789699570816, 'no_speech_prob': 0.00026526208966970444}, {'id': 163, 'seek': 92140, 'start': 933.88, 'end': 941.9599999999999, 'text': \" these inputs as x's. Each of these numbers, each of these inputs is then multiplied by a corresponding\", 'tokens': [50988, 613, 15743, 382, 2031, 311, 13, 6947, 295, 613, 3547, 11, 1184, 295, 613, 15743, 307, 550, 17207, 538, 257, 11760, 51392], 'temperature': 0.0, 'avg_logprob': -0.11333922431582497, 'compression_ratio': 1.6952789699570816, 'no_speech_prob': 0.00026526208966970444}, {'id': 164, 'seek': 92140, 'start': 941.9599999999999, 'end': 948.76, 'text': \" weight, which we can call w, right? So x1 will be multiplied by w1 and we'll add the result of all\", 'tokens': [51392, 3364, 11, 597, 321, 393, 818, 261, 11, 558, 30, 407, 2031, 16, 486, 312, 17207, 538, 261, 16, 293, 321, 603, 909, 264, 1874, 295, 439, 51732], 'temperature': 0.0, 'avg_logprob': -0.11333922431582497, 'compression_ratio': 1.6952789699570816, 'no_speech_prob': 0.00026526208966970444}, {'id': 165, 'seek': 94876, 'start': 948.76, 'end': 955.24, 'text': ' of these multiplications together. Now, we take that single number after the addition and we pass', 'tokens': [50364, 295, 613, 17596, 763, 1214, 13, 823, 11, 321, 747, 300, 2167, 1230, 934, 264, 4500, 293, 321, 1320, 50688], 'temperature': 0.0, 'avg_logprob': -0.08886981221426905, 'compression_ratio': 1.7304964539007093, 'no_speech_prob': 0.0002233868435723707}, {'id': 166, 'seek': 94876, 'start': 955.24, 'end': 960.04, 'text': ' it through this nonlinear, what we call a nonlinear activation function. And that produces our final', 'tokens': [50688, 309, 807, 341, 2107, 28263, 11, 437, 321, 818, 257, 2107, 28263, 24433, 2445, 13, 400, 300, 14725, 527, 2572, 50928], 'temperature': 0.0, 'avg_logprob': -0.08886981221426905, 'compression_ratio': 1.7304964539007093, 'no_speech_prob': 0.0002233868435723707}, {'id': 167, 'seek': 94876, 'start': 960.04, 'end': 967.3199999999999, 'text': ' output of the perceptron, which we can call y. Now, this is actually not entirely accurate of the', 'tokens': [50928, 5598, 295, 264, 43276, 2044, 11, 597, 321, 393, 818, 288, 13, 823, 11, 341, 307, 767, 406, 7696, 8559, 295, 264, 51292], 'temperature': 0.0, 'avg_logprob': -0.08886981221426905, 'compression_ratio': 1.7304964539007093, 'no_speech_prob': 0.0002233868435723707}, {'id': 168, 'seek': 94876, 'start': 967.96, 'end': 972.68, 'text': \" picture of a perceptron. There's one step that I forgot to mention here. So in addition to\", 'tokens': [51324, 3036, 295, 257, 43276, 2044, 13, 821, 311, 472, 1823, 300, 286, 5298, 281, 2152, 510, 13, 407, 294, 4500, 281, 51560], 'temperature': 0.0, 'avg_logprob': -0.08886981221426905, 'compression_ratio': 1.7304964539007093, 'no_speech_prob': 0.0002233868435723707}, {'id': 169, 'seek': 94876, 'start': 973.4, 'end': 977.3199999999999, 'text': \" multiplying all of these inputs with their corresponding weights, we're also now going to add what's\", 'tokens': [51596, 30955, 439, 295, 613, 15743, 365, 641, 11760, 17443, 11, 321, 434, 611, 586, 516, 281, 909, 437, 311, 51792], 'temperature': 0.0, 'avg_logprob': -0.08886981221426905, 'compression_ratio': 1.7304964539007093, 'no_speech_prob': 0.0002233868435723707}, {'id': 170, 'seek': 97732, 'start': 977.32, 'end': 983.48, 'text': ' called a bias term. Here denoted as this w0, which is just a scalar weight, and you can think of it', 'tokens': [50364, 1219, 257, 12577, 1433, 13, 1692, 1441, 23325, 382, 341, 261, 15, 11, 597, 307, 445, 257, 39684, 3364, 11, 293, 291, 393, 519, 295, 309, 50672], 'temperature': 0.0, 'avg_logprob': -0.10506993649052639, 'compression_ratio': 1.6097560975609757, 'no_speech_prob': 0.0002694064751267433}, {'id': 171, 'seek': 97732, 'start': 983.48, 'end': 990.0400000000001, 'text': \" coming with an input of just 1. So that's going to allow the network to basically shift its nonlinear\", 'tokens': [50672, 1348, 365, 364, 4846, 295, 445, 502, 13, 407, 300, 311, 516, 281, 2089, 264, 3209, 281, 1936, 5513, 1080, 2107, 28263, 51000], 'temperature': 0.0, 'avg_logprob': -0.10506993649052639, 'compression_ratio': 1.6097560975609757, 'no_speech_prob': 0.0002694064751267433}, {'id': 172, 'seek': 97732, 'start': 990.0400000000001, 'end': 997.6400000000001, 'text': ' activation function, you know, nonlinearly, right, as it sees its inputs. Now, on the right hand', 'tokens': [51000, 24433, 2445, 11, 291, 458, 11, 2107, 28263, 356, 11, 558, 11, 382, 309, 8194, 1080, 15743, 13, 823, 11, 322, 264, 558, 1011, 51380], 'temperature': 0.0, 'avg_logprob': -0.10506993649052639, 'compression_ratio': 1.6097560975609757, 'no_speech_prob': 0.0002694064751267433}, {'id': 173, 'seek': 97732, 'start': 997.6400000000001, 'end': 1003.24, 'text': ' side, you can see this diagram mathematically formulated, right? As a single equation, we can now', 'tokens': [51380, 1252, 11, 291, 393, 536, 341, 10686, 44003, 48936, 11, 558, 30, 1018, 257, 2167, 5367, 11, 321, 393, 586, 51660], 'temperature': 0.0, 'avg_logprob': -0.10506993649052639, 'compression_ratio': 1.6097560975609757, 'no_speech_prob': 0.0002694064751267433}, {'id': 174, 'seek': 100324, 'start': 1003.24, 'end': 1009.88, 'text': ' rewrite this linear, this equation with linear algebra terms of vectors and dot products, right? So', 'tokens': [50364, 28132, 341, 8213, 11, 341, 5367, 365, 8213, 21989, 2115, 295, 18875, 293, 5893, 3383, 11, 558, 30, 407, 50696], 'temperature': 0.0, 'avg_logprob': -0.17520193735758463, 'compression_ratio': 1.5706806282722514, 'no_speech_prob': 0.0008826102712191641}, {'id': 175, 'seek': 100324, 'start': 1009.88, 'end': 1018.6800000000001, 'text': ' for example, we can define our entire inputs x1 to xm as large vector x, right? That large vector x', 'tokens': [50696, 337, 1365, 11, 321, 393, 6964, 527, 2302, 15743, 2031, 16, 281, 2031, 76, 382, 2416, 8062, 2031, 11, 558, 30, 663, 2416, 8062, 2031, 51136], 'temperature': 0.0, 'avg_logprob': -0.17520193735758463, 'compression_ratio': 1.5706806282722514, 'no_speech_prob': 0.0008826102712191641}, {'id': 176, 'seek': 100324, 'start': 1018.6800000000001, 'end': 1026.36, 'text': ' can be multiplied by, or take you a dot, excuse me, matrix multiplied with our weights w. This again', 'tokens': [51136, 393, 312, 17207, 538, 11, 420, 747, 291, 257, 5893, 11, 8960, 385, 11, 8141, 17207, 365, 527, 17443, 261, 13, 639, 797, 51520], 'temperature': 0.0, 'avg_logprob': -0.17520193735758463, 'compression_ratio': 1.5706806282722514, 'no_speech_prob': 0.0008826102712191641}, {'id': 177, 'seek': 102636, 'start': 1026.36, 'end': 1033.6399999999999, 'text': ' another vector of our weights w1 to wn, taking their dot product, not only multiplies them,', 'tokens': [50364, 1071, 8062, 295, 527, 17443, 261, 16, 281, 261, 77, 11, 1940, 641, 5893, 1674, 11, 406, 787, 12788, 530, 552, 11, 50728], 'temperature': 0.0, 'avg_logprob': -0.12068110764628709, 'compression_ratio': 1.5819672131147542, 'no_speech_prob': 0.0016481098718941212}, {'id': 178, 'seek': 102636, 'start': 1033.6399999999999, 'end': 1039.3999999999999, 'text': ' but it also adds the resulting terms together, adding a bias, like we said before, and applying', 'tokens': [50728, 457, 309, 611, 10860, 264, 16505, 2115, 1214, 11, 5127, 257, 12577, 11, 411, 321, 848, 949, 11, 293, 9275, 51016], 'temperature': 0.0, 'avg_logprob': -0.12068110764628709, 'compression_ratio': 1.5819672131147542, 'no_speech_prob': 0.0016481098718941212}, {'id': 179, 'seek': 102636, 'start': 1039.3999999999999, 'end': 1045.9599999999998, 'text': \" this nonlinearity. Now, you might be wondering what is this nonlinear function? I've mentioned it\", 'tokens': [51016, 341, 2107, 1889, 17409, 13, 823, 11, 291, 1062, 312, 6359, 437, 307, 341, 2107, 28263, 2445, 30, 286, 600, 2835, 309, 51344], 'temperature': 0.0, 'avg_logprob': -0.12068110764628709, 'compression_ratio': 1.5819672131147542, 'no_speech_prob': 0.0016481098718941212}, {'id': 180, 'seek': 102636, 'start': 1045.9599999999998, 'end': 1052.36, 'text': \" a few times already. Well, I said it is a function, right? That's past that we passed the outputs of\", 'tokens': [51344, 257, 1326, 1413, 1217, 13, 1042, 11, 286, 848, 309, 307, 257, 2445, 11, 558, 30, 663, 311, 1791, 300, 321, 4678, 264, 23930, 295, 51664], 'temperature': 0.0, 'avg_logprob': -0.12068110764628709, 'compression_ratio': 1.5819672131147542, 'no_speech_prob': 0.0016481098718941212}, {'id': 181, 'seek': 105236, 'start': 1052.36, 'end': 1058.6799999999998, 'text': ' the neural network through before we return it to the next neuron in the pipeline, right? So one', 'tokens': [50364, 264, 18161, 3209, 807, 949, 321, 2736, 309, 281, 264, 958, 34090, 294, 264, 15517, 11, 558, 30, 407, 472, 50680], 'temperature': 0.0, 'avg_logprob': -0.09682860494661732, 'compression_ratio': 1.7581227436823104, 'no_speech_prob': 0.0003981117042712867}, {'id': 182, 'seek': 105236, 'start': 1058.6799999999998, 'end': 1063.08, 'text': \" common example of a nonlinear function that's very popular in deep neural networks is called the\", 'tokens': [50680, 2689, 1365, 295, 257, 2107, 28263, 2445, 300, 311, 588, 3743, 294, 2452, 18161, 9590, 307, 1219, 264, 50900], 'temperature': 0.0, 'avg_logprob': -0.09682860494661732, 'compression_ratio': 1.7581227436823104, 'no_speech_prob': 0.0003981117042712867}, {'id': 183, 'seek': 105236, 'start': 1063.08, 'end': 1067.4799999999998, 'text': ' sigmoid function. You can think of this as kind of a continuous version of a threshold function,', 'tokens': [50900, 4556, 3280, 327, 2445, 13, 509, 393, 519, 295, 341, 382, 733, 295, 257, 10957, 3037, 295, 257, 14678, 2445, 11, 51120], 'temperature': 0.0, 'avg_logprob': -0.09682860494661732, 'compression_ratio': 1.7581227436823104, 'no_speech_prob': 0.0003981117042712867}, {'id': 184, 'seek': 105236, 'start': 1067.4799999999998, 'end': 1073.4799999999998, 'text': \" right? It goes from zero to one, and it's having, it can take as input any real number on the real\", 'tokens': [51120, 558, 30, 467, 1709, 490, 4018, 281, 472, 11, 293, 309, 311, 1419, 11, 309, 393, 747, 382, 4846, 604, 957, 1230, 322, 264, 957, 51420], 'temperature': 0.0, 'avg_logprob': -0.09682860494661732, 'compression_ratio': 1.7581227436823104, 'no_speech_prob': 0.0003981117042712867}, {'id': 185, 'seek': 105236, 'start': 1073.4799999999998, 'end': 1078.6, 'text': ' number line, and you can see an example of it illustrated on the bottom right hand. Now, in fact,', 'tokens': [51420, 1230, 1622, 11, 293, 291, 393, 536, 364, 1365, 295, 309, 33875, 322, 264, 2767, 558, 1011, 13, 823, 11, 294, 1186, 11, 51676], 'temperature': 0.0, 'avg_logprob': -0.09682860494661732, 'compression_ratio': 1.7581227436823104, 'no_speech_prob': 0.0003981117042712867}, {'id': 186, 'seek': 107860, 'start': 1078.6, 'end': 1083.3999999999999, 'text': ' there are many types of nonlinear activation functions that are popular in deep neural networks,', 'tokens': [50364, 456, 366, 867, 3467, 295, 2107, 28263, 24433, 6828, 300, 366, 3743, 294, 2452, 18161, 9590, 11, 50604], 'temperature': 0.0, 'avg_logprob': -0.09088572554700956, 'compression_ratio': 1.8081761006289307, 'no_speech_prob': 0.0025896648876369}, {'id': 187, 'seek': 107860, 'start': 1083.3999999999999, 'end': 1088.04, 'text': \" and here are some common ones. Throughout this presentation, you'll actually see some examples of\", 'tokens': [50604, 293, 510, 366, 512, 2689, 2306, 13, 22775, 341, 5860, 11, 291, 603, 767, 536, 512, 5110, 295, 50836], 'temperature': 0.0, 'avg_logprob': -0.09088572554700956, 'compression_ratio': 1.8081761006289307, 'no_speech_prob': 0.0025896648876369}, {'id': 188, 'seek': 107860, 'start': 1088.04, 'end': 1093.0, 'text': \" these code snippets on the bottom of the slides, where we'll try and actually tie in some of what\", 'tokens': [50836, 613, 3089, 35623, 1385, 322, 264, 2767, 295, 264, 9788, 11, 689, 321, 603, 853, 293, 767, 7582, 294, 512, 295, 437, 51084], 'temperature': 0.0, 'avg_logprob': -0.09088572554700956, 'compression_ratio': 1.8081761006289307, 'no_speech_prob': 0.0025896648876369}, {'id': 189, 'seek': 107860, 'start': 1093.0, 'end': 1097.6399999999999, 'text': \" you're learning in the lectures to actual software and how you can implement these pieces,\", 'tokens': [51084, 291, 434, 2539, 294, 264, 16564, 281, 3539, 4722, 293, 577, 291, 393, 4445, 613, 3755, 11, 51316], 'temperature': 0.0, 'avg_logprob': -0.09088572554700956, 'compression_ratio': 1.8081761006289307, 'no_speech_prob': 0.0025896648876369}, {'id': 190, 'seek': 107860, 'start': 1097.6399999999999, 'end': 1101.8799999999999, 'text': ' which will help you a lot for your software labs explicitly. The sigmoid activation on the left', 'tokens': [51316, 597, 486, 854, 291, 257, 688, 337, 428, 4722, 20339, 20803, 13, 440, 4556, 3280, 327, 24433, 322, 264, 1411, 51528], 'temperature': 0.0, 'avg_logprob': -0.09088572554700956, 'compression_ratio': 1.8081761006289307, 'no_speech_prob': 0.0025896648876369}, {'id': 191, 'seek': 107860, 'start': 1101.8799999999999, 'end': 1106.6799999999998, 'text': \" is very popular since it's a function that outputs between zero and one, so especially when you\", 'tokens': [51528, 307, 588, 3743, 1670, 309, 311, 257, 2445, 300, 23930, 1296, 4018, 293, 472, 11, 370, 2318, 562, 291, 51768], 'temperature': 0.0, 'avg_logprob': -0.09088572554700956, 'compression_ratio': 1.8081761006289307, 'no_speech_prob': 0.0025896648876369}, {'id': 192, 'seek': 110668, 'start': 1106.68, 'end': 1110.76, 'text': ' want to deal with probability distributions, for example, this is very important because', 'tokens': [50364, 528, 281, 2028, 365, 8482, 37870, 11, 337, 1365, 11, 341, 307, 588, 1021, 570, 50568], 'temperature': 0.0, 'avg_logprob': -0.15243039041195275, 'compression_ratio': 1.6890459363957597, 'no_speech_prob': 9.312834299635142e-05}, {'id': 193, 'seek': 110668, 'start': 1110.76, 'end': 1116.1200000000001, 'text': ' probabilities live between zero and one. In modern deep neural networks, though, the relu function,', 'tokens': [50568, 33783, 1621, 1296, 4018, 293, 472, 13, 682, 4363, 2452, 18161, 9590, 11, 1673, 11, 264, 1039, 84, 2445, 11, 50836], 'temperature': 0.0, 'avg_logprob': -0.15243039041195275, 'compression_ratio': 1.6890459363957597, 'no_speech_prob': 9.312834299635142e-05}, {'id': 194, 'seek': 110668, 'start': 1116.1200000000001, 'end': 1120.04, 'text': \" which you can see on the far right hand is a very popular activation function because it's\", 'tokens': [50836, 597, 291, 393, 536, 322, 264, 1400, 558, 1011, 307, 257, 588, 3743, 24433, 2445, 570, 309, 311, 51032], 'temperature': 0.0, 'avg_logprob': -0.15243039041195275, 'compression_ratio': 1.6890459363957597, 'no_speech_prob': 9.312834299635142e-05}, {'id': 195, 'seek': 110668, 'start': 1120.04, 'end': 1124.68, 'text': \" piecewise linear. It's extremely efficient to compute, especially when computing it's derivatives,\", 'tokens': [51032, 2522, 3711, 8213, 13, 467, 311, 4664, 7148, 281, 14722, 11, 2318, 562, 15866, 309, 311, 33733, 11, 51264], 'temperature': 0.0, 'avg_logprob': -0.15243039041195275, 'compression_ratio': 1.6890459363957597, 'no_speech_prob': 9.312834299635142e-05}, {'id': 196, 'seek': 110668, 'start': 1124.68, 'end': 1133.0800000000002, 'text': \" right? It's derivatives are constants, except for one nonlinear yet, zero. Now, I hope actually all\", 'tokens': [51264, 558, 30, 467, 311, 33733, 366, 35870, 11, 3993, 337, 472, 2107, 28263, 1939, 11, 4018, 13, 823, 11, 286, 1454, 767, 439, 51684], 'temperature': 0.0, 'avg_logprob': -0.15243039041195275, 'compression_ratio': 1.6890459363957597, 'no_speech_prob': 9.312834299635142e-05}, {'id': 197, 'seek': 113308, 'start': 1133.1599999999999, 'end': 1137.1599999999999, 'text': ' of you are probably asking this question to yourself of why do we even need this nonlinear activation', 'tokens': [50368, 295, 291, 366, 1391, 3365, 341, 1168, 281, 1803, 295, 983, 360, 321, 754, 643, 341, 2107, 28263, 24433, 50568], 'temperature': 0.0, 'avg_logprob': -0.09917918566999764, 'compression_ratio': 1.7706093189964158, 'no_speech_prob': 0.008057420141994953}, {'id': 198, 'seek': 113308, 'start': 1137.1599999999999, 'end': 1140.9199999999998, 'text': \" function? It seems like it kind of just complicates this whole picture when we didn't really need it\", 'tokens': [50568, 2445, 30, 467, 2544, 411, 309, 733, 295, 445, 16060, 1024, 341, 1379, 3036, 562, 321, 994, 380, 534, 643, 309, 50756], 'temperature': 0.0, 'avg_logprob': -0.09917918566999764, 'compression_ratio': 1.7706093189964158, 'no_speech_prob': 0.008057420141994953}, {'id': 199, 'seek': 113308, 'start': 1140.9199999999998, 'end': 1146.52, 'text': ' in the first place. I want to just spend a moment on answering this because the point of a nonlinear', 'tokens': [50756, 294, 264, 700, 1081, 13, 286, 528, 281, 445, 3496, 257, 1623, 322, 13430, 341, 570, 264, 935, 295, 257, 2107, 28263, 51036], 'temperature': 0.0, 'avg_logprob': -0.09917918566999764, 'compression_ratio': 1.7706093189964158, 'no_speech_prob': 0.008057420141994953}, {'id': 200, 'seek': 113308, 'start': 1146.52, 'end': 1152.52, 'text': ' activation function is, of course, number one is to introduce nonlinearities to our data, right?', 'tokens': [51036, 24433, 2445, 307, 11, 295, 1164, 11, 1230, 472, 307, 281, 5366, 2107, 28263, 1088, 281, 527, 1412, 11, 558, 30, 51336], 'temperature': 0.0, 'avg_logprob': -0.09917918566999764, 'compression_ratio': 1.7706093189964158, 'no_speech_prob': 0.008057420141994953}, {'id': 201, 'seek': 113308, 'start': 1152.52, 'end': 1157.8799999999999, 'text': ' If we think about our data, almost all data that we care about, all real world data is highly', 'tokens': [51336, 759, 321, 519, 466, 527, 1412, 11, 1920, 439, 1412, 300, 321, 1127, 466, 11, 439, 957, 1002, 1412, 307, 5405, 51604], 'temperature': 0.0, 'avg_logprob': -0.09917918566999764, 'compression_ratio': 1.7706093189964158, 'no_speech_prob': 0.008057420141994953}, {'id': 202, 'seek': 115788, 'start': 1157.88, 'end': 1162.5200000000002, 'text': ' nonlinear. Now, this is important because if we want to be able to deal with those types of', 'tokens': [50364, 2107, 28263, 13, 823, 11, 341, 307, 1021, 570, 498, 321, 528, 281, 312, 1075, 281, 2028, 365, 729, 3467, 295, 50596], 'temperature': 0.0, 'avg_logprob': -0.09219784350008578, 'compression_ratio': 1.8006134969325154, 'no_speech_prob': 0.008441310375928879}, {'id': 203, 'seek': 115788, 'start': 1162.5200000000002, 'end': 1167.24, 'text': ' data sets, we need models that are also nonlinear so they can capture those same types of patterns.', 'tokens': [50596, 1412, 6352, 11, 321, 643, 5245, 300, 366, 611, 2107, 28263, 370, 436, 393, 7983, 729, 912, 3467, 295, 8294, 13, 50832], 'temperature': 0.0, 'avg_logprob': -0.09219784350008578, 'compression_ratio': 1.8006134969325154, 'no_speech_prob': 0.008441310375928879}, {'id': 204, 'seek': 115788, 'start': 1167.24, 'end': 1171.5600000000002, 'text': ' So imagine I told you to separate, for example, I gave you this data set, red points from green points,', 'tokens': [50832, 407, 3811, 286, 1907, 291, 281, 4994, 11, 337, 1365, 11, 286, 2729, 291, 341, 1412, 992, 11, 2182, 2793, 490, 3092, 2793, 11, 51048], 'temperature': 0.0, 'avg_logprob': -0.09219784350008578, 'compression_ratio': 1.8006134969325154, 'no_speech_prob': 0.008441310375928879}, {'id': 205, 'seek': 115788, 'start': 1171.5600000000002, 'end': 1176.44, 'text': ' and I asked you to try and separate those two types of data points. Now, you might think that', 'tokens': [51048, 293, 286, 2351, 291, 281, 853, 293, 4994, 729, 732, 3467, 295, 1412, 2793, 13, 823, 11, 291, 1062, 519, 300, 51292], 'temperature': 0.0, 'avg_logprob': -0.09219784350008578, 'compression_ratio': 1.8006134969325154, 'no_speech_prob': 0.008441310375928879}, {'id': 206, 'seek': 115788, 'start': 1176.44, 'end': 1181.0800000000002, 'text': ' this is easy, but what if I could only, if I told you that you could only use a single line to do', 'tokens': [51292, 341, 307, 1858, 11, 457, 437, 498, 286, 727, 787, 11, 498, 286, 1907, 291, 300, 291, 727, 787, 764, 257, 2167, 1622, 281, 360, 51524], 'temperature': 0.0, 'avg_logprob': -0.09219784350008578, 'compression_ratio': 1.8006134969325154, 'no_speech_prob': 0.008441310375928879}, {'id': 207, 'seek': 115788, 'start': 1181.0800000000002, 'end': 1185.4, 'text': \" so? Well, now it becomes a very complicated problem. In fact, you can't really solve it effectively\", 'tokens': [51524, 370, 30, 1042, 11, 586, 309, 3643, 257, 588, 6179, 1154, 13, 682, 1186, 11, 291, 393, 380, 534, 5039, 309, 8659, 51740], 'temperature': 0.0, 'avg_logprob': -0.09219784350008578, 'compression_ratio': 1.8006134969325154, 'no_speech_prob': 0.008441310375928879}, {'id': 208, 'seek': 118540, 'start': 1185.48, 'end': 1192.1200000000001, 'text': ' with a single line. And in fact, if you introduce nonlinear activation functions to your solution,', 'tokens': [50368, 365, 257, 2167, 1622, 13, 400, 294, 1186, 11, 498, 291, 5366, 2107, 28263, 24433, 6828, 281, 428, 3827, 11, 50700], 'temperature': 0.0, 'avg_logprob': -0.07082020111803738, 'compression_ratio': 1.7686567164179106, 'no_speech_prob': 0.0022164464462548494}, {'id': 209, 'seek': 118540, 'start': 1192.1200000000001, 'end': 1197.72, 'text': \" that's exactly what allows you to deal with these types of problems. Nonlinear activation\", 'tokens': [50700, 300, 311, 2293, 437, 4045, 291, 281, 2028, 365, 613, 3467, 295, 2740, 13, 8774, 28263, 24433, 50980], 'temperature': 0.0, 'avg_logprob': -0.07082020111803738, 'compression_ratio': 1.7686567164179106, 'no_speech_prob': 0.0022164464462548494}, {'id': 210, 'seek': 118540, 'start': 1197.72, 'end': 1203.16, 'text': \" functions allow you to deal with nonlinear types of data. Now, and that's what exactly makes\", 'tokens': [50980, 6828, 2089, 291, 281, 2028, 365, 2107, 28263, 3467, 295, 1412, 13, 823, 11, 293, 300, 311, 437, 2293, 1669, 51252], 'temperature': 0.0, 'avg_logprob': -0.07082020111803738, 'compression_ratio': 1.7686567164179106, 'no_speech_prob': 0.0022164464462548494}, {'id': 211, 'seek': 118540, 'start': 1203.16, 'end': 1208.68, 'text': \" neural networks so powerful at their core. So let's understand this maybe with a very simple\", 'tokens': [51252, 18161, 9590, 370, 4005, 412, 641, 4965, 13, 407, 718, 311, 1223, 341, 1310, 365, 257, 588, 2199, 51528], 'temperature': 0.0, 'avg_logprob': -0.07082020111803738, 'compression_ratio': 1.7686567164179106, 'no_speech_prob': 0.0022164464462548494}, {'id': 212, 'seek': 118540, 'start': 1208.68, 'end': 1214.1200000000001, 'text': ' example walking through this diagram of a perceptron one more time. Imagine I give you this trained', 'tokens': [51528, 1365, 4494, 807, 341, 10686, 295, 257, 43276, 2044, 472, 544, 565, 13, 11739, 286, 976, 291, 341, 8895, 51800], 'temperature': 0.0, 'avg_logprob': -0.07082020111803738, 'compression_ratio': 1.7686567164179106, 'no_speech_prob': 0.0022164464462548494}, {'id': 213, 'seek': 121412, 'start': 1214.12, 'end': 1219.6399999999999, 'text': \" neural network with weights now, not W1, W2, I'm going to actually give you numbers at these locations.\", 'tokens': [50364, 18161, 3209, 365, 17443, 586, 11, 406, 343, 16, 11, 343, 17, 11, 286, 478, 516, 281, 767, 976, 291, 3547, 412, 613, 9253, 13, 50640], 'temperature': 0.0, 'avg_logprob': -0.13779905759371244, 'compression_ratio': 1.7426470588235294, 'no_speech_prob': 0.0065856450237333775}, {'id': 214, 'seek': 121412, 'start': 1219.6399999999999, 'end': 1226.36, 'text': ' So the trained weights, W0 will be 1, and W will be a vector of 3 and negative 2.', 'tokens': [50640, 407, 264, 8895, 17443, 11, 343, 15, 486, 312, 502, 11, 293, 343, 486, 312, 257, 8062, 295, 805, 293, 3671, 568, 13, 50976], 'temperature': 0.0, 'avg_logprob': -0.13779905759371244, 'compression_ratio': 1.7426470588235294, 'no_speech_prob': 0.0065856450237333775}, {'id': 215, 'seek': 121412, 'start': 1227.1599999999999, 'end': 1232.36, 'text': ' So this neural network has two inputs, like we said before, it has input x1 and has input x2.', 'tokens': [51016, 407, 341, 18161, 3209, 575, 732, 15743, 11, 411, 321, 848, 949, 11, 309, 575, 4846, 2031, 16, 293, 575, 4846, 2031, 17, 13, 51276], 'temperature': 0.0, 'avg_logprob': -0.13779905759371244, 'compression_ratio': 1.7426470588235294, 'no_speech_prob': 0.0065856450237333775}, {'id': 216, 'seek': 121412, 'start': 1232.9199999999998, 'end': 1237.4799999999998, 'text': ' If we want to get the output of it, this is also the main thing I want all of you to take away from', 'tokens': [51304, 759, 321, 528, 281, 483, 264, 5598, 295, 309, 11, 341, 307, 611, 264, 2135, 551, 286, 528, 439, 295, 291, 281, 747, 1314, 490, 51532], 'temperature': 0.0, 'avg_logprob': -0.13779905759371244, 'compression_ratio': 1.7426470588235294, 'no_speech_prob': 0.0065856450237333775}, {'id': 217, 'seek': 121412, 'start': 1237.4799999999998, 'end': 1241.3999999999999, 'text': ' this lecture today is that to get the output of a perceptron, there are three steps we need to', 'tokens': [51532, 341, 7991, 965, 307, 300, 281, 483, 264, 5598, 295, 257, 43276, 2044, 11, 456, 366, 1045, 4439, 321, 643, 281, 51728], 'temperature': 0.0, 'avg_logprob': -0.13779905759371244, 'compression_ratio': 1.7426470588235294, 'no_speech_prob': 0.0065856450237333775}, {'id': 218, 'seek': 124140, 'start': 1241.4, 'end': 1246.6000000000001, 'text': ' take right from this stage. We first compute the multiplication of our inputs with our weights.', 'tokens': [50364, 747, 558, 490, 341, 3233, 13, 492, 700, 14722, 264, 27290, 295, 527, 15743, 365, 527, 17443, 13, 50624], 'temperature': 0.0, 'avg_logprob': -0.16161720232031812, 'compression_ratio': 1.6293103448275863, 'no_speech_prob': 0.0009105986682698131}, {'id': 219, 'seek': 124140, 'start': 1248.6000000000001, 'end': 1255.0, 'text': \" Sorry, yeah multiply them together, add their result and compute a nonlinearity. It's these three\", 'tokens': [50724, 4919, 11, 1338, 12972, 552, 1214, 11, 909, 641, 1874, 293, 14722, 257, 2107, 1889, 17409, 13, 467, 311, 613, 1045, 51044], 'temperature': 0.0, 'avg_logprob': -0.16161720232031812, 'compression_ratio': 1.6293103448275863, 'no_speech_prob': 0.0009105986682698131}, {'id': 220, 'seek': 124140, 'start': 1255.0, 'end': 1261.72, 'text': \" steps that define the forward propagation of information through a perceptron. So let's take a\", 'tokens': [51044, 4439, 300, 6964, 264, 2128, 38377, 295, 1589, 807, 257, 43276, 2044, 13, 407, 718, 311, 747, 257, 51380], 'temperature': 0.0, 'avg_logprob': -0.16161720232031812, 'compression_ratio': 1.6293103448275863, 'no_speech_prob': 0.0009105986682698131}, {'id': 221, 'seek': 124140, 'start': 1261.72, 'end': 1267.24, 'text': ' look at how that exactly works, right? So if we plug in these numbers to those equations,', 'tokens': [51380, 574, 412, 577, 300, 2293, 1985, 11, 558, 30, 407, 498, 321, 5452, 294, 613, 3547, 281, 729, 11787, 11, 51656], 'temperature': 0.0, 'avg_logprob': -0.16161720232031812, 'compression_ratio': 1.6293103448275863, 'no_speech_prob': 0.0009105986682698131}, {'id': 222, 'seek': 126724, 'start': 1267.88, 'end': 1272.68, 'text': ' we can see that everything inside of our nonlinearity, here the nonlinearity is G,', 'tokens': [50396, 321, 393, 536, 300, 1203, 1854, 295, 527, 2107, 1889, 17409, 11, 510, 264, 2107, 1889, 17409, 307, 460, 11, 50636], 'temperature': 0.0, 'avg_logprob': -0.16366685846800444, 'compression_ratio': 1.6772727272727272, 'no_speech_prob': 0.002589397830888629}, {'id': 223, 'seek': 126724, 'start': 1273.4, 'end': 1280.1200000000001, 'text': ' that function G, which could be a sigmoid, you saw a previous slide. That component inside our', 'tokens': [50672, 300, 2445, 460, 11, 597, 727, 312, 257, 4556, 3280, 327, 11, 291, 1866, 257, 3894, 4137, 13, 663, 6542, 1854, 527, 51008], 'temperature': 0.0, 'avg_logprob': -0.16366685846800444, 'compression_ratio': 1.6772727272727272, 'no_speech_prob': 0.002589397830888629}, {'id': 224, 'seek': 126724, 'start': 1280.76, 'end': 1286.36, 'text': ' nonlinearity is in fact just a two-dimensional line, it has two inputs and if we consider the space', 'tokens': [51040, 2107, 1889, 17409, 307, 294, 1186, 445, 257, 732, 12, 18759, 1622, 11, 309, 575, 732, 15743, 293, 498, 321, 1949, 264, 1901, 51320], 'temperature': 0.0, 'avg_logprob': -0.16366685846800444, 'compression_ratio': 1.6772727272727272, 'no_speech_prob': 0.002589397830888629}, {'id': 225, 'seek': 126724, 'start': 1286.36, 'end': 1291.64, 'text': ' of all of the possible inputs that this neural network could see, we can actually plot this', 'tokens': [51320, 295, 439, 295, 264, 1944, 15743, 300, 341, 18161, 3209, 727, 536, 11, 321, 393, 767, 7542, 341, 51584], 'temperature': 0.0, 'avg_logprob': -0.16366685846800444, 'compression_ratio': 1.6772727272727272, 'no_speech_prob': 0.002589397830888629}, {'id': 226, 'seek': 129164, 'start': 1291.72, 'end': 1298.6000000000001, 'text': ' on a decision boundary, right? We can plot this two-dimensional line as a decision boundary,', 'tokens': [50368, 322, 257, 3537, 12866, 11, 558, 30, 492, 393, 7542, 341, 732, 12, 18759, 1622, 382, 257, 3537, 12866, 11, 50712], 'temperature': 0.0, 'avg_logprob': -0.08753825823465983, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.002799669047817588}, {'id': 227, 'seek': 129164, 'start': 1298.6000000000001, 'end': 1305.0, 'text': ' as a plane separating these two components of our space. In fact, not only is it a single plane,', 'tokens': [50712, 382, 257, 5720, 29279, 613, 732, 6677, 295, 527, 1901, 13, 682, 1186, 11, 406, 787, 307, 309, 257, 2167, 5720, 11, 51032], 'temperature': 0.0, 'avg_logprob': -0.08753825823465983, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.002799669047817588}, {'id': 228, 'seek': 129164, 'start': 1305.0, 'end': 1309.16, 'text': \" there's a directionality component depending on which side of the plane that we live on.\", 'tokens': [51032, 456, 311, 257, 3513, 1860, 6542, 5413, 322, 597, 1252, 295, 264, 5720, 300, 321, 1621, 322, 13, 51240], 'temperature': 0.0, 'avg_logprob': -0.08753825823465983, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.002799669047817588}, {'id': 229, 'seek': 129164, 'start': 1309.16, 'end': 1315.0, 'text': ' If we see an input, for example, here, negative one, two, we actually know that it lives on one', 'tokens': [51240, 759, 321, 536, 364, 4846, 11, 337, 1365, 11, 510, 11, 3671, 472, 11, 732, 11, 321, 767, 458, 300, 309, 2909, 322, 472, 51532], 'temperature': 0.0, 'avg_logprob': -0.08753825823465983, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.002799669047817588}, {'id': 230, 'seek': 129164, 'start': 1315.0, 'end': 1320.1200000000001, 'text': ' side of the plane and it will have a certain type of output. In this case, that output is going to be', 'tokens': [51532, 1252, 295, 264, 5720, 293, 309, 486, 362, 257, 1629, 2010, 295, 5598, 13, 682, 341, 1389, 11, 300, 5598, 307, 516, 281, 312, 51788], 'temperature': 0.0, 'avg_logprob': -0.08753825823465983, 'compression_ratio': 1.7827715355805243, 'no_speech_prob': 0.002799669047817588}, {'id': 231, 'seek': 132012, 'start': 1320.76, 'end': 1325.7199999999998, 'text': ' positive, right? Because in this case, when we plug those components into our equation,', 'tokens': [50396, 3353, 11, 558, 30, 1436, 294, 341, 1389, 11, 562, 321, 5452, 729, 6677, 666, 527, 5367, 11, 50644], 'temperature': 0.0, 'avg_logprob': -0.0736963532187722, 'compression_ratio': 1.8160919540229885, 'no_speech_prob': 0.0011510145850479603}, {'id': 232, 'seek': 132012, 'start': 1325.7199999999998, 'end': 1331.3999999999999, 'text': \" we'll get a positive number that passes through the nonlinearity component and that gets propagated\", 'tokens': [50644, 321, 603, 483, 257, 3353, 1230, 300, 11335, 807, 264, 2107, 1889, 17409, 6542, 293, 300, 2170, 12425, 770, 50928], 'temperature': 0.0, 'avg_logprob': -0.0736963532187722, 'compression_ratio': 1.8160919540229885, 'no_speech_prob': 0.0011510145850479603}, {'id': 233, 'seek': 132012, 'start': 1331.3999999999999, 'end': 1336.76, 'text': \" through as well. Of course, if you're on the other side of the space, you're going to have the opposite\", 'tokens': [50928, 807, 382, 731, 13, 2720, 1164, 11, 498, 291, 434, 322, 264, 661, 1252, 295, 264, 1901, 11, 291, 434, 516, 281, 362, 264, 6182, 51196], 'temperature': 0.0, 'avg_logprob': -0.0736963532187722, 'compression_ratio': 1.8160919540229885, 'no_speech_prob': 0.0011510145850479603}, {'id': 234, 'seek': 132012, 'start': 1336.76, 'end': 1341.56, 'text': ' result, right? And that thresholding function is going to essentially live at this decision', 'tokens': [51196, 1874, 11, 558, 30, 400, 300, 14678, 278, 2445, 307, 516, 281, 4476, 1621, 412, 341, 3537, 51436], 'temperature': 0.0, 'avg_logprob': -0.0736963532187722, 'compression_ratio': 1.8160919540229885, 'no_speech_prob': 0.0011510145850479603}, {'id': 235, 'seek': 132012, 'start': 1341.56, 'end': 1345.6399999999999, 'text': ' boundary. So depending on which side of the space you live on, that thresholding function,', 'tokens': [51436, 12866, 13, 407, 5413, 322, 597, 1252, 295, 264, 1901, 291, 1621, 322, 11, 300, 14678, 278, 2445, 11, 51640], 'temperature': 0.0, 'avg_logprob': -0.0736963532187722, 'compression_ratio': 1.8160919540229885, 'no_speech_prob': 0.0011510145850479603}, {'id': 236, 'seek': 134564, 'start': 1345.64, 'end': 1350.76, 'text': ' that sigmoid function, is going to then control how you move to one side of the other.', 'tokens': [50364, 300, 4556, 3280, 327, 2445, 11, 307, 516, 281, 550, 1969, 577, 291, 1286, 281, 472, 1252, 295, 264, 661, 13, 50620], 'temperature': 0.0, 'avg_logprob': -0.07659532256045584, 'compression_ratio': 1.6642599277978338, 'no_speech_prob': 0.0013453656574711204}, {'id': 237, 'seek': 134564, 'start': 1352.5200000000002, 'end': 1357.4, 'text': ' Now, in this particular example, this is very convenient, right? Because we can actually visualize,', 'tokens': [50708, 823, 11, 294, 341, 1729, 1365, 11, 341, 307, 588, 10851, 11, 558, 30, 1436, 321, 393, 767, 23273, 11, 50952], 'temperature': 0.0, 'avg_logprob': -0.07659532256045584, 'compression_ratio': 1.6642599277978338, 'no_speech_prob': 0.0013453656574711204}, {'id': 238, 'seek': 134564, 'start': 1357.4, 'end': 1362.3600000000001, 'text': \" and I can draw this exact full space for you on this slide. It's only a two-dimensional space,\", 'tokens': [50952, 293, 286, 393, 2642, 341, 1900, 1577, 1901, 337, 291, 322, 341, 4137, 13, 467, 311, 787, 257, 732, 12, 18759, 1901, 11, 51200], 'temperature': 0.0, 'avg_logprob': -0.07659532256045584, 'compression_ratio': 1.6642599277978338, 'no_speech_prob': 0.0013453656574711204}, {'id': 239, 'seek': 134564, 'start': 1362.3600000000001, 'end': 1367.96, 'text': \" so it's very easy for us to visualize. But of course, for almost all problems that we care about,\", 'tokens': [51200, 370, 309, 311, 588, 1858, 337, 505, 281, 23273, 13, 583, 295, 1164, 11, 337, 1920, 439, 2740, 300, 321, 1127, 466, 11, 51480], 'temperature': 0.0, 'avg_logprob': -0.07659532256045584, 'compression_ratio': 1.6642599277978338, 'no_speech_prob': 0.0013453656574711204}, {'id': 240, 'seek': 134564, 'start': 1367.96, 'end': 1371.88, 'text': ' our data points are not going to be two-dimensional. If you think about an image,', 'tokens': [51480, 527, 1412, 2793, 366, 406, 516, 281, 312, 732, 12, 18759, 13, 759, 291, 519, 466, 364, 3256, 11, 51676], 'temperature': 0.0, 'avg_logprob': -0.07659532256045584, 'compression_ratio': 1.6642599277978338, 'no_speech_prob': 0.0013453656574711204}, {'id': 241, 'seek': 137188, 'start': 1372.44, 'end': 1376.6000000000001, 'text': ' the dimensionality of an image is going to be the number of pixels that you have in the image,', 'tokens': [50392, 264, 10139, 1860, 295, 364, 3256, 307, 516, 281, 312, 264, 1230, 295, 18668, 300, 291, 362, 294, 264, 3256, 11, 50600], 'temperature': 0.0, 'avg_logprob': -0.09013956387837728, 'compression_ratio': 1.7591240875912408, 'no_speech_prob': 0.003270345041528344}, {'id': 242, 'seek': 137188, 'start': 1376.6000000000001, 'end': 1381.0, 'text': ' right? So these are going to be thousands of dimensions, millions of dimensions, or even more.', 'tokens': [50600, 558, 30, 407, 613, 366, 516, 281, 312, 5383, 295, 12819, 11, 6803, 295, 12819, 11, 420, 754, 544, 13, 50820], 'temperature': 0.0, 'avg_logprob': -0.09013956387837728, 'compression_ratio': 1.7591240875912408, 'no_speech_prob': 0.003270345041528344}, {'id': 243, 'seek': 137188, 'start': 1381.64, 'end': 1387.0, 'text': \" And then drawing these types of plots, like you see here, is simply not feasible, right? So we can't\", 'tokens': [50852, 400, 550, 6316, 613, 3467, 295, 28609, 11, 411, 291, 536, 510, 11, 307, 2935, 406, 26648, 11, 558, 30, 407, 321, 393, 380, 51120], 'temperature': 0.0, 'avg_logprob': -0.09013956387837728, 'compression_ratio': 1.7591240875912408, 'no_speech_prob': 0.003270345041528344}, {'id': 244, 'seek': 137188, 'start': 1387.0, 'end': 1391.96, 'text': ' always do this, but hopefully this gives you some intuition to understand, kind of as we build up', 'tokens': [51120, 1009, 360, 341, 11, 457, 4696, 341, 2709, 291, 512, 24002, 281, 1223, 11, 733, 295, 382, 321, 1322, 493, 51368], 'temperature': 0.0, 'avg_logprob': -0.09013956387837728, 'compression_ratio': 1.7591240875912408, 'no_speech_prob': 0.003270345041528344}, {'id': 245, 'seek': 137188, 'start': 1391.96, 'end': 1398.0400000000002, 'text': \" into more complex models. So now that we have an idea of the perceptron, let's see how we can\", 'tokens': [51368, 666, 544, 3997, 5245, 13, 407, 586, 300, 321, 362, 364, 1558, 295, 264, 43276, 2044, 11, 718, 311, 536, 577, 321, 393, 51672], 'temperature': 0.0, 'avg_logprob': -0.09013956387837728, 'compression_ratio': 1.7591240875912408, 'no_speech_prob': 0.003270345041528344}, {'id': 246, 'seek': 139804, 'start': 1398.04, 'end': 1401.8799999999999, 'text': ' actually take this single neuron and start to build it up into something more complicated,', 'tokens': [50364, 767, 747, 341, 2167, 34090, 293, 722, 281, 1322, 309, 493, 666, 746, 544, 6179, 11, 50556], 'temperature': 0.0, 'avg_logprob': -0.07167428887408712, 'compression_ratio': 1.7056737588652482, 'no_speech_prob': 0.0019550772849470377}, {'id': 247, 'seek': 139804, 'start': 1401.8799999999999, 'end': 1407.56, 'text': \" a full neural network, and build a model from that. So let's revisit, again, this previous diagram\", 'tokens': [50556, 257, 1577, 18161, 3209, 11, 293, 1322, 257, 2316, 490, 300, 13, 407, 718, 311, 32676, 11, 797, 11, 341, 3894, 10686, 50840], 'temperature': 0.0, 'avg_logprob': -0.07167428887408712, 'compression_ratio': 1.7056737588652482, 'no_speech_prob': 0.0019550772849470377}, {'id': 248, 'seek': 139804, 'start': 1407.56, 'end': 1413.32, 'text': ' of the perceptron. If, again, just to reiterate one more time, this core piece of information that I', 'tokens': [50840, 295, 264, 43276, 2044, 13, 759, 11, 797, 11, 445, 281, 33528, 472, 544, 565, 11, 341, 4965, 2522, 295, 1589, 300, 286, 51128], 'temperature': 0.0, 'avg_logprob': -0.07167428887408712, 'compression_ratio': 1.7056737588652482, 'no_speech_prob': 0.0019550772849470377}, {'id': 249, 'seek': 139804, 'start': 1413.32, 'end': 1418.92, 'text': ' want all of you to take away from this class is how a perceptron works and how it propagates', 'tokens': [51128, 528, 439, 295, 291, 281, 747, 1314, 490, 341, 1508, 307, 577, 257, 43276, 2044, 1985, 293, 577, 309, 12425, 1024, 51408], 'temperature': 0.0, 'avg_logprob': -0.07167428887408712, 'compression_ratio': 1.7056737588652482, 'no_speech_prob': 0.0019550772849470377}, {'id': 250, 'seek': 139804, 'start': 1418.92, 'end': 1424.92, 'text': ' information to its decision. There are three steps. First is the dot product, second is the bias,', 'tokens': [51408, 1589, 281, 1080, 3537, 13, 821, 366, 1045, 4439, 13, 2386, 307, 264, 5893, 1674, 11, 1150, 307, 264, 12577, 11, 51708], 'temperature': 0.0, 'avg_logprob': -0.07167428887408712, 'compression_ratio': 1.7056737588652482, 'no_speech_prob': 0.0019550772849470377}, {'id': 251, 'seek': 142492, 'start': 1424.92, 'end': 1429.48, 'text': ' and third is the non-miniarity. And you keep repeating this process for every single perceptron', 'tokens': [50364, 293, 2636, 307, 264, 2107, 12, 2367, 72, 17409, 13, 400, 291, 1066, 18617, 341, 1399, 337, 633, 2167, 43276, 2044, 50592], 'temperature': 0.0, 'avg_logprob': -0.10328298651653788, 'compression_ratio': 1.754646840148699, 'no_speech_prob': 0.0009542157058604062}, {'id': 252, 'seek': 142492, 'start': 1429.48, 'end': 1434.92, 'text': \" in your neural network. Let's simplify the diagram a little bit. I'll get rid of the weights,\", 'tokens': [50592, 294, 428, 18161, 3209, 13, 961, 311, 20460, 264, 10686, 257, 707, 857, 13, 286, 603, 483, 3973, 295, 264, 17443, 11, 50864], 'temperature': 0.0, 'avg_logprob': -0.10328298651653788, 'compression_ratio': 1.754646840148699, 'no_speech_prob': 0.0009542157058604062}, {'id': 253, 'seek': 142492, 'start': 1435.64, 'end': 1439.88, 'text': \" and you can assume that every line here now basically has an associated weight scalar that's\", 'tokens': [50900, 293, 291, 393, 6552, 300, 633, 1622, 510, 586, 1936, 575, 364, 6615, 3364, 39684, 300, 311, 51112], 'temperature': 0.0, 'avg_logprob': -0.10328298651653788, 'compression_ratio': 1.754646840148699, 'no_speech_prob': 0.0009542157058604062}, {'id': 254, 'seek': 142492, 'start': 1439.88, 'end': 1445.48, 'text': \" associated with it. Every line also has a corresponds to the input that's coming in. It has a\", 'tokens': [51112, 6615, 365, 309, 13, 2048, 1622, 611, 575, 257, 23249, 281, 264, 4846, 300, 311, 1348, 294, 13, 467, 575, 257, 51392], 'temperature': 0.0, 'avg_logprob': -0.10328298651653788, 'compression_ratio': 1.754646840148699, 'no_speech_prob': 0.0009542157058604062}, {'id': 255, 'seek': 142492, 'start': 1445.48, 'end': 1451.72, 'text': \" weight that's coming in also at the on the line itself, and I've also removed the bias just for\", 'tokens': [51392, 3364, 300, 311, 1348, 294, 611, 412, 264, 322, 264, 1622, 2564, 11, 293, 286, 600, 611, 7261, 264, 12577, 445, 337, 51704], 'temperature': 0.0, 'avg_logprob': -0.10328298651653788, 'compression_ratio': 1.754646840148699, 'no_speech_prob': 0.0009542157058604062}, {'id': 256, 'seek': 145172, 'start': 1451.72, 'end': 1458.76, 'text': \" sake of simplicity, but it's still there. So now the result is that z, which let's call that the\", 'tokens': [50364, 9717, 295, 25632, 11, 457, 309, 311, 920, 456, 13, 407, 586, 264, 1874, 307, 300, 710, 11, 597, 718, 311, 818, 300, 264, 50716], 'temperature': 0.0, 'avg_logprob': -0.10974039212621824, 'compression_ratio': 1.7716894977168949, 'no_speech_prob': 0.0022510988637804985}, {'id': 257, 'seek': 145172, 'start': 1458.76, 'end': 1464.28, 'text': \" result of our dot product plus the bias, is going, and that's what we pass into our non-linear function,\", 'tokens': [50716, 1874, 295, 527, 5893, 1674, 1804, 264, 12577, 11, 307, 516, 11, 293, 300, 311, 437, 321, 1320, 666, 527, 2107, 12, 28263, 2445, 11, 50992], 'temperature': 0.0, 'avg_logprob': -0.10974039212621824, 'compression_ratio': 1.7716894977168949, 'no_speech_prob': 0.0022510988637804985}, {'id': 258, 'seek': 145172, 'start': 1464.92, 'end': 1470.3600000000001, 'text': ' that piece is going to be applied to that activation function. Now the final output here', 'tokens': [51024, 300, 2522, 307, 516, 281, 312, 6456, 281, 300, 24433, 2445, 13, 823, 264, 2572, 5598, 510, 51296], 'temperature': 0.0, 'avg_logprob': -0.10974039212621824, 'compression_ratio': 1.7716894977168949, 'no_speech_prob': 0.0022510988637804985}, {'id': 259, 'seek': 145172, 'start': 1471.0, 'end': 1478.44, 'text': ' is simply going to be g, which is our activation function of z, right? z is going to be basically', 'tokens': [51328, 307, 2935, 516, 281, 312, 290, 11, 597, 307, 527, 24433, 2445, 295, 710, 11, 558, 30, 710, 307, 516, 281, 312, 1936, 51700], 'temperature': 0.0, 'avg_logprob': -0.10974039212621824, 'compression_ratio': 1.7716894977168949, 'no_speech_prob': 0.0022510988637804985}, {'id': 260, 'seek': 147844, 'start': 1478.44, 'end': 1482.68, 'text': \" you can think of the state of this neuron. It's the result of that dot product plus bias.\", 'tokens': [50364, 291, 393, 519, 295, 264, 1785, 295, 341, 34090, 13, 467, 311, 264, 1874, 295, 300, 5893, 1674, 1804, 12577, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.07607717595548712, 'compression_ratio': 1.712686567164179, 'no_speech_prob': 0.0013041128404438496}, {'id': 261, 'seek': 147844, 'start': 1484.04, 'end': 1488.52, 'text': ' Now if we want to define and build up a multi-layered output neural network,', 'tokens': [50644, 823, 498, 321, 528, 281, 6964, 293, 1322, 493, 257, 4825, 12, 8376, 4073, 5598, 18161, 3209, 11, 50868], 'temperature': 0.0, 'avg_logprob': -0.07607717595548712, 'compression_ratio': 1.712686567164179, 'no_speech_prob': 0.0013041128404438496}, {'id': 262, 'seek': 147844, 'start': 1489.0, 'end': 1493.0800000000002, 'text': \" if we want two outputs to this function, for example, it's a very simple procedure. We just have\", 'tokens': [50892, 498, 321, 528, 732, 23930, 281, 341, 2445, 11, 337, 1365, 11, 309, 311, 257, 588, 2199, 10747, 13, 492, 445, 362, 51096], 'temperature': 0.0, 'avg_logprob': -0.07607717595548712, 'compression_ratio': 1.712686567164179, 'no_speech_prob': 0.0013041128404438496}, {'id': 263, 'seek': 147844, 'start': 1493.0800000000002, 'end': 1499.0, 'text': ' now two neurons, two perceptrons. Each perceptron will control the output for its associated piece,', 'tokens': [51096, 586, 732, 22027, 11, 732, 43276, 13270, 13, 6947, 43276, 2044, 486, 1969, 264, 5598, 337, 1080, 6615, 2522, 11, 51392], 'temperature': 0.0, 'avg_logprob': -0.07607717595548712, 'compression_ratio': 1.712686567164179, 'no_speech_prob': 0.0013041128404438496}, {'id': 264, 'seek': 147844, 'start': 1499.0, 'end': 1504.28, 'text': ' right? So now we have two outputs. Each one is a normal perceptron. It takes all of the inputs,', 'tokens': [51392, 558, 30, 407, 586, 321, 362, 732, 23930, 13, 6947, 472, 307, 257, 2710, 43276, 2044, 13, 467, 2516, 439, 295, 264, 15743, 11, 51656], 'temperature': 0.0, 'avg_logprob': -0.07607717595548712, 'compression_ratio': 1.712686567164179, 'no_speech_prob': 0.0013041128404438496}, {'id': 265, 'seek': 150428, 'start': 1504.28, 'end': 1509.3999999999999, 'text': ' so they both take the same inputs, but amazingly, now with this mathematical understanding,', 'tokens': [50364, 370, 436, 1293, 747, 264, 912, 15743, 11, 457, 31762, 11, 586, 365, 341, 18894, 3701, 11, 50620], 'temperature': 0.0, 'avg_logprob': -0.08865145400718406, 'compression_ratio': 1.7262773722627738, 'no_speech_prob': 0.001224932144396007}, {'id': 266, 'seek': 150428, 'start': 1509.3999999999999, 'end': 1515.0, 'text': ' we can start to build our first neural network entirely from scratch. So what does that look like?', 'tokens': [50620, 321, 393, 722, 281, 1322, 527, 700, 18161, 3209, 7696, 490, 8459, 13, 407, 437, 775, 300, 574, 411, 30, 50900], 'temperature': 0.0, 'avg_logprob': -0.08865145400718406, 'compression_ratio': 1.7262773722627738, 'no_speech_prob': 0.001224932144396007}, {'id': 267, 'seek': 150428, 'start': 1515.0, 'end': 1519.8799999999999, 'text': ' So we can start by firstly initializing these two components. The first component that we saw', 'tokens': [50900, 407, 321, 393, 722, 538, 27376, 5883, 3319, 613, 732, 6677, 13, 440, 700, 6542, 300, 321, 1866, 51144], 'temperature': 0.0, 'avg_logprob': -0.08865145400718406, 'compression_ratio': 1.7262773722627738, 'no_speech_prob': 0.001224932144396007}, {'id': 268, 'seek': 150428, 'start': 1519.8799999999999, 'end': 1525.0, 'text': \" was the weight matrix, excuse me, the weight vector. It's a vector of weights in this case.\", 'tokens': [51144, 390, 264, 3364, 8141, 11, 8960, 385, 11, 264, 3364, 8062, 13, 467, 311, 257, 8062, 295, 17443, 294, 341, 1389, 13, 51400], 'temperature': 0.0, 'avg_logprob': -0.08865145400718406, 'compression_ratio': 1.7262773722627738, 'no_speech_prob': 0.001224932144396007}, {'id': 269, 'seek': 150428, 'start': 1526.44, 'end': 1532.04, 'text': \" And the second component is the bias vector that we're going to multiply with the dot product of\", 'tokens': [51472, 400, 264, 1150, 6542, 307, 264, 12577, 8062, 300, 321, 434, 516, 281, 12972, 365, 264, 5893, 1674, 295, 51752], 'temperature': 0.0, 'avg_logprob': -0.08865145400718406, 'compression_ratio': 1.7262773722627738, 'no_speech_prob': 0.001224932144396007}, {'id': 270, 'seek': 153204, 'start': 1532.04, 'end': 1539.48, 'text': \" all of our inputs by our weights. So the only remaining step now after we've defined these parameters\", 'tokens': [50364, 439, 295, 527, 15743, 538, 527, 17443, 13, 407, 264, 787, 8877, 1823, 586, 934, 321, 600, 7642, 613, 9834, 50736], 'temperature': 0.0, 'avg_logprob': -0.10669992161893296, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0006260182126425207}, {'id': 271, 'seek': 153204, 'start': 1539.48, 'end': 1545.6399999999999, 'text': \" of our layer is to now define how this forward propagation of information works. And that's exactly\", 'tokens': [50736, 295, 527, 4583, 307, 281, 586, 6964, 577, 341, 2128, 38377, 295, 1589, 1985, 13, 400, 300, 311, 2293, 51044], 'temperature': 0.0, 'avg_logprob': -0.10669992161893296, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0006260182126425207}, {'id': 272, 'seek': 153204, 'start': 1545.6399999999999, 'end': 1551.08, 'text': \" those three main components that I've been stressing to you. So we can create this call function\", 'tokens': [51044, 729, 1045, 2135, 6677, 300, 286, 600, 668, 48233, 281, 291, 13, 407, 321, 393, 1884, 341, 818, 2445, 51316], 'temperature': 0.0, 'avg_logprob': -0.10669992161893296, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0006260182126425207}, {'id': 273, 'seek': 153204, 'start': 1551.08, 'end': 1556.52, 'text': ' to do exactly that, to define this forward propagation of information. And the story here is exactly', 'tokens': [51316, 281, 360, 2293, 300, 11, 281, 6964, 341, 2128, 38377, 295, 1589, 13, 400, 264, 1657, 510, 307, 2293, 51588], 'temperature': 0.0, 'avg_logprob': -0.10669992161893296, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0006260182126425207}, {'id': 274, 'seek': 155652, 'start': 1556.52, 'end': 1563.32, 'text': \" the same as we've been seeing it, right? Matrix multiply our inputs with our weights, right? Add a bias,\", 'tokens': [50364, 264, 912, 382, 321, 600, 668, 2577, 309, 11, 558, 30, 36274, 12972, 527, 15743, 365, 527, 17443, 11, 558, 30, 5349, 257, 12577, 11, 50704], 'temperature': 0.0, 'avg_logprob': -0.15376198645865563, 'compression_ratio': 1.6779661016949152, 'no_speech_prob': 0.0022505649831146}, {'id': 275, 'seek': 155652, 'start': 1564.6, 'end': 1570.12, 'text': ' and then apply a non-linearity and return the result, right? And that literally this code will run,', 'tokens': [50768, 293, 550, 3079, 257, 2107, 12, 1889, 17409, 293, 2736, 264, 1874, 11, 558, 30, 400, 300, 3736, 341, 3089, 486, 1190, 11, 51044], 'temperature': 0.0, 'avg_logprob': -0.15376198645865563, 'compression_ratio': 1.6779661016949152, 'no_speech_prob': 0.0022505649831146}, {'id': 276, 'seek': 155652, 'start': 1570.12, 'end': 1577.8799999999999, 'text': ' this will define a full neural network layer that you can then take like this. And of course,', 'tokens': [51044, 341, 486, 6964, 257, 1577, 18161, 3209, 4583, 300, 291, 393, 550, 747, 411, 341, 13, 400, 295, 1164, 11, 51432], 'temperature': 0.0, 'avg_logprob': -0.15376198645865563, 'compression_ratio': 1.6779661016949152, 'no_speech_prob': 0.0022505649831146}, {'id': 277, 'seek': 155652, 'start': 1577.8799999999999, 'end': 1582.12, 'text': \" actually luckily for all of you, all of that code, which wasn't much code, that's been abstracted\", 'tokens': [51432, 767, 22880, 337, 439, 295, 291, 11, 439, 295, 300, 3089, 11, 597, 2067, 380, 709, 3089, 11, 300, 311, 668, 12649, 292, 51644], 'temperature': 0.0, 'avg_logprob': -0.15376198645865563, 'compression_ratio': 1.6779661016949152, 'no_speech_prob': 0.0022505649831146}, {'id': 278, 'seek': 158212, 'start': 1582.12, 'end': 1587.6399999999999, 'text': ' away by these libraries like TensorFlow, you can simply call functions like this, which will actually', 'tokens': [50364, 1314, 538, 613, 15148, 411, 37624, 11, 291, 393, 2935, 818, 6828, 411, 341, 11, 597, 486, 767, 50640], 'temperature': 0.0, 'avg_logprob': -0.11326329604439113, 'compression_ratio': 1.65, 'no_speech_prob': 0.0022508101537823677}, {'id': 279, 'seek': 158212, 'start': 1587.6399999999999, 'end': 1593.0, 'text': \" replicate exactly that piece of code. So you don't need to necessarily copy all of that code down,\", 'tokens': [50640, 25356, 2293, 300, 2522, 295, 3089, 13, 407, 291, 500, 380, 643, 281, 4725, 5055, 439, 295, 300, 3089, 760, 11, 50908], 'temperature': 0.0, 'avg_logprob': -0.11326329604439113, 'compression_ratio': 1.65, 'no_speech_prob': 0.0022508101537823677}, {'id': 280, 'seek': 158212, 'start': 1594.1999999999998, 'end': 1601.08, 'text': ' you can just call it. And with that understanding, we just saw how you could build a single layer.', 'tokens': [50968, 291, 393, 445, 818, 309, 13, 400, 365, 300, 3701, 11, 321, 445, 1866, 577, 291, 727, 1322, 257, 2167, 4583, 13, 51312], 'temperature': 0.0, 'avg_logprob': -0.11326329604439113, 'compression_ratio': 1.65, 'no_speech_prob': 0.0022508101537823677}, {'id': 281, 'seek': 158212, 'start': 1601.08, 'end': 1607.08, 'text': ' But of course, now you can actually start to think about how you can stack these layers as well.', 'tokens': [51312, 583, 295, 1164, 11, 586, 291, 393, 767, 722, 281, 519, 466, 577, 291, 393, 8630, 613, 7914, 382, 731, 13, 51612], 'temperature': 0.0, 'avg_logprob': -0.11326329604439113, 'compression_ratio': 1.65, 'no_speech_prob': 0.0022508101537823677}, {'id': 282, 'seek': 160708, 'start': 1607.08, 'end': 1613.24, 'text': ' So since we now have this transformation, essentially, from our inputs to a hidden output,', 'tokens': [50364, 407, 1670, 321, 586, 362, 341, 9887, 11, 4476, 11, 490, 527, 15743, 281, 257, 7633, 5598, 11, 50672], 'temperature': 0.0, 'avg_logprob': -0.08148963110787528, 'compression_ratio': 1.6872246696035242, 'no_speech_prob': 0.007569643668830395}, {'id': 283, 'seek': 160708, 'start': 1613.24, 'end': 1620.52, 'text': ' you can think of this as basically how we can define some way of transforming those inputs,', 'tokens': [50672, 291, 393, 519, 295, 341, 382, 1936, 577, 321, 393, 6964, 512, 636, 295, 27210, 729, 15743, 11, 51036], 'temperature': 0.0, 'avg_logprob': -0.08148963110787528, 'compression_ratio': 1.6872246696035242, 'no_speech_prob': 0.007569643668830395}, {'id': 284, 'seek': 160708, 'start': 1621.08, 'end': 1627.08, 'text': ' right, into some new dimensional space, right? Perhaps closer to the value that we want to predict.', 'tokens': [51064, 558, 11, 666, 512, 777, 18795, 1901, 11, 558, 30, 10517, 4966, 281, 264, 2158, 300, 321, 528, 281, 6069, 13, 51364], 'temperature': 0.0, 'avg_logprob': -0.08148963110787528, 'compression_ratio': 1.6872246696035242, 'no_speech_prob': 0.007569643668830395}, {'id': 285, 'seek': 160708, 'start': 1627.08, 'end': 1632.4399999999998, 'text': ' And that transformation is going to be eventually learned to know how to transform those inputs into', 'tokens': [51364, 400, 300, 9887, 307, 516, 281, 312, 4728, 3264, 281, 458, 577, 281, 4088, 729, 15743, 666, 51632], 'temperature': 0.0, 'avg_logprob': -0.08148963110787528, 'compression_ratio': 1.6872246696035242, 'no_speech_prob': 0.007569643668830395}, {'id': 286, 'seek': 163244, 'start': 1632.44, 'end': 1637.3200000000002, 'text': \" our desired outputs. And we'll get to that later. But for now, the piece that I want to really focus\", 'tokens': [50364, 527, 14721, 23930, 13, 400, 321, 603, 483, 281, 300, 1780, 13, 583, 337, 586, 11, 264, 2522, 300, 286, 528, 281, 534, 1879, 50608], 'temperature': 0.0, 'avg_logprob': -0.1030720443725586, 'compression_ratio': 1.7689530685920578, 'no_speech_prob': 0.0006877047708258033}, {'id': 287, 'seek': 163244, 'start': 1637.3200000000002, 'end': 1641.96, 'text': ' on is if we have these more complex neural networks, I want to really distill down that this is', 'tokens': [50608, 322, 307, 498, 321, 362, 613, 544, 3997, 18161, 9590, 11, 286, 528, 281, 534, 42923, 760, 300, 341, 307, 50840], 'temperature': 0.0, 'avg_logprob': -0.1030720443725586, 'compression_ratio': 1.7689530685920578, 'no_speech_prob': 0.0006877047708258033}, {'id': 288, 'seek': 163244, 'start': 1641.96, 'end': 1647.48, 'text': \" nothing more complex than what we've already seen. If we focus on just one neuron in this diagram,\", 'tokens': [50840, 1825, 544, 3997, 813, 437, 321, 600, 1217, 1612, 13, 759, 321, 1879, 322, 445, 472, 34090, 294, 341, 10686, 11, 51116], 'temperature': 0.0, 'avg_logprob': -0.1030720443725586, 'compression_ratio': 1.7689530685920578, 'no_speech_prob': 0.0006877047708258033}, {'id': 289, 'seek': 163244, 'start': 1648.44, 'end': 1653.64, 'text': \" take, as here for example, Z2, right, Z2 is this neuron that's highlighted in the middle layer.\", 'tokens': [51164, 747, 11, 382, 510, 337, 1365, 11, 1176, 17, 11, 558, 11, 1176, 17, 307, 341, 34090, 300, 311, 17173, 294, 264, 2808, 4583, 13, 51424], 'temperature': 0.0, 'avg_logprob': -0.1030720443725586, 'compression_ratio': 1.7689530685920578, 'no_speech_prob': 0.0006877047708258033}, {'id': 290, 'seek': 163244, 'start': 1654.52, 'end': 1660.28, 'text': \" It's just the same perceptron that we've been seeing so far in this class. It's output is obtained\", 'tokens': [51468, 467, 311, 445, 264, 912, 43276, 2044, 300, 321, 600, 668, 2577, 370, 1400, 294, 341, 1508, 13, 467, 311, 5598, 307, 14879, 51756], 'temperature': 0.0, 'avg_logprob': -0.1030720443725586, 'compression_ratio': 1.7689530685920578, 'no_speech_prob': 0.0006877047708258033}, {'id': 291, 'seek': 166028, 'start': 1660.28, 'end': 1665.24, 'text': ' by taking a dot product, adding a bias, and then applying that non-linearity between all of its inputs.', 'tokens': [50364, 538, 1940, 257, 5893, 1674, 11, 5127, 257, 12577, 11, 293, 550, 9275, 300, 2107, 12, 1889, 17409, 1296, 439, 295, 1080, 15743, 13, 50612], 'temperature': 0.0, 'avg_logprob': -0.08510853539050466, 'compression_ratio': 1.7725856697819315, 'no_speech_prob': 0.0018671315629035234}, {'id': 292, 'seek': 166028, 'start': 1666.28, 'end': 1669.96, 'text': ' If we look at a different node, for example, Z3, which is the one right below it,', 'tokens': [50664, 759, 321, 574, 412, 257, 819, 9984, 11, 337, 1365, 11, 1176, 18, 11, 597, 307, 264, 472, 558, 2507, 309, 11, 50848], 'temperature': 0.0, 'avg_logprob': -0.08510853539050466, 'compression_ratio': 1.7725856697819315, 'no_speech_prob': 0.0018671315629035234}, {'id': 293, 'seek': 166028, 'start': 1669.96, 'end': 1674.04, 'text': \" it's the exact same story again. It sees all the same inputs, but it has a different set of weight\", 'tokens': [50848, 309, 311, 264, 1900, 912, 1657, 797, 13, 467, 8194, 439, 264, 912, 15743, 11, 457, 309, 575, 257, 819, 992, 295, 3364, 51052], 'temperature': 0.0, 'avg_logprob': -0.08510853539050466, 'compression_ratio': 1.7725856697819315, 'no_speech_prob': 0.0018671315629035234}, {'id': 294, 'seek': 166028, 'start': 1674.04, 'end': 1678.84, 'text': \" matrix that it's going to apply to those inputs. So we'll have a different output. But the\", 'tokens': [51052, 8141, 300, 309, 311, 516, 281, 3079, 281, 729, 15743, 13, 407, 321, 603, 362, 257, 819, 5598, 13, 583, 264, 51292], 'temperature': 0.0, 'avg_logprob': -0.08510853539050466, 'compression_ratio': 1.7725856697819315, 'no_speech_prob': 0.0018671315629035234}, {'id': 295, 'seek': 166028, 'start': 1678.84, 'end': 1683.56, 'text': \" mathematically equations are exactly the same. So from now on, I'm just going to kind of simplify\", 'tokens': [51292, 44003, 11787, 366, 2293, 264, 912, 13, 407, 490, 586, 322, 11, 286, 478, 445, 516, 281, 733, 295, 20460, 51528], 'temperature': 0.0, 'avg_logprob': -0.08510853539050466, 'compression_ratio': 1.7725856697819315, 'no_speech_prob': 0.0018671315629035234}, {'id': 296, 'seek': 166028, 'start': 1683.56, 'end': 1688.76, 'text': ' all of these lines and diagrams just to show these icons in the middle just to demonstrate that', 'tokens': [51528, 439, 295, 613, 3876, 293, 36709, 445, 281, 855, 613, 23308, 294, 264, 2808, 445, 281, 11698, 300, 51788], 'temperature': 0.0, 'avg_logprob': -0.08510853539050466, 'compression_ratio': 1.7725856697819315, 'no_speech_prob': 0.0018671315629035234}, {'id': 297, 'seek': 168876, 'start': 1688.76, 'end': 1692.44, 'text': ' these mean that everything is going to fully connect it to everything and defined by those', 'tokens': [50364, 613, 914, 300, 1203, 307, 516, 281, 4498, 1745, 309, 281, 1203, 293, 7642, 538, 729, 50548], 'temperature': 0.0, 'avg_logprob': -0.08922979139512585, 'compression_ratio': 1.894736842105263, 'no_speech_prob': 0.00023046191199682653}, {'id': 298, 'seek': 168876, 'start': 1692.44, 'end': 1697.64, 'text': \" mathematical equations that we've been covering. But there's no extra complexity in these models\", 'tokens': [50548, 18894, 11787, 300, 321, 600, 668, 10322, 13, 583, 456, 311, 572, 2857, 14024, 294, 613, 5245, 50808], 'temperature': 0.0, 'avg_logprob': -0.08922979139512585, 'compression_ratio': 1.894736842105263, 'no_speech_prob': 0.00023046191199682653}, {'id': 299, 'seek': 168876, 'start': 1697.64, 'end': 1703.72, 'text': \" from what you've already seen. Now, if you want to stack these types of solutions on top of each\", 'tokens': [50808, 490, 437, 291, 600, 1217, 1612, 13, 823, 11, 498, 291, 528, 281, 8630, 613, 3467, 295, 6547, 322, 1192, 295, 1184, 51112], 'temperature': 0.0, 'avg_logprob': -0.08922979139512585, 'compression_ratio': 1.894736842105263, 'no_speech_prob': 0.00023046191199682653}, {'id': 300, 'seek': 168876, 'start': 1703.72, 'end': 1708.52, 'text': ' other, these layers on top of each other, you can not only define one layer very easily, but you can', 'tokens': [51112, 661, 11, 613, 7914, 322, 1192, 295, 1184, 661, 11, 291, 393, 406, 787, 6964, 472, 4583, 588, 3612, 11, 457, 291, 393, 51352], 'temperature': 0.0, 'avg_logprob': -0.08922979139512585, 'compression_ratio': 1.894736842105263, 'no_speech_prob': 0.00023046191199682653}, {'id': 301, 'seek': 168876, 'start': 1708.52, 'end': 1712.92, 'text': ' actually create what are called sequential models. These sequential models, you can define one', 'tokens': [51352, 767, 1884, 437, 366, 1219, 42881, 5245, 13, 1981, 42881, 5245, 11, 291, 393, 6964, 472, 51572], 'temperature': 0.0, 'avg_logprob': -0.08922979139512585, 'compression_ratio': 1.894736842105263, 'no_speech_prob': 0.00023046191199682653}, {'id': 302, 'seek': 168876, 'start': 1712.92, 'end': 1718.2, 'text': ' layer after another, and they define basically the forward propagation of information, not just', 'tokens': [51572, 4583, 934, 1071, 11, 293, 436, 6964, 1936, 264, 2128, 38377, 295, 1589, 11, 406, 445, 51836], 'temperature': 0.0, 'avg_logprob': -0.08922979139512585, 'compression_ratio': 1.894736842105263, 'no_speech_prob': 0.00023046191199682653}, {'id': 303, 'seek': 171820, 'start': 1718.2, 'end': 1723.0800000000002, 'text': ' from the neuron level, but now from the layer level. Every layer will be fully connected to the', 'tokens': [50364, 490, 264, 34090, 1496, 11, 457, 586, 490, 264, 4583, 1496, 13, 2048, 4583, 486, 312, 4498, 4582, 281, 264, 50608], 'temperature': 0.0, 'avg_logprob': -0.08258802462846805, 'compression_ratio': 1.8431372549019607, 'no_speech_prob': 0.0008966303430497646}, {'id': 304, 'seek': 171820, 'start': 1723.0800000000002, 'end': 1728.3600000000001, 'text': ' next layer, and the inputs of the secondary layer will be all of the outputs of the prior layer.', 'tokens': [50608, 958, 4583, 11, 293, 264, 15743, 295, 264, 11396, 4583, 486, 312, 439, 295, 264, 23930, 295, 264, 4059, 4583, 13, 50872], 'temperature': 0.0, 'avg_logprob': -0.08258802462846805, 'compression_ratio': 1.8431372549019607, 'no_speech_prob': 0.0008966303430497646}, {'id': 305, 'seek': 171820, 'start': 1730.04, 'end': 1734.2, 'text': ' Now, of course, if you want to create a very deep neural network, all the deep neural network is,', 'tokens': [50956, 823, 11, 295, 1164, 11, 498, 291, 528, 281, 1884, 257, 588, 2452, 18161, 3209, 11, 439, 264, 2452, 18161, 3209, 307, 11, 51164], 'temperature': 0.0, 'avg_logprob': -0.08258802462846805, 'compression_ratio': 1.8431372549019607, 'no_speech_prob': 0.0008966303430497646}, {'id': 306, 'seek': 171820, 'start': 1734.2, 'end': 1738.44, 'text': \" is we just keep stacking these layers on top of each other. There's nothing else to this story.\", 'tokens': [51164, 307, 321, 445, 1066, 41376, 613, 7914, 322, 1192, 295, 1184, 661, 13, 821, 311, 1825, 1646, 281, 341, 1657, 13, 51376], 'temperature': 0.0, 'avg_logprob': -0.08258802462846805, 'compression_ratio': 1.8431372549019607, 'no_speech_prob': 0.0008966303430497646}, {'id': 307, 'seek': 171820, 'start': 1738.44, 'end': 1743.4, 'text': \" That's really as simple as it is. Once, so these layers are basically all they are,\", 'tokens': [51376, 663, 311, 534, 382, 2199, 382, 309, 307, 13, 3443, 11, 370, 613, 7914, 366, 1936, 439, 436, 366, 11, 51624], 'temperature': 0.0, 'avg_logprob': -0.08258802462846805, 'compression_ratio': 1.8431372549019607, 'no_speech_prob': 0.0008966303430497646}, {'id': 308, 'seek': 174340, 'start': 1743.4, 'end': 1749.3200000000002, 'text': \" it's just layers where the final output is computed by going deeper and deeper into this\", 'tokens': [50364, 309, 311, 445, 7914, 689, 264, 2572, 5598, 307, 40610, 538, 516, 7731, 293, 7731, 666, 341, 50660], 'temperature': 0.0, 'avg_logprob': -0.11080497853896197, 'compression_ratio': 1.7509727626459144, 'no_speech_prob': 0.003704858710989356}, {'id': 309, 'seek': 174340, 'start': 1749.3200000000002, 'end': 1754.0400000000002, 'text': ' progression of different layers. You just keep stacking them until you get to the last layer,', 'tokens': [50660, 18733, 295, 819, 7914, 13, 509, 445, 1066, 41376, 552, 1826, 291, 483, 281, 264, 1036, 4583, 11, 50896], 'temperature': 0.0, 'avg_logprob': -0.11080497853896197, 'compression_ratio': 1.7509727626459144, 'no_speech_prob': 0.003704858710989356}, {'id': 310, 'seek': 174340, 'start': 1754.0400000000002, 'end': 1757.0, 'text': \" which is your output layer. It's your final prediction that you want to output.\", 'tokens': [50896, 597, 307, 428, 5598, 4583, 13, 467, 311, 428, 2572, 17630, 300, 291, 528, 281, 5598, 13, 51044], 'temperature': 0.0, 'avg_logprob': -0.11080497853896197, 'compression_ratio': 1.7509727626459144, 'no_speech_prob': 0.003704858710989356}, {'id': 311, 'seek': 174340, 'start': 1758.68, 'end': 1762.76, 'text': ' We can create a deep neural network to do all of this by stacking these layers and creating', 'tokens': [51128, 492, 393, 1884, 257, 2452, 18161, 3209, 281, 360, 439, 295, 341, 538, 41376, 613, 7914, 293, 4084, 51332], 'temperature': 0.0, 'avg_logprob': -0.11080497853896197, 'compression_ratio': 1.7509727626459144, 'no_speech_prob': 0.003704858710989356}, {'id': 312, 'seek': 174340, 'start': 1762.76, 'end': 1767.88, 'text': \" these more hierarchical models, like we saw very early in the beginning of today's lecture. One\", 'tokens': [51332, 613, 544, 35250, 804, 5245, 11, 411, 321, 1866, 588, 2440, 294, 264, 2863, 295, 965, 311, 7991, 13, 1485, 51588], 'temperature': 0.0, 'avg_logprob': -0.11080497853896197, 'compression_ratio': 1.7509727626459144, 'no_speech_prob': 0.003704858710989356}, {'id': 313, 'seek': 176788, 'start': 1767.88, 'end': 1773.0, 'text': ' where the final output is really computed by just going deeper and deeper into this system.', 'tokens': [50364, 689, 264, 2572, 5598, 307, 534, 40610, 538, 445, 516, 7731, 293, 7731, 666, 341, 1185, 13, 50620], 'temperature': 0.0, 'avg_logprob': -0.10994017809286885, 'compression_ratio': 1.587719298245614, 'no_speech_prob': 0.0005355945322662592}, {'id': 314, 'seek': 176788, 'start': 1774.8400000000001, 'end': 1781.0, 'text': \" Okay, so that's awesome. We've now seen how we can go from a single neuron to a layer,\", 'tokens': [50712, 1033, 11, 370, 300, 311, 3476, 13, 492, 600, 586, 1612, 577, 321, 393, 352, 490, 257, 2167, 34090, 281, 257, 4583, 11, 51020], 'temperature': 0.0, 'avg_logprob': -0.10994017809286885, 'compression_ratio': 1.587719298245614, 'no_speech_prob': 0.0005355945322662592}, {'id': 315, 'seek': 176788, 'start': 1781.0, 'end': 1785.48, 'text': ' to all the way to a deep neural network, building off of these foundational principles.', 'tokens': [51020, 281, 439, 264, 636, 281, 257, 2452, 18161, 3209, 11, 2390, 766, 295, 613, 32195, 9156, 13, 51244], 'temperature': 0.0, 'avg_logprob': -0.10994017809286885, 'compression_ratio': 1.587719298245614, 'no_speech_prob': 0.0005355945322662592}, {'id': 316, 'seek': 176788, 'start': 1786.44, 'end': 1793.88, 'text': \" Let's take a look at how exactly we can use these principles that we've just discussed to solve\", 'tokens': [51292, 961, 311, 747, 257, 574, 412, 577, 2293, 321, 393, 764, 613, 9156, 300, 321, 600, 445, 7152, 281, 5039, 51664], 'temperature': 0.0, 'avg_logprob': -0.10994017809286885, 'compression_ratio': 1.587719298245614, 'no_speech_prob': 0.0005355945322662592}, {'id': 317, 'seek': 179388, 'start': 1793.88, 'end': 1799.8000000000002, 'text': ' a very real problem that I think all of you are probably very concerned about this morning when you', 'tokens': [50364, 257, 588, 957, 1154, 300, 286, 519, 439, 295, 291, 366, 1391, 588, 5922, 466, 341, 2446, 562, 291, 50660], 'temperature': 0.0, 'avg_logprob': -0.12712408589050833, 'compression_ratio': 1.8583333333333334, 'no_speech_prob': 0.001956312218680978}, {'id': 318, 'seek': 179388, 'start': 1799.8000000000002, 'end': 1805.48, 'text': ' woke up. So that problem is how we can build a neural network to answer this question,', 'tokens': [50660, 12852, 493, 13, 407, 300, 1154, 307, 577, 321, 393, 1322, 257, 18161, 3209, 281, 1867, 341, 1168, 11, 50944], 'temperature': 0.0, 'avg_logprob': -0.12712408589050833, 'compression_ratio': 1.8583333333333334, 'no_speech_prob': 0.001956312218680978}, {'id': 319, 'seek': 179388, 'start': 1805.48, 'end': 1812.2, 'text': ' which is, will I pass this class if I will or will I not? So to answer this question,', 'tokens': [50944, 597, 307, 11, 486, 286, 1320, 341, 1508, 498, 286, 486, 420, 486, 286, 406, 30, 407, 281, 1867, 341, 1168, 11, 51280], 'temperature': 0.0, 'avg_logprob': -0.12712408589050833, 'compression_ratio': 1.8583333333333334, 'no_speech_prob': 0.001956312218680978}, {'id': 320, 'seek': 179388, 'start': 1812.2, 'end': 1816.92, 'text': \" let's see if we can train a neural network to solve this problem. Okay, so to do this,\", 'tokens': [51280, 718, 311, 536, 498, 321, 393, 3847, 257, 18161, 3209, 281, 5039, 341, 1154, 13, 1033, 11, 370, 281, 360, 341, 11, 51516], 'temperature': 0.0, 'avg_logprob': -0.12712408589050833, 'compression_ratio': 1.8583333333333334, 'no_speech_prob': 0.001956312218680978}, {'id': 321, 'seek': 179388, 'start': 1816.92, 'end': 1821.64, 'text': \" let's start with a very simple neural network. We'll train this model with two inputs,\", 'tokens': [51516, 718, 311, 722, 365, 257, 588, 2199, 18161, 3209, 13, 492, 603, 3847, 341, 2316, 365, 732, 15743, 11, 51752], 'temperature': 0.0, 'avg_logprob': -0.12712408589050833, 'compression_ratio': 1.8583333333333334, 'no_speech_prob': 0.001956312218680978}, {'id': 322, 'seek': 182164, 'start': 1821.64, 'end': 1826.2, 'text': ' just two inputs. One input is going to be the number of lectures that you attend over the course', 'tokens': [50364, 445, 732, 15743, 13, 1485, 4846, 307, 516, 281, 312, 264, 1230, 295, 16564, 300, 291, 6888, 670, 264, 1164, 50592], 'temperature': 0.0, 'avg_logprob': -0.07754462201830367, 'compression_ratio': 1.8685897435897436, 'no_speech_prob': 0.002216035732999444}, {'id': 323, 'seek': 182164, 'start': 1826.2, 'end': 1831.4, 'text': ' of this one week. And the second input is going to be how many hours that you spend on your final', 'tokens': [50592, 295, 341, 472, 1243, 13, 400, 264, 1150, 4846, 307, 516, 281, 312, 577, 867, 2496, 300, 291, 3496, 322, 428, 2572, 50852], 'temperature': 0.0, 'avg_logprob': -0.07754462201830367, 'compression_ratio': 1.8685897435897436, 'no_speech_prob': 0.002216035732999444}, {'id': 324, 'seek': 182164, 'start': 1831.4, 'end': 1837.88, 'text': \" project or your competition. Okay, so what we're going to do is firstly go out and collect a lot of\", 'tokens': [50852, 1716, 420, 428, 6211, 13, 1033, 11, 370, 437, 321, 434, 516, 281, 360, 307, 27376, 352, 484, 293, 2500, 257, 688, 295, 51176], 'temperature': 0.0, 'avg_logprob': -0.07754462201830367, 'compression_ratio': 1.8685897435897436, 'no_speech_prob': 0.002216035732999444}, {'id': 325, 'seek': 182164, 'start': 1837.88, 'end': 1842.0400000000002, 'text': \" data from all of the past years that we've taught this course. And we can plot all of this data\", 'tokens': [51176, 1412, 490, 439, 295, 264, 1791, 924, 300, 321, 600, 5928, 341, 1164, 13, 400, 321, 393, 7542, 439, 295, 341, 1412, 51384], 'temperature': 0.0, 'avg_logprob': -0.07754462201830367, 'compression_ratio': 1.8685897435897436, 'no_speech_prob': 0.002216035732999444}, {'id': 326, 'seek': 182164, 'start': 1842.0400000000002, 'end': 1846.6000000000001, 'text': \" because it's only two input space. We can plot this data on a two-dimensional feature space,\", 'tokens': [51384, 570, 309, 311, 787, 732, 4846, 1901, 13, 492, 393, 7542, 341, 1412, 322, 257, 732, 12, 18759, 4111, 1901, 11, 51612], 'temperature': 0.0, 'avg_logprob': -0.07754462201830367, 'compression_ratio': 1.8685897435897436, 'no_speech_prob': 0.002216035732999444}, {'id': 327, 'seek': 182164, 'start': 1846.6000000000001, 'end': 1851.4, 'text': ' right? We can actually look at all of the students before you that have passed the class and failed', 'tokens': [51612, 558, 30, 492, 393, 767, 574, 412, 439, 295, 264, 1731, 949, 291, 300, 362, 4678, 264, 1508, 293, 7612, 51852], 'temperature': 0.0, 'avg_logprob': -0.07754462201830367, 'compression_ratio': 1.8685897435897436, 'no_speech_prob': 0.002216035732999444}, {'id': 328, 'seek': 185140, 'start': 1851.48, 'end': 1856.1200000000001, 'text': \" the class and see where they lived in this space for the amount of hours that they've spent,\", 'tokens': [50368, 264, 1508, 293, 536, 689, 436, 5152, 294, 341, 1901, 337, 264, 2372, 295, 2496, 300, 436, 600, 4418, 11, 50600], 'temperature': 0.0, 'avg_logprob': -0.11093698788995612, 'compression_ratio': 1.8338658146964857, 'no_speech_prob': 0.0005700752953998744}, {'id': 329, 'seek': 185140, 'start': 1856.1200000000001, 'end': 1859.88, 'text': \" the number of lectures that they've attended and so on. Green points are the people who have\", 'tokens': [50600, 264, 1230, 295, 16564, 300, 436, 600, 15990, 293, 370, 322, 13, 6969, 2793, 366, 264, 561, 567, 362, 50788], 'temperature': 0.0, 'avg_logprob': -0.11093698788995612, 'compression_ratio': 1.8338658146964857, 'no_speech_prob': 0.0005700752953998744}, {'id': 330, 'seek': 185140, 'start': 1859.88, 'end': 1865.72, 'text': \" passed, read, or those who have failed. Now, and here's you, right? You're right here. Four,\", 'tokens': [50788, 4678, 11, 1401, 11, 420, 729, 567, 362, 7612, 13, 823, 11, 293, 510, 311, 291, 11, 558, 30, 509, 434, 558, 510, 13, 7451, 11, 51080], 'temperature': 0.0, 'avg_logprob': -0.11093698788995612, 'compression_ratio': 1.8338658146964857, 'no_speech_prob': 0.0005700752953998744}, {'id': 331, 'seek': 185140, 'start': 1865.72, 'end': 1870.92, 'text': \" five is your coordinate space. You fall right there and you've attended four lectures. You've spent\", 'tokens': [51080, 1732, 307, 428, 15670, 1901, 13, 509, 2100, 558, 456, 293, 291, 600, 15990, 1451, 16564, 13, 509, 600, 4418, 51340], 'temperature': 0.0, 'avg_logprob': -0.11093698788995612, 'compression_ratio': 1.8338658146964857, 'no_speech_prob': 0.0005700752953998744}, {'id': 332, 'seek': 185140, 'start': 1870.92, 'end': 1875.48, 'text': ' five hours on your final project. We want to build a neural network to answer the question of,', 'tokens': [51340, 1732, 2496, 322, 428, 2572, 1716, 13, 492, 528, 281, 1322, 257, 18161, 3209, 281, 1867, 264, 1168, 295, 11, 51568], 'temperature': 0.0, 'avg_logprob': -0.11093698788995612, 'compression_ratio': 1.8338658146964857, 'no_speech_prob': 0.0005700752953998744}, {'id': 333, 'seek': 185140, 'start': 1875.48, 'end': 1881.16, 'text': \" will you pass the class or will you fail the class? So let's do it. We have two inputs. One is four,\", 'tokens': [51568, 486, 291, 1320, 264, 1508, 420, 486, 291, 3061, 264, 1508, 30, 407, 718, 311, 360, 309, 13, 492, 362, 732, 15743, 13, 1485, 307, 1451, 11, 51852], 'temperature': 0.0, 'avg_logprob': -0.11093698788995612, 'compression_ratio': 1.8338658146964857, 'no_speech_prob': 0.0005700752953998744}, {'id': 334, 'seek': 188116, 'start': 1881.16, 'end': 1885.64, 'text': \" one is five. There's two numbers. We can feed them through a neural network that we've just seen\", 'tokens': [50364, 472, 307, 1732, 13, 821, 311, 732, 3547, 13, 492, 393, 3154, 552, 807, 257, 18161, 3209, 300, 321, 600, 445, 1612, 50588], 'temperature': 0.0, 'avg_logprob': -0.13147378558954917, 'compression_ratio': 1.6258503401360545, 'no_speech_prob': 0.0007550950977019966}, {'id': 335, 'seek': 188116, 'start': 1885.64, 'end': 1891.24, 'text': ' how we can build that. And we feed that into a single layered neural network. Three hidden units', 'tokens': [50588, 577, 321, 393, 1322, 300, 13, 400, 321, 3154, 300, 666, 257, 2167, 34666, 18161, 3209, 13, 6244, 7633, 6815, 50868], 'temperature': 0.0, 'avg_logprob': -0.13147378558954917, 'compression_ratio': 1.6258503401360545, 'no_speech_prob': 0.0007550950977019966}, {'id': 336, 'seek': 188116, 'start': 1891.24, 'end': 1895.24, 'text': ' in this example, but we could make it larger if we want it to be more expressive and more powerful.', 'tokens': [50868, 294, 341, 1365, 11, 457, 321, 727, 652, 309, 4833, 498, 321, 528, 309, 281, 312, 544, 40189, 293, 544, 4005, 13, 51068], 'temperature': 0.0, 'avg_logprob': -0.13147378558954917, 'compression_ratio': 1.6258503401360545, 'no_speech_prob': 0.0007550950977019966}, {'id': 337, 'seek': 188116, 'start': 1896.2, 'end': 1900.6000000000001, 'text': \" And we see here that the probability of you passing those classes point one. It's pretty\", 'tokens': [51116, 400, 321, 536, 510, 300, 264, 8482, 295, 291, 8437, 729, 5359, 935, 472, 13, 467, 311, 1238, 51336], 'temperature': 0.0, 'avg_logprob': -0.13147378558954917, 'compression_ratio': 1.6258503401360545, 'no_speech_prob': 0.0007550950977019966}, {'id': 338, 'seek': 188116, 'start': 1900.6000000000001, 'end': 1905.96, 'text': \" bismill. So why would this be the case, right? What did we do wrong? Because I don't think it's\", 'tokens': [51336, 272, 1434, 373, 13, 407, 983, 576, 341, 312, 264, 1389, 11, 558, 30, 708, 630, 321, 360, 2085, 30, 1436, 286, 500, 380, 519, 309, 311, 51604], 'temperature': 0.0, 'avg_logprob': -0.13147378558954917, 'compression_ratio': 1.6258503401360545, 'no_speech_prob': 0.0007550950977019966}, {'id': 339, 'seek': 190596, 'start': 1906.04, 'end': 1910.92, 'text': ' correct, right? When we looked at the space, it looked like actually you were a good candidate to', 'tokens': [50368, 3006, 11, 558, 30, 1133, 321, 2956, 412, 264, 1901, 11, 309, 2956, 411, 767, 291, 645, 257, 665, 11532, 281, 50612], 'temperature': 0.0, 'avg_logprob': -0.15656173706054688, 'compression_ratio': 1.6175298804780875, 'no_speech_prob': 0.011853840202093124}, {'id': 340, 'seek': 190596, 'start': 1910.92, 'end': 1915.64, 'text': \" pass the class. But why is the neural network saying that there's only 10% likelihood that you should\", 'tokens': [50612, 1320, 264, 1508, 13, 583, 983, 307, 264, 18161, 3209, 1566, 300, 456, 311, 787, 1266, 4, 22119, 300, 291, 820, 50848], 'temperature': 0.0, 'avg_logprob': -0.15656173706054688, 'compression_ratio': 1.6175298804780875, 'no_speech_prob': 0.011853840202093124}, {'id': 341, 'seek': 190596, 'start': 1915.64, 'end': 1928.92, 'text': ' pass? Does anyone have any ideas? Exactly. Exactly. So this neural network is just, like it was just born,', 'tokens': [50848, 1320, 30, 4402, 2878, 362, 604, 3487, 30, 7587, 13, 7587, 13, 407, 341, 18161, 3209, 307, 445, 11, 411, 309, 390, 445, 4232, 11, 51512], 'temperature': 0.0, 'avg_logprob': -0.15656173706054688, 'compression_ratio': 1.6175298804780875, 'no_speech_prob': 0.011853840202093124}, {'id': 342, 'seek': 190596, 'start': 1928.92, 'end': 1934.04, 'text': \" right? It has no information about the, the world or this class. It doesn't know what four and five\", 'tokens': [51512, 558, 30, 467, 575, 572, 1589, 466, 264, 11, 264, 1002, 420, 341, 1508, 13, 467, 1177, 380, 458, 437, 1451, 293, 1732, 51768], 'temperature': 0.0, 'avg_logprob': -0.15656173706054688, 'compression_ratio': 1.6175298804780875, 'no_speech_prob': 0.011853840202093124}, {'id': 343, 'seek': 193404, 'start': 1934.04, 'end': 1941.1599999999999, 'text': ' mean or what the notion of passing or failing means, right? So exactly right. This neural network has', 'tokens': [50364, 914, 420, 437, 264, 10710, 295, 8437, 420, 18223, 1355, 11, 558, 30, 407, 2293, 558, 13, 639, 18161, 3209, 575, 50720], 'temperature': 0.0, 'avg_logprob': -0.09872863406226748, 'compression_ratio': 1.7700348432055748, 'no_speech_prob': 0.0033217323943972588}, {'id': 344, 'seek': 193404, 'start': 1941.1599999999999, 'end': 1946.44, 'text': \" not been trained. You can think of it kind of as a baby. It hasn't learned anything yet. So our job\", 'tokens': [50720, 406, 668, 8895, 13, 509, 393, 519, 295, 309, 733, 295, 382, 257, 3186, 13, 467, 6132, 380, 3264, 1340, 1939, 13, 407, 527, 1691, 50984], 'temperature': 0.0, 'avg_logprob': -0.09872863406226748, 'compression_ratio': 1.7700348432055748, 'no_speech_prob': 0.0033217323943972588}, {'id': 345, 'seek': 193404, 'start': 1946.44, 'end': 1951.72, 'text': ' firstly is to train it. And part of that understanding is we first need to tell the neural network when', 'tokens': [50984, 27376, 307, 281, 3847, 309, 13, 400, 644, 295, 300, 3701, 307, 321, 700, 643, 281, 980, 264, 18161, 3209, 562, 51248], 'temperature': 0.0, 'avg_logprob': -0.09872863406226748, 'compression_ratio': 1.7700348432055748, 'no_speech_prob': 0.0033217323943972588}, {'id': 346, 'seek': 193404, 'start': 1951.72, 'end': 1956.84, 'text': ' it makes mistakes, right? So mathematically, we should now think about how we can answer this question,', 'tokens': [51248, 309, 1669, 8038, 11, 558, 30, 407, 44003, 11, 321, 820, 586, 519, 466, 577, 321, 393, 1867, 341, 1168, 11, 51504], 'temperature': 0.0, 'avg_logprob': -0.09872863406226748, 'compression_ratio': 1.7700348432055748, 'no_speech_prob': 0.0033217323943972588}, {'id': 347, 'seek': 193404, 'start': 1956.84, 'end': 1962.28, 'text': ' which is, does, did my neural network make a mistake? And if it made a mistake, how can I tell it,', 'tokens': [51504, 597, 307, 11, 775, 11, 630, 452, 18161, 3209, 652, 257, 6146, 30, 400, 498, 309, 1027, 257, 6146, 11, 577, 393, 286, 980, 309, 11, 51776], 'temperature': 0.0, 'avg_logprob': -0.09872863406226748, 'compression_ratio': 1.7700348432055748, 'no_speech_prob': 0.0033217323943972588}, {'id': 348, 'seek': 196228, 'start': 1962.28, 'end': 1967.48, 'text': ' how big of a mistake it was so that the next time it sees this data point, can it do better?', 'tokens': [50364, 577, 955, 295, 257, 6146, 309, 390, 370, 300, 264, 958, 565, 309, 8194, 341, 1412, 935, 11, 393, 309, 360, 1101, 30, 50624], 'temperature': 0.0, 'avg_logprob': -0.08991644932673527, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0005355481989681721}, {'id': 349, 'seek': 196228, 'start': 1967.48, 'end': 1974.52, 'text': ' Minimize that mistake. So in neural network language, those mistakes are called losses, right? And', 'tokens': [50624, 2829, 43890, 300, 6146, 13, 407, 294, 18161, 3209, 2856, 11, 729, 8038, 366, 1219, 15352, 11, 558, 30, 400, 50976], 'temperature': 0.0, 'avg_logprob': -0.08991644932673527, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0005355481989681721}, {'id': 350, 'seek': 196228, 'start': 1974.52, 'end': 1979.0, 'text': \" specifically, you want to define what's called a loss function, which is going to take as input\", 'tokens': [50976, 4682, 11, 291, 528, 281, 6964, 437, 311, 1219, 257, 4470, 2445, 11, 597, 307, 516, 281, 747, 382, 4846, 51200], 'temperature': 0.0, 'avg_logprob': -0.08991644932673527, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0005355481989681721}, {'id': 351, 'seek': 196228, 'start': 1979.6399999999999, 'end': 1985.32, 'text': ' your prediction and the true prediction, right? And how far away your prediction is from the', 'tokens': [51232, 428, 17630, 293, 264, 2074, 17630, 11, 558, 30, 400, 577, 1400, 1314, 428, 17630, 307, 490, 264, 51516], 'temperature': 0.0, 'avg_logprob': -0.08991644932673527, 'compression_ratio': 1.6521739130434783, 'no_speech_prob': 0.0005355481989681721}, {'id': 352, 'seek': 198532, 'start': 1985.32, 'end': 1992.6799999999998, 'text': \" true prediction tells you how big of a loss there is, right? So for example, let's say we want to build\", 'tokens': [50364, 2074, 17630, 5112, 291, 577, 955, 295, 257, 4470, 456, 307, 11, 558, 30, 407, 337, 1365, 11, 718, 311, 584, 321, 528, 281, 1322, 50732], 'temperature': 0.0, 'avg_logprob': -0.0959845204507151, 'compression_ratio': 1.5912698412698412, 'no_speech_prob': 0.0001608917082194239}, {'id': 353, 'seek': 198532, 'start': 1993.32, 'end': 1999.6399999999999, 'text': ' a neural network to do classification of, or sorry, actually even before that, I want to maybe give', 'tokens': [50764, 257, 18161, 3209, 281, 360, 21538, 295, 11, 420, 2597, 11, 767, 754, 949, 300, 11, 286, 528, 281, 1310, 976, 51080], 'temperature': 0.0, 'avg_logprob': -0.0959845204507151, 'compression_ratio': 1.5912698412698412, 'no_speech_prob': 0.0001608917082194239}, {'id': 354, 'seek': 198532, 'start': 1999.6399999999999, 'end': 2005.48, 'text': ' you some terminology. So there are multiple different ways of saying the same thing in neural networks', 'tokens': [51080, 291, 512, 27575, 13, 407, 456, 366, 3866, 819, 2098, 295, 1566, 264, 912, 551, 294, 18161, 9590, 51372], 'temperature': 0.0, 'avg_logprob': -0.0959845204507151, 'compression_ratio': 1.5912698412698412, 'no_speech_prob': 0.0001608917082194239}, {'id': 355, 'seek': 198532, 'start': 2005.48, 'end': 2010.2, 'text': ' and deep learning. So what I just described as a loss function is also commonly referred to as', 'tokens': [51372, 293, 2452, 2539, 13, 407, 437, 286, 445, 7619, 382, 257, 4470, 2445, 307, 611, 12719, 10839, 281, 382, 51608], 'temperature': 0.0, 'avg_logprob': -0.0959845204507151, 'compression_ratio': 1.5912698412698412, 'no_speech_prob': 0.0001608917082194239}, {'id': 356, 'seek': 201020, 'start': 2010.2, 'end': 2015.48, 'text': ' an objective function, empirical risk, a cost function. These are all exactly the same thing.', 'tokens': [50364, 364, 10024, 2445, 11, 31886, 3148, 11, 257, 2063, 2445, 13, 1981, 366, 439, 2293, 264, 912, 551, 13, 50628], 'temperature': 0.0, 'avg_logprob': -0.06638329070911073, 'compression_ratio': 1.8482490272373542, 'no_speech_prob': 0.002756583271548152}, {'id': 357, 'seek': 201020, 'start': 2015.48, 'end': 2019.8, 'text': \" They're all the way for us to train the neural network, to teach the neural network when it makes\", 'tokens': [50628, 814, 434, 439, 264, 636, 337, 505, 281, 3847, 264, 18161, 3209, 11, 281, 2924, 264, 18161, 3209, 562, 309, 1669, 50844], 'temperature': 0.0, 'avg_logprob': -0.06638329070911073, 'compression_ratio': 1.8482490272373542, 'no_speech_prob': 0.002756583271548152}, {'id': 358, 'seek': 201020, 'start': 2019.8, 'end': 2026.2, 'text': ' mistakes. And what we really ultimately want to do is over the course of an entire data set,', 'tokens': [50844, 8038, 13, 400, 437, 321, 534, 6284, 528, 281, 360, 307, 670, 264, 1164, 295, 364, 2302, 1412, 992, 11, 51164], 'temperature': 0.0, 'avg_logprob': -0.06638329070911073, 'compression_ratio': 1.8482490272373542, 'no_speech_prob': 0.002756583271548152}, {'id': 359, 'seek': 201020, 'start': 2026.2, 'end': 2031.64, 'text': ' not just one data point of mistakes, we want to say over the entire data set, we want to minimize', 'tokens': [51164, 406, 445, 472, 1412, 935, 295, 8038, 11, 321, 528, 281, 584, 670, 264, 2302, 1412, 992, 11, 321, 528, 281, 17522, 51436], 'temperature': 0.0, 'avg_logprob': -0.06638329070911073, 'compression_ratio': 1.8482490272373542, 'no_speech_prob': 0.002756583271548152}, {'id': 360, 'seek': 201020, 'start': 2031.64, 'end': 2037.88, 'text': ' all of the mistakes on average that this neural network makes. So if we look at the problem,', 'tokens': [51436, 439, 295, 264, 8038, 322, 4274, 300, 341, 18161, 3209, 1669, 13, 407, 498, 321, 574, 412, 264, 1154, 11, 51748], 'temperature': 0.0, 'avg_logprob': -0.06638329070911073, 'compression_ratio': 1.8482490272373542, 'no_speech_prob': 0.002756583271548152}, {'id': 361, 'seek': 203788, 'start': 2037.88, 'end': 2042.6000000000001, 'text': \" like I said, of binary classification, will I pass this class or will I not? There's a yes or\", 'tokens': [50364, 411, 286, 848, 11, 295, 17434, 21538, 11, 486, 286, 1320, 341, 1508, 420, 486, 286, 406, 30, 821, 311, 257, 2086, 420, 50600], 'temperature': 0.0, 'avg_logprob': -0.1671419047346019, 'compression_ratio': 1.6042553191489362, 'no_speech_prob': 0.0005789223359897733}, {'id': 362, 'seek': 203788, 'start': 2042.6000000000001, 'end': 2048.76, 'text': \" a no answer. That means binary classification. Now we can use what's called a loss function of\", 'tokens': [50600, 257, 572, 1867, 13, 663, 1355, 17434, 21538, 13, 823, 321, 393, 764, 437, 311, 1219, 257, 4470, 2445, 295, 50908], 'temperature': 0.0, 'avg_logprob': -0.1671419047346019, 'compression_ratio': 1.6042553191489362, 'no_speech_prob': 0.0005789223359897733}, {'id': 363, 'seek': 203788, 'start': 2048.76, 'end': 2053.88, 'text': \" the softmax cross entropy loss. And for those of you who aren't familiar, this notion of cross\", 'tokens': [50908, 264, 2787, 41167, 3278, 30867, 4470, 13, 400, 337, 729, 295, 291, 567, 3212, 380, 4963, 11, 341, 10710, 295, 3278, 51164], 'temperature': 0.0, 'avg_logprob': -0.1671419047346019, 'compression_ratio': 1.6042553191489362, 'no_speech_prob': 0.0005789223359897733}, {'id': 364, 'seek': 203788, 'start': 2053.88, 'end': 2062.92, 'text': ' entropy is actually developed here at MIT by Shod Klan, Shod, excuse me, yes, Claude Shannon,', 'tokens': [51164, 30867, 307, 767, 4743, 510, 412, 13100, 538, 1160, 378, 591, 8658, 11, 1160, 378, 11, 8960, 385, 11, 2086, 11, 12947, 2303, 28974, 11, 51616], 'temperature': 0.0, 'avg_logprob': -0.1671419047346019, 'compression_ratio': 1.6042553191489362, 'no_speech_prob': 0.0005789223359897733}, {'id': 365, 'seek': 206292, 'start': 2062.92, 'end': 2069.2400000000002, 'text': \" who is visionary. He did his master's here over 50 years ago. He introduced this notion of\", 'tokens': [50364, 567, 307, 49442, 13, 634, 630, 702, 4505, 311, 510, 670, 2625, 924, 2057, 13, 634, 7268, 341, 10710, 295, 50680], 'temperature': 0.0, 'avg_logprob': -0.12097720689671014, 'compression_ratio': 1.5826446280991735, 'no_speech_prob': 0.0014092718483880162}, {'id': 366, 'seek': 206292, 'start': 2069.2400000000002, 'end': 2076.52, 'text': ' cross entropy, and that was pivotal in the ability for us to train these types of neural networks,', 'tokens': [50680, 3278, 30867, 11, 293, 300, 390, 39078, 294, 264, 3485, 337, 505, 281, 3847, 613, 3467, 295, 18161, 9590, 11, 51044], 'temperature': 0.0, 'avg_logprob': -0.12097720689671014, 'compression_ratio': 1.5826446280991735, 'no_speech_prob': 0.0014092718483880162}, {'id': 367, 'seek': 206292, 'start': 2076.52, 'end': 2084.12, 'text': \" even now into the future. So let's start by, instead of predicting a binary cross entropy output,\", 'tokens': [51044, 754, 586, 666, 264, 2027, 13, 407, 718, 311, 722, 538, 11, 2602, 295, 32884, 257, 17434, 3278, 30867, 5598, 11, 51424], 'temperature': 0.0, 'avg_logprob': -0.12097720689671014, 'compression_ratio': 1.5826446280991735, 'no_speech_prob': 0.0014092718483880162}, {'id': 368, 'seek': 206292, 'start': 2084.12, 'end': 2089.88, 'text': \" what if we wanted to predict a final grade of your class score, for example? That's no longer a\", 'tokens': [51424, 437, 498, 321, 1415, 281, 6069, 257, 2572, 7204, 295, 428, 1508, 6175, 11, 337, 1365, 30, 663, 311, 572, 2854, 257, 51712], 'temperature': 0.0, 'avg_logprob': -0.12097720689671014, 'compression_ratio': 1.5826446280991735, 'no_speech_prob': 0.0014092718483880162}, {'id': 369, 'seek': 208988, 'start': 2089.88, 'end': 2094.6, 'text': \" binary output, yes or no? It's actually a continuous variable, right? It's the grade, let's say out of\", 'tokens': [50364, 17434, 5598, 11, 2086, 420, 572, 30, 467, 311, 767, 257, 10957, 7006, 11, 558, 30, 467, 311, 264, 7204, 11, 718, 311, 584, 484, 295, 50600], 'temperature': 0.0, 'avg_logprob': -0.10760760307312012, 'compression_ratio': 1.6655172413793105, 'no_speech_prob': 0.0010156388161703944}, {'id': 370, 'seek': 208988, 'start': 2094.6, 'end': 2100.92, 'text': ' a hundred points. What is the value of your score in the class project, right? For this type of loss,', 'tokens': [50600, 257, 3262, 2793, 13, 708, 307, 264, 2158, 295, 428, 6175, 294, 264, 1508, 1716, 11, 558, 30, 1171, 341, 2010, 295, 4470, 11, 50916], 'temperature': 0.0, 'avg_logprob': -0.10760760307312012, 'compression_ratio': 1.6655172413793105, 'no_speech_prob': 0.0010156388161703944}, {'id': 371, 'seek': 208988, 'start': 2100.92, 'end': 2104.36, 'text': \" we can use what's called a mean squared error loss. You can think of this literally as just\", 'tokens': [50916, 321, 393, 764, 437, 311, 1219, 257, 914, 8889, 6713, 4470, 13, 509, 393, 519, 295, 341, 3736, 382, 445, 51088], 'temperature': 0.0, 'avg_logprob': -0.10760760307312012, 'compression_ratio': 1.6655172413793105, 'no_speech_prob': 0.0010156388161703944}, {'id': 372, 'seek': 208988, 'start': 2104.36, 'end': 2110.2000000000003, 'text': ' subtracting your predicted grade from the true grade and minimizing that distance apart.', 'tokens': [51088, 16390, 278, 428, 19147, 7204, 490, 264, 2074, 7204, 293, 46608, 300, 4560, 4936, 13, 51380], 'temperature': 0.0, 'avg_logprob': -0.10760760307312012, 'compression_ratio': 1.6655172413793105, 'no_speech_prob': 0.0010156388161703944}, {'id': 373, 'seek': 208988, 'start': 2112.44, 'end': 2118.44, 'text': \" So I think now we're ready to really put all of this information together and tackle this problem\", 'tokens': [51492, 407, 286, 519, 586, 321, 434, 1919, 281, 534, 829, 439, 295, 341, 1589, 1214, 293, 14896, 341, 1154, 51792], 'temperature': 0.0, 'avg_logprob': -0.10760760307312012, 'compression_ratio': 1.6655172413793105, 'no_speech_prob': 0.0010156388161703944}, {'id': 374, 'seek': 211844, 'start': 2118.44, 'end': 2126.12, 'text': ' of training a neural network, right? To not just identify how erroneous it is, how large its', 'tokens': [50364, 295, 3097, 257, 18161, 3209, 11, 558, 30, 1407, 406, 445, 5876, 577, 1189, 26446, 563, 309, 307, 11, 577, 2416, 1080, 50748], 'temperature': 0.0, 'avg_logprob': -0.09641981648874808, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 0.0017001235391944647}, {'id': 375, 'seek': 211844, 'start': 2126.12, 'end': 2131.4, 'text': ' loss is, but more importantly, minimize that loss as a function of seeing all of this training data', 'tokens': [50748, 4470, 307, 11, 457, 544, 8906, 11, 17522, 300, 4470, 382, 257, 2445, 295, 2577, 439, 295, 341, 3097, 1412, 51012], 'temperature': 0.0, 'avg_logprob': -0.09641981648874808, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 0.0017001235391944647}, {'id': 376, 'seek': 211844, 'start': 2131.4, 'end': 2137.16, 'text': ' that is observed. So we know that we want to find this neural network, like we mentioned before,', 'tokens': [51012, 300, 307, 13095, 13, 407, 321, 458, 300, 321, 528, 281, 915, 341, 18161, 3209, 11, 411, 321, 2835, 949, 11, 51300], 'temperature': 0.0, 'avg_logprob': -0.09641981648874808, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 0.0017001235391944647}, {'id': 377, 'seek': 211844, 'start': 2137.16, 'end': 2144.12, 'text': ' that minimizes this empirical risk or this empirical loss averaged across our entire data set.', 'tokens': [51300, 300, 4464, 5660, 341, 31886, 3148, 420, 341, 31886, 4470, 18247, 2980, 2108, 527, 2302, 1412, 992, 13, 51648], 'temperature': 0.0, 'avg_logprob': -0.09641981648874808, 'compression_ratio': 1.7297297297297298, 'no_speech_prob': 0.0017001235391944647}, {'id': 378, 'seek': 214412, 'start': 2144.3599999999997, 'end': 2151.4, 'text': \" Now this means that we want to find mathematically these W's, right, that minimize j of W.\", 'tokens': [50376, 823, 341, 1355, 300, 321, 528, 281, 915, 44003, 613, 343, 311, 11, 558, 11, 300, 17522, 361, 295, 343, 13, 50728], 'temperature': 0.0, 'avg_logprob': -0.1579409127283578, 'compression_ratio': 1.7027027027027026, 'no_speech_prob': 0.0014546692837029696}, {'id': 379, 'seek': 214412, 'start': 2151.4, 'end': 2156.44, 'text': ' J of W is our loss function, averaged over our entire data set, and W is our weight. So we want', 'tokens': [50728, 508, 295, 343, 307, 527, 4470, 2445, 11, 18247, 2980, 670, 527, 2302, 1412, 992, 11, 293, 343, 307, 527, 3364, 13, 407, 321, 528, 50980], 'temperature': 0.0, 'avg_logprob': -0.1579409127283578, 'compression_ratio': 1.7027027027027026, 'no_speech_prob': 0.0014546692837029696}, {'id': 380, 'seek': 214412, 'start': 2156.44, 'end': 2164.44, 'text': ' to find the set of weights that on average is going to give us the smallest loss as possible.', 'tokens': [50980, 281, 915, 264, 992, 295, 17443, 300, 322, 4274, 307, 516, 281, 976, 505, 264, 16998, 4470, 382, 1944, 13, 51380], 'temperature': 0.0, 'avg_logprob': -0.1579409127283578, 'compression_ratio': 1.7027027027027026, 'no_speech_prob': 0.0014546692837029696}, {'id': 381, 'seek': 214412, 'start': 2165.56, 'end': 2170.8399999999997, 'text': \" Now remember that W here is just a list, basically it's just a group of all of the weights in our\", 'tokens': [51436, 823, 1604, 300, 343, 510, 307, 445, 257, 1329, 11, 1936, 309, 311, 445, 257, 1594, 295, 439, 295, 264, 17443, 294, 527, 51700], 'temperature': 0.0, 'avg_logprob': -0.1579409127283578, 'compression_ratio': 1.7027027027027026, 'no_speech_prob': 0.0014546692837029696}, {'id': 382, 'seek': 217084, 'start': 2170.84, 'end': 2176.36, 'text': \" neural network. You may have hundreds of weights and a very, very small neural network, or in today's\", 'tokens': [50364, 18161, 3209, 13, 509, 815, 362, 6779, 295, 17443, 293, 257, 588, 11, 588, 1359, 18161, 3209, 11, 420, 294, 965, 311, 50640], 'temperature': 0.0, 'avg_logprob': -0.12627447265939615, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0001739765575621277}, {'id': 383, 'seek': 217084, 'start': 2176.36, 'end': 2181.1600000000003, 'text': ' neural networks you may have billions or trillions of weights, and you want to find what is the value of', 'tokens': [50640, 18161, 9590, 291, 815, 362, 17375, 420, 504, 46279, 295, 17443, 11, 293, 291, 528, 281, 915, 437, 307, 264, 2158, 295, 50880], 'temperature': 0.0, 'avg_logprob': -0.12627447265939615, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0001739765575621277}, {'id': 384, 'seek': 217084, 'start': 2181.1600000000003, 'end': 2185.48, 'text': ' every single one of these weights that is going to result in the smallest loss as possible.', 'tokens': [50880, 633, 2167, 472, 295, 613, 17443, 300, 307, 516, 281, 1874, 294, 264, 16998, 4470, 382, 1944, 13, 51096], 'temperature': 0.0, 'avg_logprob': -0.12627447265939615, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0001739765575621277}, {'id': 385, 'seek': 217084, 'start': 2186.6000000000004, 'end': 2193.1600000000003, 'text': ' Now how can you do this? Remember that our loss function, j of W, is just a function of our weights,', 'tokens': [51152, 823, 577, 393, 291, 360, 341, 30, 5459, 300, 527, 4470, 2445, 11, 361, 295, 343, 11, 307, 445, 257, 2445, 295, 527, 17443, 11, 51480], 'temperature': 0.0, 'avg_logprob': -0.12627447265939615, 'compression_ratio': 1.789237668161435, 'no_speech_prob': 0.0001739765575621277}, {'id': 386, 'seek': 219316, 'start': 2193.16, 'end': 2200.2799999999997, 'text': ' right? So for any instantiation of our weights, we can compute a scalar value of how', 'tokens': [50364, 558, 30, 407, 337, 604, 9836, 6642, 295, 527, 17443, 11, 321, 393, 14722, 257, 39684, 2158, 295, 577, 50720], 'temperature': 0.0, 'avg_logprob': -0.12609852801312457, 'compression_ratio': 1.7857142857142858, 'no_speech_prob': 0.0008423854596912861}, {'id': 387, 'seek': 219316, 'start': 2200.2799999999997, 'end': 2206.44, 'text': \" erroneous would our neural network be for this instantiation of our weights. So let's try and\", 'tokens': [50720, 1189, 26446, 563, 576, 527, 18161, 3209, 312, 337, 341, 9836, 6642, 295, 527, 17443, 13, 407, 718, 311, 853, 293, 51028], 'temperature': 0.0, 'avg_logprob': -0.12609852801312457, 'compression_ratio': 1.7857142857142858, 'no_speech_prob': 0.0008423854596912861}, {'id': 388, 'seek': 219316, 'start': 2206.44, 'end': 2211.48, 'text': ' visualize, for example, in a very simple example of a two-dimensional space where we have only two', 'tokens': [51028, 23273, 11, 337, 1365, 11, 294, 257, 588, 2199, 1365, 295, 257, 732, 12, 18759, 1901, 689, 321, 362, 787, 732, 51280], 'temperature': 0.0, 'avg_logprob': -0.12609852801312457, 'compression_ratio': 1.7857142857142858, 'no_speech_prob': 0.0008423854596912861}, {'id': 389, 'seek': 219316, 'start': 2211.48, 'end': 2217.3199999999997, 'text': ' weights, extremely simple neural network here, very small, two-weight neural network, and we want', 'tokens': [51280, 17443, 11, 4664, 2199, 18161, 3209, 510, 11, 588, 1359, 11, 732, 12, 12329, 18161, 3209, 11, 293, 321, 528, 51572], 'temperature': 0.0, 'avg_logprob': -0.12609852801312457, 'compression_ratio': 1.7857142857142858, 'no_speech_prob': 0.0008423854596912861}, {'id': 390, 'seek': 221732, 'start': 2217.32, 'end': 2223.56, 'text': ' to find what are the optimal weights that would train this neural network. We can plot, basically,', 'tokens': [50364, 281, 915, 437, 366, 264, 16252, 17443, 300, 576, 3847, 341, 18161, 3209, 13, 492, 393, 7542, 11, 1936, 11, 50676], 'temperature': 0.0, 'avg_logprob': -0.10788443565368652, 'compression_ratio': 1.6638655462184875, 'no_speech_prob': 0.0003352779312990606}, {'id': 391, 'seek': 221732, 'start': 2223.56, 'end': 2230.84, 'text': ' the loss, how erroneous the neural network is for every single instantiation of these two weights,', 'tokens': [50676, 264, 4470, 11, 577, 1189, 26446, 563, 264, 18161, 3209, 307, 337, 633, 2167, 9836, 6642, 295, 613, 732, 17443, 11, 51040], 'temperature': 0.0, 'avg_logprob': -0.10788443565368652, 'compression_ratio': 1.6638655462184875, 'no_speech_prob': 0.0003352779312990606}, {'id': 392, 'seek': 221732, 'start': 2230.84, 'end': 2236.28, 'text': \" right? This is a huge space, it's an infinite space, but still we can try to, we can have a function\", 'tokens': [51040, 558, 30, 639, 307, 257, 2603, 1901, 11, 309, 311, 364, 13785, 1901, 11, 457, 920, 321, 393, 853, 281, 11, 321, 393, 362, 257, 2445, 51312], 'temperature': 0.0, 'avg_logprob': -0.10788443565368652, 'compression_ratio': 1.6638655462184875, 'no_speech_prob': 0.0003352779312990606}, {'id': 393, 'seek': 221732, 'start': 2236.28, 'end': 2242.44, 'text': ' that evaluates at every point in this space. Now what we ultimately want to do is, again, we want', 'tokens': [51312, 300, 6133, 1024, 412, 633, 935, 294, 341, 1901, 13, 823, 437, 321, 6284, 528, 281, 360, 307, 11, 797, 11, 321, 528, 51620], 'temperature': 0.0, 'avg_logprob': -0.10788443565368652, 'compression_ratio': 1.6638655462184875, 'no_speech_prob': 0.0003352779312990606}, {'id': 394, 'seek': 224244, 'start': 2242.44, 'end': 2250.92, 'text': \" to find which set of W's will give us the smallest loss possible. That means basically the lowest\", 'tokens': [50364, 281, 915, 597, 992, 295, 343, 311, 486, 976, 505, 264, 16998, 4470, 1944, 13, 663, 1355, 1936, 264, 12437, 50788], 'temperature': 0.0, 'avg_logprob': -0.08466047110016812, 'compression_ratio': 1.7123893805309736, 'no_speech_prob': 0.0012839061673730612}, {'id': 395, 'seek': 224244, 'start': 2250.92, 'end': 2257.0, 'text': \" point on this landscape that you can see here, where is the W's that bring us to that lowest point?\", 'tokens': [50788, 935, 322, 341, 9661, 300, 291, 393, 536, 510, 11, 689, 307, 264, 343, 311, 300, 1565, 505, 281, 300, 12437, 935, 30, 51092], 'temperature': 0.0, 'avg_logprob': -0.08466047110016812, 'compression_ratio': 1.7123893805309736, 'no_speech_prob': 0.0012839061673730612}, {'id': 396, 'seek': 224244, 'start': 2258.92, 'end': 2264.04, 'text': ' The way that we do this is actually just by firstly starting at a random place, we have no idea', 'tokens': [51188, 440, 636, 300, 321, 360, 341, 307, 767, 445, 538, 27376, 2891, 412, 257, 4974, 1081, 11, 321, 362, 572, 1558, 51444], 'temperature': 0.0, 'avg_logprob': -0.08466047110016812, 'compression_ratio': 1.7123893805309736, 'no_speech_prob': 0.0012839061673730612}, {'id': 397, 'seek': 224244, 'start': 2264.04, 'end': 2269.0, 'text': \" where to start, so pick a random place to start in this space, and let's start there. At this\", 'tokens': [51444, 689, 281, 722, 11, 370, 1888, 257, 4974, 1081, 281, 722, 294, 341, 1901, 11, 293, 718, 311, 722, 456, 13, 1711, 341, 51692], 'temperature': 0.0, 'avg_logprob': -0.08466047110016812, 'compression_ratio': 1.7123893805309736, 'no_speech_prob': 0.0012839061673730612}, {'id': 398, 'seek': 226900, 'start': 2269.08, 'end': 2274.36, 'text': \" location, let's evaluate our neural network. We can compute the loss at this specific location,\", 'tokens': [50368, 4914, 11, 718, 311, 13059, 527, 18161, 3209, 13, 492, 393, 14722, 264, 4470, 412, 341, 2685, 4914, 11, 50632], 'temperature': 0.0, 'avg_logprob': -0.08091474879871716, 'compression_ratio': 2.042016806722689, 'no_speech_prob': 0.001596967689692974}, {'id': 399, 'seek': 226900, 'start': 2274.92, 'end': 2280.44, 'text': ' and on top of that, we can actually compute how the loss is changing. We can compute the gradient', 'tokens': [50660, 293, 322, 1192, 295, 300, 11, 321, 393, 767, 14722, 577, 264, 4470, 307, 4473, 13, 492, 393, 14722, 264, 16235, 50936], 'temperature': 0.0, 'avg_logprob': -0.08091474879871716, 'compression_ratio': 2.042016806722689, 'no_speech_prob': 0.001596967689692974}, {'id': 400, 'seek': 226900, 'start': 2280.44, 'end': 2285.8, 'text': ' of the loss because our loss function is a continuous function, right? So we can actually compute', 'tokens': [50936, 295, 264, 4470, 570, 527, 4470, 2445, 307, 257, 10957, 2445, 11, 558, 30, 407, 321, 393, 767, 14722, 51204], 'temperature': 0.0, 'avg_logprob': -0.08091474879871716, 'compression_ratio': 2.042016806722689, 'no_speech_prob': 0.001596967689692974}, {'id': 401, 'seek': 226900, 'start': 2285.8, 'end': 2292.36, 'text': ' derivatives of our function across the space of our weights, and the gradient tells us the direction', 'tokens': [51204, 33733, 295, 527, 2445, 2108, 264, 1901, 295, 527, 17443, 11, 293, 264, 16235, 5112, 505, 264, 3513, 51532], 'temperature': 0.0, 'avg_logprob': -0.08091474879871716, 'compression_ratio': 2.042016806722689, 'no_speech_prob': 0.001596967689692974}, {'id': 402, 'seek': 226900, 'start': 2292.36, 'end': 2297.48, 'text': ' of the highest point, right? So from where we stand, the gradient tells us where we should go', 'tokens': [51532, 295, 264, 6343, 935, 11, 558, 30, 407, 490, 689, 321, 1463, 11, 264, 16235, 5112, 505, 689, 321, 820, 352, 51788], 'temperature': 0.0, 'avg_logprob': -0.08091474879871716, 'compression_ratio': 2.042016806722689, 'no_speech_prob': 0.001596967689692974}, {'id': 403, 'seek': 229748, 'start': 2298.04, 'end': 2303.0, 'text': \" to increase our loss. Now, of course, we don't want to increase our loss, we want to decrease our\", 'tokens': [50392, 281, 3488, 527, 4470, 13, 823, 11, 295, 1164, 11, 321, 500, 380, 528, 281, 3488, 527, 4470, 11, 321, 528, 281, 11514, 527, 50640], 'temperature': 0.0, 'avg_logprob': -0.07463465796576606, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0004372406401671469}, {'id': 404, 'seek': 229748, 'start': 2303.0, 'end': 2308.44, 'text': ' loss, so we negate our gradient, and we take a step in the opposite direction of the gradient. That', 'tokens': [50640, 4470, 11, 370, 321, 2485, 473, 527, 16235, 11, 293, 321, 747, 257, 1823, 294, 264, 6182, 3513, 295, 264, 16235, 13, 663, 50912], 'temperature': 0.0, 'avg_logprob': -0.07463465796576606, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0004372406401671469}, {'id': 405, 'seek': 229748, 'start': 2308.44, 'end': 2314.52, 'text': ' brings us one step closer to the bottom of the landscape, and we just keep repeating this process,', 'tokens': [50912, 5607, 505, 472, 1823, 4966, 281, 264, 2767, 295, 264, 9661, 11, 293, 321, 445, 1066, 18617, 341, 1399, 11, 51216], 'temperature': 0.0, 'avg_logprob': -0.07463465796576606, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0004372406401671469}, {'id': 406, 'seek': 229748, 'start': 2314.52, 'end': 2318.92, 'text': ' right? Over and over again, we evaluate the neural network at this new location, compute its', 'tokens': [51216, 558, 30, 4886, 293, 670, 797, 11, 321, 13059, 264, 18161, 3209, 412, 341, 777, 4914, 11, 14722, 1080, 51436], 'temperature': 0.0, 'avg_logprob': -0.07463465796576606, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0004372406401671469}, {'id': 407, 'seek': 229748, 'start': 2318.92, 'end': 2324.68, 'text': ' gradient, and step in that new direction. We keep traversing this landscape until we converge to', 'tokens': [51436, 16235, 11, 293, 1823, 294, 300, 777, 3513, 13, 492, 1066, 23149, 278, 341, 9661, 1826, 321, 41881, 281, 51724], 'temperature': 0.0, 'avg_logprob': -0.07463465796576606, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0004372406401671469}, {'id': 408, 'seek': 232468, 'start': 2324.68, 'end': 2331.96, 'text': ' the minimum. We can really summarize this algorithm, which is known formally as gradient descent,', 'tokens': [50364, 264, 7285, 13, 492, 393, 534, 20858, 341, 9284, 11, 597, 307, 2570, 25983, 382, 16235, 23475, 11, 50728], 'temperature': 0.0, 'avg_logprob': -0.0931406021118164, 'compression_ratio': 1.8452380952380953, 'no_speech_prob': 0.0005525971064344049}, {'id': 409, 'seek': 232468, 'start': 2331.96, 'end': 2336.68, 'text': ' right? So gradient descent simply can be written like this. We initialize all of our weights,', 'tokens': [50728, 558, 30, 407, 16235, 23475, 2935, 393, 312, 3720, 411, 341, 13, 492, 5883, 1125, 439, 295, 527, 17443, 11, 50964], 'temperature': 0.0, 'avg_logprob': -0.0931406021118164, 'compression_ratio': 1.8452380952380953, 'no_speech_prob': 0.0005525971064344049}, {'id': 410, 'seek': 232468, 'start': 2337.3199999999997, 'end': 2341.48, 'text': ' right? This can be two weights, like you saw in the previous example, it can be billions of', 'tokens': [50996, 558, 30, 639, 393, 312, 732, 17443, 11, 411, 291, 1866, 294, 264, 3894, 1365, 11, 309, 393, 312, 17375, 295, 51204], 'temperature': 0.0, 'avg_logprob': -0.0931406021118164, 'compression_ratio': 1.8452380952380953, 'no_speech_prob': 0.0005525971064344049}, {'id': 411, 'seek': 232468, 'start': 2341.48, 'end': 2348.04, 'text': ' weights, like in real neural networks. We compute this gradient of the partial derivative', 'tokens': [51204, 17443, 11, 411, 294, 957, 18161, 9590, 13, 492, 14722, 341, 16235, 295, 264, 14641, 13760, 51532], 'temperature': 0.0, 'avg_logprob': -0.0931406021118164, 'compression_ratio': 1.8452380952380953, 'no_speech_prob': 0.0005525971064344049}, {'id': 412, 'seek': 232468, 'start': 2348.6, 'end': 2352.68, 'text': ' of our loss with respect to the weights, and then we can update our weights in the opposite', 'tokens': [51560, 295, 527, 4470, 365, 3104, 281, 264, 17443, 11, 293, 550, 321, 393, 5623, 527, 17443, 294, 264, 6182, 51764], 'temperature': 0.0, 'avg_logprob': -0.0931406021118164, 'compression_ratio': 1.8452380952380953, 'no_speech_prob': 0.0005525971064344049}, {'id': 413, 'seek': 235268, 'start': 2352.68, 'end': 2359.7999999999997, 'text': ' direction of this gradient. So essentially, we just take this small amount, small step, you can', 'tokens': [50364, 3513, 295, 341, 16235, 13, 407, 4476, 11, 321, 445, 747, 341, 1359, 2372, 11, 1359, 1823, 11, 291, 393, 50720], 'temperature': 0.0, 'avg_logprob': -0.0843056471451469, 'compression_ratio': 1.7397769516728625, 'no_speech_prob': 0.0005032338085584342}, {'id': 414, 'seek': 235268, 'start': 2359.7999999999997, 'end': 2366.8399999999997, 'text': ' think of it, which here is denoted as eta, and we refer to this small step, right? This is', 'tokens': [50720, 519, 295, 309, 11, 597, 510, 307, 1441, 23325, 382, 32415, 11, 293, 321, 2864, 281, 341, 1359, 1823, 11, 558, 30, 639, 307, 51072], 'temperature': 0.0, 'avg_logprob': -0.0843056471451469, 'compression_ratio': 1.7397769516728625, 'no_speech_prob': 0.0005032338085584342}, {'id': 415, 'seek': 235268, 'start': 2366.8399999999997, 'end': 2371.8799999999997, 'text': \" commonly referred to as what's known as the learning rate. It's like how much we want to trust\", 'tokens': [51072, 12719, 10839, 281, 382, 437, 311, 2570, 382, 264, 2539, 3314, 13, 467, 311, 411, 577, 709, 321, 528, 281, 3361, 51324], 'temperature': 0.0, 'avg_logprob': -0.0843056471451469, 'compression_ratio': 1.7397769516728625, 'no_speech_prob': 0.0005032338085584342}, {'id': 416, 'seek': 235268, 'start': 2371.8799999999997, 'end': 2375.7999999999997, 'text': \" that gradient and step in the direction of that gradient. We'll talk more about this later,\", 'tokens': [51324, 300, 16235, 293, 1823, 294, 264, 3513, 295, 300, 16235, 13, 492, 603, 751, 544, 466, 341, 1780, 11, 51520], 'temperature': 0.0, 'avg_logprob': -0.0843056471451469, 'compression_ratio': 1.7397769516728625, 'no_speech_prob': 0.0005032338085584342}, {'id': 417, 'seek': 235268, 'start': 2376.44, 'end': 2382.52, 'text': ' but just to give you some sense of code, this algorithm is very well translatable to real code', 'tokens': [51552, 457, 445, 281, 976, 291, 512, 2020, 295, 3089, 11, 341, 9284, 307, 588, 731, 5105, 31415, 281, 957, 3089, 51856], 'temperature': 0.0, 'avg_logprob': -0.0843056471451469, 'compression_ratio': 1.7397769516728625, 'no_speech_prob': 0.0005032338085584342}, {'id': 418, 'seek': 238268, 'start': 2382.68, 'end': 2386.8399999999997, 'text': ' as well. For every line on the pseudo code you can see on the left, you can see corresponding', 'tokens': [50364, 382, 731, 13, 1171, 633, 1622, 322, 264, 35899, 3089, 291, 393, 536, 322, 264, 1411, 11, 291, 393, 536, 11760, 50572], 'temperature': 0.0, 'avg_logprob': -0.07762845357259114, 'compression_ratio': 1.7409638554216869, 'no_speech_prob': 0.0001511494192527607}, {'id': 419, 'seek': 238268, 'start': 2386.8399999999997, 'end': 2390.8399999999997, 'text': ' real code on the right that is runnable and directly implementable by all of you in your labs.', 'tokens': [50572, 957, 3089, 322, 264, 558, 300, 307, 1190, 77, 712, 293, 3838, 4445, 712, 538, 439, 295, 291, 294, 428, 20339, 13, 50772], 'temperature': 0.0, 'avg_logprob': -0.07762845357259114, 'compression_ratio': 1.7409638554216869, 'no_speech_prob': 0.0001511494192527607}, {'id': 420, 'seek': 238268, 'start': 2391.72, 'end': 2396.2799999999997, 'text': \" But now let's take a look specifically at this term here. This is the gradient. We touch very\", 'tokens': [50816, 583, 586, 718, 311, 747, 257, 574, 4682, 412, 341, 1433, 510, 13, 639, 307, 264, 16235, 13, 492, 2557, 588, 51044], 'temperature': 0.0, 'avg_logprob': -0.07762845357259114, 'compression_ratio': 1.7409638554216869, 'no_speech_prob': 0.0001511494192527607}, {'id': 421, 'seek': 238268, 'start': 2396.2799999999997, 'end': 2402.2, 'text': ' briefly on this in the visual example. This explains, like I said, how the loss is changing as a', 'tokens': [51044, 10515, 322, 341, 294, 264, 5056, 1365, 13, 639, 13948, 11, 411, 286, 848, 11, 577, 264, 4470, 307, 4473, 382, 257, 51340], 'temperature': 0.0, 'avg_logprob': -0.07762845357259114, 'compression_ratio': 1.7409638554216869, 'no_speech_prob': 0.0001511494192527607}, {'id': 422, 'seek': 238268, 'start': 2402.2, 'end': 2407.16, 'text': ' function of the weights, right? So as the weights move around, will my loss increase or decrease,', 'tokens': [51340, 2445, 295, 264, 17443, 11, 558, 30, 407, 382, 264, 17443, 1286, 926, 11, 486, 452, 4470, 3488, 420, 11514, 11, 51588], 'temperature': 0.0, 'avg_logprob': -0.07762845357259114, 'compression_ratio': 1.7409638554216869, 'no_speech_prob': 0.0001511494192527607}, {'id': 423, 'seek': 238268, 'start': 2407.16, 'end': 2411.48, 'text': ' and that will tell the neural network if it needs to move the weights in a certain direction or not.', 'tokens': [51588, 293, 300, 486, 980, 264, 18161, 3209, 498, 309, 2203, 281, 1286, 264, 17443, 294, 257, 1629, 3513, 420, 406, 13, 51804], 'temperature': 0.0, 'avg_logprob': -0.07762845357259114, 'compression_ratio': 1.7409638554216869, 'no_speech_prob': 0.0001511494192527607}, {'id': 424, 'seek': 241268, 'start': 2412.68, 'end': 2417.0, 'text': \" I never actually told you how to compute this, right? I think that's an extremely important\", 'tokens': [50364, 286, 1128, 767, 1907, 291, 577, 281, 14722, 341, 11, 558, 30, 286, 519, 300, 311, 364, 4664, 1021, 50580], 'temperature': 0.0, 'avg_logprob': -0.13399435679117838, 'compression_ratio': 1.7890625, 'no_speech_prob': 0.00021306317648850381}, {'id': 425, 'seek': 241268, 'start': 2417.0, 'end': 2422.3599999999997, 'text': \" part because if you don't know that, then you can't train your neural network. This is a critical\", 'tokens': [50580, 644, 570, 498, 291, 500, 380, 458, 300, 11, 550, 291, 393, 380, 3847, 428, 18161, 3209, 13, 639, 307, 257, 4924, 50848], 'temperature': 0.0, 'avg_logprob': -0.13399435679117838, 'compression_ratio': 1.7890625, 'no_speech_prob': 0.00021306317648850381}, {'id': 426, 'seek': 241268, 'start': 2422.3599999999997, 'end': 2429.24, 'text': ' part of training neural networks. That process of computing this gradient line is known as', 'tokens': [50848, 644, 295, 3097, 18161, 9590, 13, 663, 1399, 295, 15866, 341, 16235, 1622, 307, 2570, 382, 51192], 'temperature': 0.0, 'avg_logprob': -0.13399435679117838, 'compression_ratio': 1.7890625, 'no_speech_prob': 0.00021306317648850381}, {'id': 427, 'seek': 241268, 'start': 2429.24, 'end': 2434.9199999999996, 'text': \" back propagation. Let's do a very quick intro to back propagation and how it works.\", 'tokens': [51192, 646, 38377, 13, 961, 311, 360, 257, 588, 1702, 12897, 281, 646, 38377, 293, 577, 309, 1985, 13, 51476], 'temperature': 0.0, 'avg_logprob': -0.13399435679117838, 'compression_ratio': 1.7890625, 'no_speech_prob': 0.00021306317648850381}, {'id': 428, 'seek': 241268, 'start': 2436.3599999999997, 'end': 2441.56, 'text': \" Let's start with the simplest neural network in existence. This neural network has one input,\", 'tokens': [51548, 961, 311, 722, 365, 264, 22811, 18161, 3209, 294, 9123, 13, 639, 18161, 3209, 575, 472, 4846, 11, 51808], 'temperature': 0.0, 'avg_logprob': -0.13399435679117838, 'compression_ratio': 1.7890625, 'no_speech_prob': 0.00021306317648850381}, {'id': 429, 'seek': 244156, 'start': 2441.56, 'end': 2447.16, 'text': ' one output, and only one neuron. This is as simple as it gets. We want to compute the gradient', 'tokens': [50364, 472, 5598, 11, 293, 787, 472, 34090, 13, 639, 307, 382, 2199, 382, 309, 2170, 13, 492, 528, 281, 14722, 264, 16235, 50644], 'temperature': 0.0, 'avg_logprob': -0.06002593631586753, 'compression_ratio': 1.875, 'no_speech_prob': 0.0005701972986571491}, {'id': 430, 'seek': 244156, 'start': 2447.16, 'end': 2452.6, 'text': \" of our loss with respect to our weight. In this case, let's compute it with respect to W2, the\", 'tokens': [50644, 295, 527, 4470, 365, 3104, 281, 527, 3364, 13, 682, 341, 1389, 11, 718, 311, 14722, 309, 365, 3104, 281, 343, 17, 11, 264, 50916], 'temperature': 0.0, 'avg_logprob': -0.06002593631586753, 'compression_ratio': 1.875, 'no_speech_prob': 0.0005701972986571491}, {'id': 431, 'seek': 244156, 'start': 2452.6, 'end': 2460.12, 'text': ' second weight. So this derivative is going to tell us how much a small change in this weight will', 'tokens': [50916, 1150, 3364, 13, 407, 341, 13760, 307, 516, 281, 980, 505, 577, 709, 257, 1359, 1319, 294, 341, 3364, 486, 51292], 'temperature': 0.0, 'avg_logprob': -0.06002593631586753, 'compression_ratio': 1.875, 'no_speech_prob': 0.0005701972986571491}, {'id': 432, 'seek': 244156, 'start': 2460.12, 'end': 2464.44, 'text': ' affect our loss. If a small change, if we change our weight a little bit in one direction,', 'tokens': [51292, 3345, 527, 4470, 13, 759, 257, 1359, 1319, 11, 498, 321, 1319, 527, 3364, 257, 707, 857, 294, 472, 3513, 11, 51508], 'temperature': 0.0, 'avg_logprob': -0.06002593631586753, 'compression_ratio': 1.875, 'no_speech_prob': 0.0005701972986571491}, {'id': 433, 'seek': 244156, 'start': 2464.44, 'end': 2469.88, 'text': ' will it increase our loss or decrease our loss? So to compute that, we can write out this derivative.', 'tokens': [51508, 486, 309, 3488, 527, 4470, 420, 11514, 527, 4470, 30, 407, 281, 14722, 300, 11, 321, 393, 2464, 484, 341, 13760, 13, 51780], 'temperature': 0.0, 'avg_logprob': -0.06002593631586753, 'compression_ratio': 1.875, 'no_speech_prob': 0.0005701972986571491}, {'id': 434, 'seek': 246988, 'start': 2469.88, 'end': 2476.52, 'text': ' We can start with applying the chain rule backwards from the loss function through the output.', 'tokens': [50364, 492, 393, 722, 365, 9275, 264, 5021, 4978, 12204, 490, 264, 4470, 2445, 807, 264, 5598, 13, 50696], 'temperature': 0.0, 'avg_logprob': -0.07671236524394914, 'compression_ratio': 1.8228346456692914, 'no_speech_prob': 0.000687550229486078}, {'id': 435, 'seek': 246988, 'start': 2476.52, 'end': 2482.28, 'text': ' Specifically, what we can do is we can actually just decompose this derivative into two components.', 'tokens': [50696, 26058, 11, 437, 321, 393, 360, 307, 321, 393, 767, 445, 22867, 541, 341, 13760, 666, 732, 6677, 13, 50984], 'temperature': 0.0, 'avg_logprob': -0.07671236524394914, 'compression_ratio': 1.8228346456692914, 'no_speech_prob': 0.000687550229486078}, {'id': 436, 'seek': 246988, 'start': 2482.28, 'end': 2486.28, 'text': ' The first component is the derivative of our loss with respect to our output,', 'tokens': [50984, 440, 700, 6542, 307, 264, 13760, 295, 527, 4470, 365, 3104, 281, 527, 5598, 11, 51184], 'temperature': 0.0, 'avg_logprob': -0.07671236524394914, 'compression_ratio': 1.8228346456692914, 'no_speech_prob': 0.000687550229486078}, {'id': 437, 'seek': 246988, 'start': 2486.28, 'end': 2491.7200000000003, 'text': ' multiplied by the derivative of our output with respect to W2, right? This is just a standard', 'tokens': [51184, 17207, 538, 264, 13760, 295, 527, 5598, 365, 3104, 281, 343, 17, 11, 558, 30, 639, 307, 445, 257, 3832, 51456], 'temperature': 0.0, 'avg_logprob': -0.07671236524394914, 'compression_ratio': 1.8228346456692914, 'no_speech_prob': 0.000687550229486078}, {'id': 438, 'seek': 246988, 'start': 2493.6400000000003, 'end': 2498.52, 'text': ' instantiation of the chain rule with this original derivative that we had on the left-hand side.', 'tokens': [51552, 9836, 6642, 295, 264, 5021, 4978, 365, 341, 3380, 13760, 300, 321, 632, 322, 264, 1411, 12, 5543, 1252, 13, 51796], 'temperature': 0.0, 'avg_logprob': -0.07671236524394914, 'compression_ratio': 1.8228346456692914, 'no_speech_prob': 0.000687550229486078}, {'id': 439, 'seek': 249988, 'start': 2499.88, 'end': 2505.0, 'text': ' Suppose we want to compute the gradients of the weight before that, which in this case are not W1,', 'tokens': [50364, 21360, 321, 528, 281, 14722, 264, 2771, 2448, 295, 264, 3364, 949, 300, 11, 597, 294, 341, 1389, 366, 406, 343, 16, 11, 50620], 'temperature': 0.0, 'avg_logprob': -0.11783138662576675, 'compression_ratio': 1.7292418772563176, 'no_speech_prob': 0.0008966494933702052}, {'id': 440, 'seek': 249988, 'start': 2505.0, 'end': 2512.76, 'text': ' but W, excuse me, not W2, but W1. Well, all we do is replace W2 with W1, and that chain rule still', 'tokens': [50620, 457, 343, 11, 8960, 385, 11, 406, 343, 17, 11, 457, 343, 16, 13, 1042, 11, 439, 321, 360, 307, 7406, 343, 17, 365, 343, 16, 11, 293, 300, 5021, 4978, 920, 51008], 'temperature': 0.0, 'avg_logprob': -0.11783138662576675, 'compression_ratio': 1.7292418772563176, 'no_speech_prob': 0.0008966494933702052}, {'id': 441, 'seek': 249988, 'start': 2512.76, 'end': 2518.04, 'text': ' holds, right, that same equation holds. But now, you can see on the red component, that last', 'tokens': [51008, 9190, 11, 558, 11, 300, 912, 5367, 9190, 13, 583, 586, 11, 291, 393, 536, 322, 264, 2182, 6542, 11, 300, 1036, 51272], 'temperature': 0.0, 'avg_logprob': -0.11783138662576675, 'compression_ratio': 1.7292418772563176, 'no_speech_prob': 0.0008966494933702052}, {'id': 442, 'seek': 249988, 'start': 2518.04, 'end': 2522.6, 'text': ' component of the chain rule, we have to, once again, recursively apply one more chain rule,', 'tokens': [51272, 6542, 295, 264, 5021, 4978, 11, 321, 362, 281, 11, 1564, 797, 11, 20560, 3413, 3079, 472, 544, 5021, 4978, 11, 51500], 'temperature': 0.0, 'avg_logprob': -0.11783138662576675, 'compression_ratio': 1.7292418772563176, 'no_speech_prob': 0.0008966494933702052}, {'id': 443, 'seek': 249988, 'start': 2522.6, 'end': 2527.56, 'text': \" because that's again another derivative that we can't directly evaluate. We can expand that once\", 'tokens': [51500, 570, 300, 311, 797, 1071, 13760, 300, 321, 393, 380, 3838, 13059, 13, 492, 393, 5268, 300, 1564, 51748], 'temperature': 0.0, 'avg_logprob': -0.11783138662576675, 'compression_ratio': 1.7292418772563176, 'no_speech_prob': 0.0008966494933702052}, {'id': 444, 'seek': 252756, 'start': 2527.56, 'end': 2532.92, 'text': ' more with another instantiation of the chain rule. And now, all of these components, we can', 'tokens': [50364, 544, 365, 1071, 9836, 6642, 295, 264, 5021, 4978, 13, 400, 586, 11, 439, 295, 613, 6677, 11, 321, 393, 50632], 'temperature': 0.0, 'avg_logprob': -0.1027501531007911, 'compression_ratio': 1.702127659574468, 'no_speech_prob': 0.00045813884935341775}, {'id': 445, 'seek': 252756, 'start': 2532.92, 'end': 2538.12, 'text': ' directly propagate these gradients through the hidden units, right, in our neural network, all the way', 'tokens': [50632, 3838, 48256, 613, 2771, 2448, 807, 264, 7633, 6815, 11, 558, 11, 294, 527, 18161, 3209, 11, 439, 264, 636, 50892], 'temperature': 0.0, 'avg_logprob': -0.1027501531007911, 'compression_ratio': 1.702127659574468, 'no_speech_prob': 0.00045813884935341775}, {'id': 446, 'seek': 252756, 'start': 2538.12, 'end': 2542.52, 'text': \" back to the weight that we're interested in in this example, right? So we first computed the\", 'tokens': [50892, 646, 281, 264, 3364, 300, 321, 434, 3102, 294, 294, 341, 1365, 11, 558, 30, 407, 321, 700, 40610, 264, 51112], 'temperature': 0.0, 'avg_logprob': -0.1027501531007911, 'compression_ratio': 1.702127659574468, 'no_speech_prob': 0.00045813884935341775}, {'id': 447, 'seek': 252756, 'start': 2542.52, 'end': 2547.96, 'text': ' derivative with respect to W2, then we can back-propagate that and use that information also with W1.', 'tokens': [51112, 13760, 365, 3104, 281, 343, 17, 11, 550, 321, 393, 646, 12, 79, 1513, 559, 473, 300, 293, 764, 300, 1589, 611, 365, 343, 16, 13, 51384], 'temperature': 0.0, 'avg_logprob': -0.1027501531007911, 'compression_ratio': 1.702127659574468, 'no_speech_prob': 0.00045813884935341775}, {'id': 448, 'seek': 252756, 'start': 2547.96, 'end': 2551.88, 'text': \" That's why we really call it back-propagation, because this process occurs from the output\", 'tokens': [51384, 663, 311, 983, 321, 534, 818, 309, 646, 12, 79, 1513, 559, 399, 11, 570, 341, 1399, 11843, 490, 264, 5598, 51580], 'temperature': 0.0, 'avg_logprob': -0.1027501531007911, 'compression_ratio': 1.702127659574468, 'no_speech_prob': 0.00045813884935341775}, {'id': 449, 'seek': 255188, 'start': 2551.88, 'end': 2559.0, 'text': ' all the way back to the input. Now, we repeat this process essentially many, many times over the', 'tokens': [50364, 439, 264, 636, 646, 281, 264, 4846, 13, 823, 11, 321, 7149, 341, 1399, 4476, 867, 11, 867, 1413, 670, 264, 50720], 'temperature': 0.0, 'avg_logprob': -0.1330691831452506, 'compression_ratio': 1.7243816254416962, 'no_speech_prob': 0.0008690199465490878}, {'id': 450, 'seek': 255188, 'start': 2559.0, 'end': 2563.6400000000003, 'text': ' course of training by propagating these gradients over and over again through the network, all the', 'tokens': [50720, 1164, 295, 3097, 538, 12425, 990, 613, 2771, 2448, 670, 293, 670, 797, 807, 264, 3209, 11, 439, 264, 50952], 'temperature': 0.0, 'avg_logprob': -0.1330691831452506, 'compression_ratio': 1.7243816254416962, 'no_speech_prob': 0.0008690199465490878}, {'id': 451, 'seek': 255188, 'start': 2563.6400000000003, 'end': 2568.84, 'text': ' way from the output to the inputs to determine for every single weight, answering this question,', 'tokens': [50952, 636, 490, 264, 5598, 281, 264, 15743, 281, 6997, 337, 633, 2167, 3364, 11, 13430, 341, 1168, 11, 51212], 'temperature': 0.0, 'avg_logprob': -0.1330691831452506, 'compression_ratio': 1.7243816254416962, 'no_speech_prob': 0.0008690199465490878}, {'id': 452, 'seek': 255188, 'start': 2568.84, 'end': 2574.36, 'text': ' which is how much does a small change in these weights affect our loss function? If it increases,', 'tokens': [51212, 597, 307, 577, 709, 775, 257, 1359, 1319, 294, 613, 17443, 3345, 527, 4470, 2445, 30, 759, 309, 8637, 11, 51488], 'temperature': 0.0, 'avg_logprob': -0.1330691831452506, 'compression_ratio': 1.7243816254416962, 'no_speech_prob': 0.0008690199465490878}, {'id': 453, 'seek': 255188, 'start': 2574.36, 'end': 2578.6, 'text': \" it reduces, it decreases, and how we can use that to improve the loss, ultimately, because that's\", 'tokens': [51488, 309, 18081, 11, 309, 24108, 11, 293, 577, 321, 393, 764, 300, 281, 3470, 264, 4470, 11, 6284, 11, 570, 300, 311, 51700], 'temperature': 0.0, 'avg_logprob': -0.1330691831452506, 'compression_ratio': 1.7243816254416962, 'no_speech_prob': 0.0008690199465490878}, {'id': 454, 'seek': 257860, 'start': 2578.6, 'end': 2586.8399999999997, 'text': \" our final goal in this class. So that's the back-propagation algorithm. That's the core of training\", 'tokens': [50364, 527, 2572, 3387, 294, 341, 1508, 13, 407, 300, 311, 264, 646, 12, 79, 1513, 559, 399, 9284, 13, 663, 311, 264, 4965, 295, 3097, 50776], 'temperature': 0.0, 'avg_logprob': -0.09148577426342254, 'compression_ratio': 1.7092511013215859, 'no_speech_prob': 0.00018225506937596947}, {'id': 455, 'seek': 257860, 'start': 2586.8399999999997, 'end': 2593.4, 'text': \" neural networks. In theory, it's very simple. It's really just an instantiation of the chain rule.\", 'tokens': [50776, 18161, 9590, 13, 682, 5261, 11, 309, 311, 588, 2199, 13, 467, 311, 534, 445, 364, 9836, 6642, 295, 264, 5021, 4978, 13, 51104], 'temperature': 0.0, 'avg_logprob': -0.09148577426342254, 'compression_ratio': 1.7092511013215859, 'no_speech_prob': 0.00018225506937596947}, {'id': 456, 'seek': 257860, 'start': 2594.36, 'end': 2599.88, 'text': \" But let's touch on some insights that make training neural networks actually extremely complicated\", 'tokens': [51152, 583, 718, 311, 2557, 322, 512, 14310, 300, 652, 3097, 18161, 9590, 767, 4664, 6179, 51428], 'temperature': 0.0, 'avg_logprob': -0.09148577426342254, 'compression_ratio': 1.7092511013215859, 'no_speech_prob': 0.00018225506937596947}, {'id': 457, 'seek': 257860, 'start': 2599.88, 'end': 2606.2799999999997, 'text': ' in practice, even though the algorithm of back-propagation is simple and many decades old.', 'tokens': [51428, 294, 3124, 11, 754, 1673, 264, 9284, 295, 646, 12, 79, 1513, 559, 399, 307, 2199, 293, 867, 7878, 1331, 13, 51748], 'temperature': 0.0, 'avg_logprob': -0.09148577426342254, 'compression_ratio': 1.7092511013215859, 'no_speech_prob': 0.00018225506937596947}, {'id': 458, 'seek': 260628, 'start': 2607.0, 'end': 2611.7200000000003, 'text': ' In practice, though, optimization of neural networks looks something like this. It looks nothing', 'tokens': [50400, 682, 3124, 11, 1673, 11, 19618, 295, 18161, 9590, 1542, 746, 411, 341, 13, 467, 1542, 1825, 50636], 'temperature': 0.0, 'avg_logprob': -0.06530816853046417, 'compression_ratio': 1.9733333333333334, 'no_speech_prob': 0.016866883262991905}, {'id': 459, 'seek': 260628, 'start': 2611.7200000000003, 'end': 2616.28, 'text': ' like that picture that I showed you before. There are ways that we can visualize very large deep', 'tokens': [50636, 411, 300, 3036, 300, 286, 4712, 291, 949, 13, 821, 366, 2098, 300, 321, 393, 23273, 588, 2416, 2452, 50864], 'temperature': 0.0, 'avg_logprob': -0.06530816853046417, 'compression_ratio': 1.9733333333333334, 'no_speech_prob': 0.016866883262991905}, {'id': 460, 'seek': 260628, 'start': 2616.28, 'end': 2621.5600000000004, 'text': ' neural networks, and you can think of the landscape of these models looking like something like this.', 'tokens': [50864, 18161, 9590, 11, 293, 291, 393, 519, 295, 264, 9661, 295, 613, 5245, 1237, 411, 746, 411, 341, 13, 51128], 'temperature': 0.0, 'avg_logprob': -0.06530816853046417, 'compression_ratio': 1.9733333333333334, 'no_speech_prob': 0.016866883262991905}, {'id': 461, 'seek': 260628, 'start': 2621.5600000000004, 'end': 2625.7200000000003, 'text': ' This is an illustration from a paper that came out several years ago where they tried to actually', 'tokens': [51128, 639, 307, 364, 22645, 490, 257, 3035, 300, 1361, 484, 2940, 924, 2057, 689, 436, 3031, 281, 767, 51336], 'temperature': 0.0, 'avg_logprob': -0.06530816853046417, 'compression_ratio': 1.9733333333333334, 'no_speech_prob': 0.016866883262991905}, {'id': 462, 'seek': 260628, 'start': 2625.7200000000003, 'end': 2630.52, 'text': \" visualize the landscape of very, very deep neural networks. And that's what this landscape actually\", 'tokens': [51336, 23273, 264, 9661, 295, 588, 11, 588, 2452, 18161, 9590, 13, 400, 300, 311, 437, 341, 9661, 767, 51576], 'temperature': 0.0, 'avg_logprob': -0.06530816853046417, 'compression_ratio': 1.9733333333333334, 'no_speech_prob': 0.016866883262991905}, {'id': 463, 'seek': 260628, 'start': 2630.52, 'end': 2633.8, 'text': \" looks like. That's what you're trying to deal with and find the minimum in this space. And you can\", 'tokens': [51576, 1542, 411, 13, 663, 311, 437, 291, 434, 1382, 281, 2028, 365, 293, 915, 264, 7285, 294, 341, 1901, 13, 400, 291, 393, 51740], 'temperature': 0.0, 'avg_logprob': -0.06530816853046417, 'compression_ratio': 1.9733333333333334, 'no_speech_prob': 0.016866883262991905}, {'id': 464, 'seek': 263380, 'start': 2633.8, 'end': 2640.28, 'text': \" imagine the challenges that come with that. So to cover the challenges, let's first think of and\", 'tokens': [50364, 3811, 264, 4759, 300, 808, 365, 300, 13, 407, 281, 2060, 264, 4759, 11, 718, 311, 700, 519, 295, 293, 50688], 'temperature': 0.0, 'avg_logprob': -0.09854584355508128, 'compression_ratio': 1.6307053941908713, 'no_speech_prob': 0.00022689432080369443}, {'id': 465, 'seek': 263380, 'start': 2640.28, 'end': 2647.4, 'text': \" recall that update equation defined in gradient descent. So I didn't talk too much about this parameter,\", 'tokens': [50688, 9901, 300, 5623, 5367, 7642, 294, 16235, 23475, 13, 407, 286, 994, 380, 751, 886, 709, 466, 341, 13075, 11, 51044], 'temperature': 0.0, 'avg_logprob': -0.09854584355508128, 'compression_ratio': 1.6307053941908713, 'no_speech_prob': 0.00022689432080369443}, {'id': 466, 'seek': 263380, 'start': 2647.4, 'end': 2651.8, 'text': \" eta, but now let's spend a bit of time thinking about this. This is called the learning rate,\", 'tokens': [51044, 32415, 11, 457, 586, 718, 311, 3496, 257, 857, 295, 565, 1953, 466, 341, 13, 639, 307, 1219, 264, 2539, 3314, 11, 51264], 'temperature': 0.0, 'avg_logprob': -0.09854584355508128, 'compression_ratio': 1.6307053941908713, 'no_speech_prob': 0.00022689432080369443}, {'id': 467, 'seek': 263380, 'start': 2651.8, 'end': 2657.7200000000003, 'text': ' like we saw before. It determines basically how big of a step we need to take in the direction of', 'tokens': [51264, 411, 321, 1866, 949, 13, 467, 24799, 1936, 577, 955, 295, 257, 1823, 321, 643, 281, 747, 294, 264, 3513, 295, 51560], 'temperature': 0.0, 'avg_logprob': -0.09854584355508128, 'compression_ratio': 1.6307053941908713, 'no_speech_prob': 0.00022689432080369443}, {'id': 468, 'seek': 265772, 'start': 2657.8799999999997, 'end': 2663.7999999999997, 'text': ' our gradient than every single iteration of back propagation. In practice, even setting the learning', 'tokens': [50372, 527, 16235, 813, 633, 2167, 24784, 295, 646, 38377, 13, 682, 3124, 11, 754, 3287, 264, 2539, 50668], 'temperature': 0.0, 'avg_logprob': -0.12926954083738074, 'compression_ratio': 1.7202797202797202, 'no_speech_prob': 0.000535575149115175}, {'id': 469, 'seek': 265772, 'start': 2663.7999999999997, 'end': 2668.68, 'text': ' rate can be very challenging. You as the designer of the neural network have to set this value,', 'tokens': [50668, 3314, 393, 312, 588, 7595, 13, 509, 382, 264, 11795, 295, 264, 18161, 3209, 362, 281, 992, 341, 2158, 11, 50912], 'temperature': 0.0, 'avg_logprob': -0.12926954083738074, 'compression_ratio': 1.7202797202797202, 'no_speech_prob': 0.000535575149115175}, {'id': 470, 'seek': 265772, 'start': 2668.68, 'end': 2673.3999999999996, 'text': ' this learning rate. And how do you pick this value? That can actually be quite difficult. It has really', 'tokens': [50912, 341, 2539, 3314, 13, 400, 577, 360, 291, 1888, 341, 2158, 30, 663, 393, 767, 312, 1596, 2252, 13, 467, 575, 534, 51148], 'temperature': 0.0, 'avg_logprob': -0.12926954083738074, 'compression_ratio': 1.7202797202797202, 'no_speech_prob': 0.000535575149115175}, {'id': 471, 'seek': 265772, 'start': 2674.2799999999997, 'end': 2680.68, 'text': ' large consequences when building a neural network. So for example, if we set the learning rate', 'tokens': [51192, 2416, 10098, 562, 2390, 257, 18161, 3209, 13, 407, 337, 1365, 11, 498, 321, 992, 264, 2539, 3314, 51512], 'temperature': 0.0, 'avg_logprob': -0.12926954083738074, 'compression_ratio': 1.7202797202797202, 'no_speech_prob': 0.000535575149115175}, {'id': 472, 'seek': 265772, 'start': 2680.68, 'end': 2686.68, 'text': \" too low, then we learn very slowly. So let's assume we start on the right-hand side here at that\", 'tokens': [51512, 886, 2295, 11, 550, 321, 1466, 588, 5692, 13, 407, 718, 311, 6552, 321, 722, 322, 264, 558, 12, 5543, 1252, 510, 412, 300, 51812], 'temperature': 0.0, 'avg_logprob': -0.12926954083738074, 'compression_ratio': 1.7202797202797202, 'no_speech_prob': 0.000535575149115175}, {'id': 473, 'seek': 268668, 'start': 2686.68, 'end': 2692.12, 'text': ' initial guess. If our learning rate is not large enough, not only do we converge slowly, we actually', 'tokens': [50364, 5883, 2041, 13, 759, 527, 2539, 3314, 307, 406, 2416, 1547, 11, 406, 787, 360, 321, 41881, 5692, 11, 321, 767, 50636], 'temperature': 0.0, 'avg_logprob': -0.11219930241250584, 'compression_ratio': 1.7381818181818183, 'no_speech_prob': 0.00040437575080432}, {'id': 474, 'seek': 268668, 'start': 2692.12, 'end': 2696.3599999999997, 'text': \" don't even converge to the global minimum, because we kind of get stuck in a local minimum.\", 'tokens': [50636, 500, 380, 754, 41881, 281, 264, 4338, 7285, 11, 570, 321, 733, 295, 483, 5541, 294, 257, 2654, 7285, 13, 50848], 'temperature': 0.0, 'avg_logprob': -0.11219930241250584, 'compression_ratio': 1.7381818181818183, 'no_speech_prob': 0.00040437575080432}, {'id': 475, 'seek': 268668, 'start': 2697.72, 'end': 2702.2, 'text': ' Now, what if we set our learning rate too high? What can actually happen is we overshoot,', 'tokens': [50916, 823, 11, 437, 498, 321, 992, 527, 2539, 3314, 886, 1090, 30, 708, 393, 767, 1051, 307, 321, 15488, 24467, 11, 51140], 'temperature': 0.0, 'avg_logprob': -0.11219930241250584, 'compression_ratio': 1.7381818181818183, 'no_speech_prob': 0.00040437575080432}, {'id': 476, 'seek': 268668, 'start': 2702.2, 'end': 2707.3999999999996, 'text': ' and we can actually start to diverge from the solution. The gradients can actually explode.', 'tokens': [51140, 293, 321, 393, 767, 722, 281, 18558, 432, 490, 264, 3827, 13, 440, 2771, 2448, 393, 767, 21411, 13, 51400], 'temperature': 0.0, 'avg_logprob': -0.11219930241250584, 'compression_ratio': 1.7381818181818183, 'no_speech_prob': 0.00040437575080432}, {'id': 477, 'seek': 268668, 'start': 2707.3999999999996, 'end': 2712.44, 'text': \" Very bad things happen, and then the neural network doesn't trade. So that's also not good. In reality,\", 'tokens': [51400, 4372, 1578, 721, 1051, 11, 293, 550, 264, 18161, 3209, 1177, 380, 4923, 13, 407, 300, 311, 611, 406, 665, 13, 682, 4103, 11, 51652], 'temperature': 0.0, 'avg_logprob': -0.11219930241250584, 'compression_ratio': 1.7381818181818183, 'no_speech_prob': 0.00040437575080432}, {'id': 478, 'seek': 271244, 'start': 2712.44, 'end': 2718.36, 'text': \" there's a very happy medium between setting it too small, setting it too large, where you set it\", 'tokens': [50364, 456, 311, 257, 588, 2055, 6399, 1296, 3287, 309, 886, 1359, 11, 3287, 309, 886, 2416, 11, 689, 291, 992, 309, 50660], 'temperature': 0.0, 'avg_logprob': -0.0912496961396316, 'compression_ratio': 1.8074074074074074, 'no_speech_prob': 0.0009104922646656632}, {'id': 479, 'seek': 271244, 'start': 2718.36, 'end': 2723.56, 'text': ' just large enough to kind of overshoot some of these local minima, put you into a reasonable', 'tokens': [50660, 445, 2416, 1547, 281, 733, 295, 15488, 24467, 512, 295, 613, 2654, 4464, 64, 11, 829, 291, 666, 257, 10585, 50920], 'temperature': 0.0, 'avg_logprob': -0.0912496961396316, 'compression_ratio': 1.8074074074074074, 'no_speech_prob': 0.0009104922646656632}, {'id': 480, 'seek': 271244, 'start': 2723.56, 'end': 2727.96, 'text': ' part of the search space, where then you can actually converge on the solutions that you care most', 'tokens': [50920, 644, 295, 264, 3164, 1901, 11, 689, 550, 291, 393, 767, 41881, 322, 264, 6547, 300, 291, 1127, 881, 51140], 'temperature': 0.0, 'avg_logprob': -0.0912496961396316, 'compression_ratio': 1.8074074074074074, 'no_speech_prob': 0.0009104922646656632}, {'id': 481, 'seek': 271244, 'start': 2727.96, 'end': 2733.7200000000003, 'text': ' about. But actually, how do you set these learning rates in practice? How do you pick what is the', 'tokens': [51140, 466, 13, 583, 767, 11, 577, 360, 291, 992, 613, 2539, 6846, 294, 3124, 30, 1012, 360, 291, 1888, 437, 307, 264, 51428], 'temperature': 0.0, 'avg_logprob': -0.0912496961396316, 'compression_ratio': 1.8074074074074074, 'no_speech_prob': 0.0009104922646656632}, {'id': 482, 'seek': 271244, 'start': 2733.7200000000003, 'end': 2739.48, 'text': ' ideal learning rate? One option, and this is actually a very common option, in practice, is to simply', 'tokens': [51428, 7157, 2539, 3314, 30, 1485, 3614, 11, 293, 341, 307, 767, 257, 588, 2689, 3614, 11, 294, 3124, 11, 307, 281, 2935, 51716], 'temperature': 0.0, 'avg_logprob': -0.0912496961396316, 'compression_ratio': 1.8074074074074074, 'no_speech_prob': 0.0009104922646656632}, {'id': 483, 'seek': 273948, 'start': 2739.48, 'end': 2744.2, 'text': \" try out a bunch of learning rates and see what works the best. So try out, let's say, a whole grade\", 'tokens': [50364, 853, 484, 257, 3840, 295, 2539, 6846, 293, 536, 437, 1985, 264, 1151, 13, 407, 853, 484, 11, 718, 311, 584, 11, 257, 1379, 7204, 50600], 'temperature': 0.0, 'avg_logprob': -0.1193264599504142, 'compression_ratio': 1.8721804511278195, 'no_speech_prob': 0.004901172127574682}, {'id': 484, 'seek': 273948, 'start': 2744.2, 'end': 2748.92, 'text': ' of different learning rates, and train all of these neural networks, see which one works the best.', 'tokens': [50600, 295, 819, 2539, 6846, 11, 293, 3847, 439, 295, 613, 18161, 9590, 11, 536, 597, 472, 1985, 264, 1151, 13, 50836], 'temperature': 0.0, 'avg_logprob': -0.1193264599504142, 'compression_ratio': 1.8721804511278195, 'no_speech_prob': 0.004901172127574682}, {'id': 485, 'seek': 273948, 'start': 2749.8, 'end': 2754.76, 'text': ' But I think we can do something a lot smarter. So what are some more intelligent ways that we could do', 'tokens': [50880, 583, 286, 519, 321, 393, 360, 746, 257, 688, 20294, 13, 407, 437, 366, 512, 544, 13232, 2098, 300, 321, 727, 360, 51128], 'temperature': 0.0, 'avg_logprob': -0.1193264599504142, 'compression_ratio': 1.8721804511278195, 'no_speech_prob': 0.004901172127574682}, {'id': 486, 'seek': 273948, 'start': 2754.76, 'end': 2759.56, 'text': ' this instead of exhaustively trying out a whole bunch of different learning rates? Can we design a', 'tokens': [51128, 341, 2602, 295, 14687, 3413, 1382, 484, 257, 1379, 3840, 295, 819, 2539, 6846, 30, 1664, 321, 1715, 257, 51368], 'temperature': 0.0, 'avg_logprob': -0.1193264599504142, 'compression_ratio': 1.8721804511278195, 'no_speech_prob': 0.004901172127574682}, {'id': 487, 'seek': 273948, 'start': 2759.56, 'end': 2764.92, 'text': ' learning rate algorithm that actually adapts to our neural network and adapts to its landscape so', 'tokens': [51368, 2539, 3314, 9284, 300, 767, 23169, 1373, 281, 527, 18161, 3209, 293, 23169, 1373, 281, 1080, 9661, 370, 51636], 'temperature': 0.0, 'avg_logprob': -0.1193264599504142, 'compression_ratio': 1.8721804511278195, 'no_speech_prob': 0.004901172127574682}, {'id': 488, 'seek': 276492, 'start': 2764.92, 'end': 2771.32, 'text': \" that it's a bit more intelligent than that previous idea? So this really ultimately means that\", 'tokens': [50364, 300, 309, 311, 257, 857, 544, 13232, 813, 300, 3894, 1558, 30, 407, 341, 534, 6284, 1355, 300, 50684], 'temperature': 0.0, 'avg_logprob': -0.07338845452596975, 'compression_ratio': 1.7348837209302326, 'no_speech_prob': 3.4261105611221865e-05}, {'id': 489, 'seek': 276492, 'start': 2772.44, 'end': 2778.44, 'text': ' the learning rate, the speed at which the algorithm is trusting the gradients that it sees,', 'tokens': [50740, 264, 2539, 3314, 11, 264, 3073, 412, 597, 264, 9284, 307, 28235, 264, 2771, 2448, 300, 309, 8194, 11, 51040], 'temperature': 0.0, 'avg_logprob': -0.07338845452596975, 'compression_ratio': 1.7348837209302326, 'no_speech_prob': 3.4261105611221865e-05}, {'id': 490, 'seek': 276492, 'start': 2778.44, 'end': 2784.36, 'text': \" is going to depend on how large the gradient is in that location and how fast we're learning,\", 'tokens': [51040, 307, 516, 281, 5672, 322, 577, 2416, 264, 16235, 307, 294, 300, 4914, 293, 577, 2370, 321, 434, 2539, 11, 51336], 'temperature': 0.0, 'avg_logprob': -0.07338845452596975, 'compression_ratio': 1.7348837209302326, 'no_speech_prob': 3.4261105611221865e-05}, {'id': 491, 'seek': 276492, 'start': 2784.36, 'end': 2791.2400000000002, 'text': ' how many other options, sorry, and many other options that we might have as part of training', 'tokens': [51336, 577, 867, 661, 3956, 11, 2597, 11, 293, 867, 661, 3956, 300, 321, 1062, 362, 382, 644, 295, 3097, 51680], 'temperature': 0.0, 'avg_logprob': -0.07338845452596975, 'compression_ratio': 1.7348837209302326, 'no_speech_prob': 3.4261105611221865e-05}, {'id': 492, 'seek': 279124, 'start': 2791.24, 'end': 2795.4799999999996, 'text': \" in neural networks, right? So it's not only how quickly we're learning, you may judge it on many\", 'tokens': [50364, 294, 18161, 9590, 11, 558, 30, 407, 309, 311, 406, 787, 577, 2661, 321, 434, 2539, 11, 291, 815, 6995, 309, 322, 867, 50576], 'temperature': 0.0, 'avg_logprob': -0.1326897004071404, 'compression_ratio': 1.6652719665271967, 'no_speech_prob': 0.00044401525519788265}, {'id': 493, 'seek': 279124, 'start': 2795.4799999999996, 'end': 2803.64, 'text': \" different factors in learning landscape. In fact, we've all been, these different algorithms that I'm\", 'tokens': [50576, 819, 6771, 294, 2539, 9661, 13, 682, 1186, 11, 321, 600, 439, 668, 11, 613, 819, 14642, 300, 286, 478, 50984], 'temperature': 0.0, 'avg_logprob': -0.1326897004071404, 'compression_ratio': 1.6652719665271967, 'no_speech_prob': 0.00044401525519788265}, {'id': 494, 'seek': 279124, 'start': 2803.64, 'end': 2808.68, 'text': ' talking about, these adaptive learning rate algorithms have been very widely studied in practice.', 'tokens': [50984, 1417, 466, 11, 613, 27912, 2539, 3314, 14642, 362, 668, 588, 13371, 9454, 294, 3124, 13, 51236], 'temperature': 0.0, 'avg_logprob': -0.1326897004071404, 'compression_ratio': 1.6652719665271967, 'no_speech_prob': 0.00044401525519788265}, {'id': 495, 'seek': 279124, 'start': 2808.68, 'end': 2814.8399999999997, 'text': ' There is a very thriving community in the deep learning research community that focuses on developing', 'tokens': [51236, 821, 307, 257, 588, 30643, 1768, 294, 264, 2452, 2539, 2132, 1768, 300, 16109, 322, 6416, 51544], 'temperature': 0.0, 'avg_logprob': -0.1326897004071404, 'compression_ratio': 1.6652719665271967, 'no_speech_prob': 0.00044401525519788265}, {'id': 496, 'seek': 281484, 'start': 2814.84, 'end': 2821.1600000000003, 'text': ' and designing new algorithms for learning rate adaptation and faster optimization of large neural', 'tokens': [50364, 293, 14685, 777, 14642, 337, 2539, 3314, 21549, 293, 4663, 19618, 295, 2416, 18161, 50680], 'temperature': 0.0, 'avg_logprob': -0.07027302055715401, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 0.0015479098074138165}, {'id': 497, 'seek': 281484, 'start': 2821.1600000000003, 'end': 2827.4, 'text': \" networks like these. And during your labs, you'll actually get the opportunity to not only try out\", 'tokens': [50680, 9590, 411, 613, 13, 400, 1830, 428, 20339, 11, 291, 603, 767, 483, 264, 2650, 281, 406, 787, 853, 484, 50992], 'temperature': 0.0, 'avg_logprob': -0.07027302055715401, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 0.0015479098074138165}, {'id': 498, 'seek': 281484, 'start': 2827.4, 'end': 2832.36, 'text': ' a lot of these different adaptive algorithms, which you can see here, but also try to uncover what are', 'tokens': [50992, 257, 688, 295, 613, 819, 27912, 14642, 11, 597, 291, 393, 536, 510, 11, 457, 611, 853, 281, 21694, 437, 366, 51240], 'temperature': 0.0, 'avg_logprob': -0.07027302055715401, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 0.0015479098074138165}, {'id': 499, 'seek': 281484, 'start': 2832.36, 'end': 2836.36, 'text': \" kind of the patterns and benefits of one versus the other. And that's going to be something that\", 'tokens': [51240, 733, 295, 264, 8294, 293, 5311, 295, 472, 5717, 264, 661, 13, 400, 300, 311, 516, 281, 312, 746, 300, 51440], 'temperature': 0.0, 'avg_logprob': -0.07027302055715401, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 0.0015479098074138165}, {'id': 500, 'seek': 281484, 'start': 2837.32, 'end': 2843.4, 'text': \" I think you'll find very insightful as part of your labs. So another key component of your labs\", 'tokens': [51488, 286, 519, 291, 603, 915, 588, 46401, 382, 644, 295, 428, 20339, 13, 407, 1071, 2141, 6542, 295, 428, 20339, 51792], 'temperature': 0.0, 'avg_logprob': -0.07027302055715401, 'compression_ratio': 1.7142857142857142, 'no_speech_prob': 0.0015479098074138165}, {'id': 501, 'seek': 284340, 'start': 2843.4, 'end': 2847.96, 'text': \" that you'll see is how you can actually put all of this information that we've covered today into\", 'tokens': [50364, 300, 291, 603, 536, 307, 577, 291, 393, 767, 829, 439, 295, 341, 1589, 300, 321, 600, 5343, 965, 666, 50592], 'temperature': 0.0, 'avg_logprob': -0.09279537902158849, 'compression_ratio': 1.8354430379746836, 'no_speech_prob': 0.0013450944097712636}, {'id': 502, 'seek': 284340, 'start': 2847.96, 'end': 2853.0, 'text': ' a single picture that looks roughly something like this, which defines your model at the first,', 'tokens': [50592, 257, 2167, 3036, 300, 1542, 9810, 746, 411, 341, 11, 597, 23122, 428, 2316, 412, 264, 700, 11, 50844], 'temperature': 0.0, 'avg_logprob': -0.09279537902158849, 'compression_ratio': 1.8354430379746836, 'no_speech_prob': 0.0013450944097712636}, {'id': 503, 'seek': 284340, 'start': 2853.0, 'end': 2856.2000000000003, 'text': \" at the top here. That's where you define your model, where you talked about this in the beginning\", 'tokens': [50844, 412, 264, 1192, 510, 13, 663, 311, 689, 291, 6964, 428, 2316, 11, 689, 291, 2825, 466, 341, 294, 264, 2863, 51004], 'temperature': 0.0, 'avg_logprob': -0.09279537902158849, 'compression_ratio': 1.8354430379746836, 'no_speech_prob': 0.0013450944097712636}, {'id': 504, 'seek': 284340, 'start': 2856.2000000000003, 'end': 2861.8, 'text': \" part of the lecture. For every piece in your model, you're now going to need to define this\", 'tokens': [51004, 644, 295, 264, 7991, 13, 1171, 633, 2522, 294, 428, 2316, 11, 291, 434, 586, 516, 281, 643, 281, 6964, 341, 51284], 'temperature': 0.0, 'avg_logprob': -0.09279537902158849, 'compression_ratio': 1.8354430379746836, 'no_speech_prob': 0.0013450944097712636}, {'id': 505, 'seek': 284340, 'start': 2861.8, 'end': 2866.6800000000003, 'text': \" optimizer, which we've just talked about. This optimizer is defined together with a learning rate,\", 'tokens': [51284, 5028, 6545, 11, 597, 321, 600, 445, 2825, 466, 13, 639, 5028, 6545, 307, 7642, 1214, 365, 257, 2539, 3314, 11, 51528], 'temperature': 0.0, 'avg_logprob': -0.09279537902158849, 'compression_ratio': 1.8354430379746836, 'no_speech_prob': 0.0013450944097712636}, {'id': 506, 'seek': 284340, 'start': 2866.6800000000003, 'end': 2871.8, 'text': \" right? How quickly you want to optimize your lost landscape. And over many loops, you're going to\", 'tokens': [51528, 558, 30, 1012, 2661, 291, 528, 281, 19719, 428, 2731, 9661, 13, 400, 670, 867, 16121, 11, 291, 434, 516, 281, 51784], 'temperature': 0.0, 'avg_logprob': -0.09279537902158849, 'compression_ratio': 1.8354430379746836, 'no_speech_prob': 0.0013450944097712636}, {'id': 507, 'seek': 287180, 'start': 2871.8, 'end': 2877.5600000000004, 'text': ' pass over all of the examples in your data set and observe essentially how to improve your network.', 'tokens': [50364, 1320, 670, 439, 295, 264, 5110, 294, 428, 1412, 992, 293, 11441, 4476, 577, 281, 3470, 428, 3209, 13, 50652], 'temperature': 0.0, 'avg_logprob': -0.11494673774355933, 'compression_ratio': 1.7509025270758123, 'no_speech_prob': 0.00017393814050592482}, {'id': 508, 'seek': 287180, 'start': 2877.5600000000004, 'end': 2881.6400000000003, 'text': \" That's the gradient. And then actually improve the network in those directions. And keep doing that\", 'tokens': [50652, 663, 311, 264, 16235, 13, 400, 550, 767, 3470, 264, 3209, 294, 729, 11095, 13, 400, 1066, 884, 300, 50856], 'temperature': 0.0, 'avg_logprob': -0.11494673774355933, 'compression_ratio': 1.7509025270758123, 'no_speech_prob': 0.00017393814050592482}, {'id': 509, 'seek': 287180, 'start': 2881.6400000000003, 'end': 2887.0800000000004, 'text': ' over and over and over again until eventually your neural network converges to some sort of solution.', 'tokens': [50856, 670, 293, 670, 293, 670, 797, 1826, 4728, 428, 18161, 3209, 9652, 2880, 281, 512, 1333, 295, 3827, 13, 51128], 'temperature': 0.0, 'avg_logprob': -0.11494673774355933, 'compression_ratio': 1.7509025270758123, 'no_speech_prob': 0.00017393814050592482}, {'id': 510, 'seek': 287180, 'start': 2889.7200000000003, 'end': 2894.52, 'text': ' So I want to very quickly, briefly, in the remaining time that we have, continue to talk about', 'tokens': [51260, 407, 286, 528, 281, 588, 2661, 11, 10515, 11, 294, 264, 8877, 565, 300, 321, 362, 11, 2354, 281, 751, 466, 51500], 'temperature': 0.0, 'avg_logprob': -0.11494673774355933, 'compression_ratio': 1.7509025270758123, 'no_speech_prob': 0.00017393814050592482}, {'id': 511, 'seek': 287180, 'start': 2894.52, 'end': 2900.28, 'text': ' tips for training these neural networks in practice and focus on this very powerful idea', 'tokens': [51500, 6082, 337, 3097, 613, 18161, 9590, 294, 3124, 293, 1879, 322, 341, 588, 4005, 1558, 51788], 'temperature': 0.0, 'avg_logprob': -0.11494673774355933, 'compression_ratio': 1.7509025270758123, 'no_speech_prob': 0.00017393814050592482}, {'id': 512, 'seek': 290028, 'start': 2900.84, 'end': 2907.5600000000004, 'text': ' of batching your data into what are called mini batches of smaller pieces of data.', 'tokens': [50392, 295, 15245, 278, 428, 1412, 666, 437, 366, 1219, 8382, 15245, 279, 295, 4356, 3755, 295, 1412, 13, 50728], 'temperature': 0.0, 'avg_logprob': -0.10528354417710077, 'compression_ratio': 1.6577777777777778, 'no_speech_prob': 0.0007791827665641904}, {'id': 513, 'seek': 290028, 'start': 2908.36, 'end': 2913.48, 'text': \" To do this, let's revisit that gradient descent algorithm, right? So here, this gradient that we\", 'tokens': [50768, 1407, 360, 341, 11, 718, 311, 32676, 300, 16235, 23475, 9284, 11, 558, 30, 407, 510, 11, 341, 16235, 300, 321, 51024], 'temperature': 0.0, 'avg_logprob': -0.10528354417710077, 'compression_ratio': 1.6577777777777778, 'no_speech_prob': 0.0007791827665641904}, {'id': 514, 'seek': 290028, 'start': 2913.48, 'end': 2919.7200000000003, 'text': \" talked about before is actually extraordinarily computationally expensive to compute because it's\", 'tokens': [51024, 2825, 466, 949, 307, 767, 34557, 24903, 379, 5124, 281, 14722, 570, 309, 311, 51336], 'temperature': 0.0, 'avg_logprob': -0.10528354417710077, 'compression_ratio': 1.6577777777777778, 'no_speech_prob': 0.0007791827665641904}, {'id': 515, 'seek': 290028, 'start': 2919.7200000000003, 'end': 2926.6000000000004, 'text': ' computed as a summation across all of the pieces in your data set, right? And in most real life', 'tokens': [51336, 40610, 382, 257, 28811, 2108, 439, 295, 264, 3755, 294, 428, 1412, 992, 11, 558, 30, 400, 294, 881, 957, 993, 51680], 'temperature': 0.0, 'avg_logprob': -0.10528354417710077, 'compression_ratio': 1.6577777777777778, 'no_speech_prob': 0.0007791827665641904}, {'id': 516, 'seek': 292660, 'start': 2926.6, 'end': 2932.6, 'text': \" for real world problems, it's simply not feasible to compute a gradient over your entire data set.\", 'tokens': [50364, 337, 957, 1002, 2740, 11, 309, 311, 2935, 406, 26648, 281, 14722, 257, 16235, 670, 428, 2302, 1412, 992, 13, 50664], 'temperature': 0.0, 'avg_logprob': -0.10283271656479946, 'compression_ratio': 1.7534246575342465, 'no_speech_prob': 0.0008557796245440841}, {'id': 517, 'seek': 292660, 'start': 2932.6, 'end': 2938.44, 'text': ' Data sets are just too large these days. So there are some alternatives, right? What are the', 'tokens': [50664, 11888, 6352, 366, 445, 886, 2416, 613, 1708, 13, 407, 456, 366, 512, 20478, 11, 558, 30, 708, 366, 264, 50956], 'temperature': 0.0, 'avg_logprob': -0.10283271656479946, 'compression_ratio': 1.7534246575342465, 'no_speech_prob': 0.0008557796245440841}, {'id': 518, 'seek': 292660, 'start': 2938.44, 'end': 2944.2, 'text': ' alternatives? Instead of computing the derivative for the gradients across your entire data set,', 'tokens': [50956, 20478, 30, 7156, 295, 15866, 264, 13760, 337, 264, 2771, 2448, 2108, 428, 2302, 1412, 992, 11, 51244], 'temperature': 0.0, 'avg_logprob': -0.10283271656479946, 'compression_ratio': 1.7534246575342465, 'no_speech_prob': 0.0008557796245440841}, {'id': 519, 'seek': 292660, 'start': 2944.8399999999997, 'end': 2950.8399999999997, 'text': ' what if you instead computed the gradient over just a single example in your data set? Just one', 'tokens': [51276, 437, 498, 291, 2602, 40610, 264, 16235, 670, 445, 257, 2167, 1365, 294, 428, 1412, 992, 30, 1449, 472, 51576], 'temperature': 0.0, 'avg_logprob': -0.10283271656479946, 'compression_ratio': 1.7534246575342465, 'no_speech_prob': 0.0008557796245440841}, {'id': 520, 'seek': 295084, 'start': 2950.84, 'end': 2956.36, 'text': \" example. Well, of course, this estimate of your gradient is going to be exactly that. It's an\", 'tokens': [50364, 1365, 13, 1042, 11, 295, 1164, 11, 341, 12539, 295, 428, 16235, 307, 516, 281, 312, 2293, 300, 13, 467, 311, 364, 50640], 'temperature': 0.0, 'avg_logprob': -0.12893373107910155, 'compression_ratio': 1.9203187250996017, 'no_speech_prob': 0.0013665362494066358}, {'id': 521, 'seek': 295084, 'start': 2956.36, 'end': 2961.56, 'text': \" estimate. It's going to be very noisy. It may roughly reflect the trends of your entire data set,\", 'tokens': [50640, 12539, 13, 467, 311, 516, 281, 312, 588, 24518, 13, 467, 815, 9810, 5031, 264, 13892, 295, 428, 2302, 1412, 992, 11, 50900], 'temperature': 0.0, 'avg_logprob': -0.12893373107910155, 'compression_ratio': 1.9203187250996017, 'no_speech_prob': 0.0013665362494066358}, {'id': 522, 'seek': 295084, 'start': 2961.56, 'end': 2966.52, 'text': \" but because it's a very, it's only one example. In fact, of your entire data set, it may be very noisy.\", 'tokens': [50900, 457, 570, 309, 311, 257, 588, 11, 309, 311, 787, 472, 1365, 13, 682, 1186, 11, 295, 428, 2302, 1412, 992, 11, 309, 815, 312, 588, 24518, 13, 51148], 'temperature': 0.0, 'avg_logprob': -0.12893373107910155, 'compression_ratio': 1.9203187250996017, 'no_speech_prob': 0.0013665362494066358}, {'id': 523, 'seek': 295084, 'start': 2967.2400000000002, 'end': 2974.52, 'text': \" Right? Well, the advantage of this, though, is that it's much faster to compute, obviously,\", 'tokens': [51184, 1779, 30, 1042, 11, 264, 5002, 295, 341, 11, 1673, 11, 307, 300, 309, 311, 709, 4663, 281, 14722, 11, 2745, 11, 51548], 'temperature': 0.0, 'avg_logprob': -0.12893373107910155, 'compression_ratio': 1.9203187250996017, 'no_speech_prob': 0.0013665362494066358}, {'id': 524, 'seek': 295084, 'start': 2974.52, 'end': 2979.7200000000003, 'text': \" the gradient over a single example because it's one example. So computationally, this has huge\", 'tokens': [51548, 264, 16235, 670, 257, 2167, 1365, 570, 309, 311, 472, 1365, 13, 407, 24903, 379, 11, 341, 575, 2603, 51808], 'temperature': 0.0, 'avg_logprob': -0.12893373107910155, 'compression_ratio': 1.9203187250996017, 'no_speech_prob': 0.0013665362494066358}, {'id': 525, 'seek': 297972, 'start': 2979.72, 'end': 2984.4399999999996, 'text': \" advantages, but the downside is that it's extremely stochastic, right? That's the reason why this\", 'tokens': [50364, 14906, 11, 457, 264, 25060, 307, 300, 309, 311, 4664, 342, 8997, 2750, 11, 558, 30, 663, 311, 264, 1778, 983, 341, 50600], 'temperature': 0.0, 'avg_logprob': -0.08230556290725181, 'compression_ratio': 1.8022388059701493, 'no_speech_prob': 0.000282350811176002}, {'id': 526, 'seek': 297972, 'start': 2984.4399999999996, 'end': 2988.2799999999997, 'text': \" algorithm is not called gradient descent. It's called stochastic gradient descent. Now,\", 'tokens': [50600, 9284, 307, 406, 1219, 16235, 23475, 13, 467, 311, 1219, 342, 8997, 2750, 16235, 23475, 13, 823, 11, 50792], 'temperature': 0.0, 'avg_logprob': -0.08230556290725181, 'compression_ratio': 1.8022388059701493, 'no_speech_prob': 0.000282350811176002}, {'id': 527, 'seek': 297972, 'start': 2989.48, 'end': 2993.16, 'text': \" now, what's the middle ground? Right? Instead of computing it with respect to one example in your\", 'tokens': [50852, 586, 11, 437, 311, 264, 2808, 2727, 30, 1779, 30, 7156, 295, 15866, 309, 365, 3104, 281, 472, 1365, 294, 428, 51036], 'temperature': 0.0, 'avg_logprob': -0.08230556290725181, 'compression_ratio': 1.8022388059701493, 'no_speech_prob': 0.000282350811176002}, {'id': 528, 'seek': 297972, 'start': 2993.16, 'end': 2999.16, 'text': \" data set, what if we computed what's called a mini batch of examples, a small batch of examples\", 'tokens': [51036, 1412, 992, 11, 437, 498, 321, 40610, 437, 311, 1219, 257, 8382, 15245, 295, 5110, 11, 257, 1359, 15245, 295, 5110, 51336], 'temperature': 0.0, 'avg_logprob': -0.08230556290725181, 'compression_ratio': 1.8022388059701493, 'no_speech_prob': 0.000282350811176002}, {'id': 529, 'seek': 297972, 'start': 2999.16, 'end': 3005.7999999999997, 'text': \" that we can compute the gradients over? And when we take these gradients, they're still computationally\", 'tokens': [51336, 300, 321, 393, 14722, 264, 2771, 2448, 670, 30, 400, 562, 321, 747, 613, 2771, 2448, 11, 436, 434, 920, 24903, 379, 51668], 'temperature': 0.0, 'avg_logprob': -0.08230556290725181, 'compression_ratio': 1.8022388059701493, 'no_speech_prob': 0.000282350811176002}, {'id': 530, 'seek': 300580, 'start': 3005.8, 'end': 3010.36, 'text': \" efficient to compute because it's a mini batch. It's not too large. Maybe we're talking on the order\", 'tokens': [50364, 7148, 281, 14722, 570, 309, 311, 257, 8382, 15245, 13, 467, 311, 406, 886, 2416, 13, 2704, 321, 434, 1417, 322, 264, 1668, 50592], 'temperature': 0.0, 'avg_logprob': -0.10073016429769582, 'compression_ratio': 1.7870036101083033, 'no_speech_prob': 0.0063836537301540375}, {'id': 531, 'seek': 300580, 'start': 3010.36, 'end': 3018.04, 'text': \" of tens or hundreds of examples in our data set, but more importantly, because we've expanded from\", 'tokens': [50592, 295, 10688, 420, 6779, 295, 5110, 294, 527, 1412, 992, 11, 457, 544, 8906, 11, 570, 321, 600, 14342, 490, 50976], 'temperature': 0.0, 'avg_logprob': -0.10073016429769582, 'compression_ratio': 1.7870036101083033, 'no_speech_prob': 0.0063836537301540375}, {'id': 532, 'seek': 300580, 'start': 3018.04, 'end': 3023.2400000000002, 'text': ' a single example to maybe a hundred examples, the stochasticity is significantly reduced and the', 'tokens': [50976, 257, 2167, 1365, 281, 1310, 257, 3262, 5110, 11, 264, 342, 8997, 2750, 507, 307, 10591, 9212, 293, 264, 51236], 'temperature': 0.0, 'avg_logprob': -0.10073016429769582, 'compression_ratio': 1.7870036101083033, 'no_speech_prob': 0.0063836537301540375}, {'id': 533, 'seek': 300580, 'start': 3023.2400000000002, 'end': 3029.5600000000004, 'text': \" accuracy of our gradient is much improved. So normally, we're thinking of batch sizes, mini batch\", 'tokens': [51236, 14170, 295, 527, 16235, 307, 709, 9689, 13, 407, 5646, 11, 321, 434, 1953, 295, 15245, 11602, 11, 8382, 15245, 51552], 'temperature': 0.0, 'avg_logprob': -0.10073016429769582, 'compression_ratio': 1.7870036101083033, 'no_speech_prob': 0.0063836537301540375}, {'id': 534, 'seek': 300580, 'start': 3029.5600000000004, 'end': 3035.48, 'text': ' sizes roughly on the order of 100 data points, tens or hundreds of data points. This is much faster,', 'tokens': [51552, 11602, 9810, 322, 264, 1668, 295, 2319, 1412, 2793, 11, 10688, 420, 6779, 295, 1412, 2793, 13, 639, 307, 709, 4663, 11, 51848], 'temperature': 0.0, 'avg_logprob': -0.10073016429769582, 'compression_ratio': 1.7870036101083033, 'no_speech_prob': 0.0063836537301540375}, {'id': 535, 'seek': 303548, 'start': 3035.48, 'end': 3040.44, 'text': ' obviously, to compute than gradient descent and much more accurate to compute compared to stochastic', 'tokens': [50364, 2745, 11, 281, 14722, 813, 16235, 23475, 293, 709, 544, 8559, 281, 14722, 5347, 281, 342, 8997, 2750, 50612], 'temperature': 0.0, 'avg_logprob': -0.08486761228002683, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.00017946069419849664}, {'id': 536, 'seek': 303548, 'start': 3040.44, 'end': 3047.32, 'text': ' gradient descent, which is that single point example. So this increase in gradient accuracy', 'tokens': [50612, 16235, 23475, 11, 597, 307, 300, 2167, 935, 1365, 13, 407, 341, 3488, 294, 16235, 14170, 50956], 'temperature': 0.0, 'avg_logprob': -0.08486761228002683, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.00017946069419849664}, {'id': 537, 'seek': 303548, 'start': 3048.28, 'end': 3053.48, 'text': ' allows us to essentially converge to our solution much quicker than it could have been possible', 'tokens': [51004, 4045, 505, 281, 4476, 41881, 281, 527, 3827, 709, 16255, 813, 309, 727, 362, 668, 1944, 51264], 'temperature': 0.0, 'avg_logprob': -0.08486761228002683, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.00017946069419849664}, {'id': 538, 'seek': 303548, 'start': 3053.48, 'end': 3058.76, 'text': ' in practice due to gradient descent limitations. It also means that we can increase our learn', 'tokens': [51264, 294, 3124, 3462, 281, 16235, 23475, 15705, 13, 467, 611, 1355, 300, 321, 393, 3488, 527, 1466, 51528], 'temperature': 0.0, 'avg_logprob': -0.08486761228002683, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.00017946069419849664}, {'id': 539, 'seek': 303548, 'start': 3058.76, 'end': 3064.2, 'text': \" grade because we can trust each of those gradients much more efficiently, right? We're now averaging\", 'tokens': [51528, 7204, 570, 321, 393, 3361, 1184, 295, 729, 2771, 2448, 709, 544, 19621, 11, 558, 30, 492, 434, 586, 47308, 51800], 'temperature': 0.0, 'avg_logprob': -0.08486761228002683, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.00017946069419849664}, {'id': 540, 'seek': 306420, 'start': 3064.2, 'end': 3068.4399999999996, 'text': \" over a batch. It's going to be much more accurate than the stochastic version, so we can increase\", 'tokens': [50364, 670, 257, 15245, 13, 467, 311, 516, 281, 312, 709, 544, 8559, 813, 264, 342, 8997, 2750, 3037, 11, 370, 321, 393, 3488, 50576], 'temperature': 0.0, 'avg_logprob': -0.09015744382684882, 'compression_ratio': 1.5772357723577235, 'no_speech_prob': 0.0007670219638384879}, {'id': 541, 'seek': 306420, 'start': 3068.4399999999996, 'end': 3075.24, 'text': ' that learning grade and actually learn faster as well. This allows us to also massively parallelize', 'tokens': [50576, 300, 2539, 7204, 293, 767, 1466, 4663, 382, 731, 13, 639, 4045, 505, 281, 611, 29379, 8952, 1125, 50916], 'temperature': 0.0, 'avg_logprob': -0.09015744382684882, 'compression_ratio': 1.5772357723577235, 'no_speech_prob': 0.0007670219638384879}, {'id': 542, 'seek': 306420, 'start': 3075.24, 'end': 3081.08, 'text': ' this entire algorithm in computation, right? We can split up batches onto separate workers and', 'tokens': [50916, 341, 2302, 9284, 294, 24903, 11, 558, 30, 492, 393, 7472, 493, 15245, 279, 3911, 4994, 5600, 293, 51208], 'temperature': 0.0, 'avg_logprob': -0.09015744382684882, 'compression_ratio': 1.5772357723577235, 'no_speech_prob': 0.0007670219638384879}, {'id': 543, 'seek': 306420, 'start': 3081.08, 'end': 3088.12, 'text': ' achieve even more significant speedups of this entire problem using GPUs. The last topic that I', 'tokens': [51208, 4584, 754, 544, 4776, 3073, 7528, 295, 341, 2302, 1154, 1228, 18407, 82, 13, 440, 1036, 4829, 300, 286, 51560], 'temperature': 0.0, 'avg_logprob': -0.09015744382684882, 'compression_ratio': 1.5772357723577235, 'no_speech_prob': 0.0007670219638384879}, {'id': 544, 'seek': 308812, 'start': 3088.12, 'end': 3094.2799999999997, 'text': \" very, very briefly want to cover in today's lecture is this topic of overfitting, right? When we\", 'tokens': [50364, 588, 11, 588, 10515, 528, 281, 2060, 294, 965, 311, 7991, 307, 341, 4829, 295, 670, 69, 2414, 11, 558, 30, 1133, 321, 50672], 'temperature': 0.0, 'avg_logprob': -0.08203669956752233, 'compression_ratio': 1.7022222222222223, 'no_speech_prob': 0.002358471043407917}, {'id': 545, 'seek': 308812, 'start': 3094.2799999999997, 'end': 3101.0, 'text': \" are optimizing a neural network with stochastic gradient descent, we have this challenge of what's\", 'tokens': [50672, 366, 40425, 257, 18161, 3209, 365, 342, 8997, 2750, 16235, 23475, 11, 321, 362, 341, 3430, 295, 437, 311, 51008], 'temperature': 0.0, 'avg_logprob': -0.08203669956752233, 'compression_ratio': 1.7022222222222223, 'no_speech_prob': 0.002358471043407917}, {'id': 546, 'seek': 308812, 'start': 3101.0, 'end': 3106.3599999999997, 'text': ' called overfitting. Overfitting, I look like this roughly, right? So on the left hand side,', 'tokens': [51008, 1219, 670, 69, 2414, 13, 4886, 69, 2414, 11, 286, 574, 411, 341, 9810, 11, 558, 30, 407, 322, 264, 1411, 1011, 1252, 11, 51276], 'temperature': 0.0, 'avg_logprob': -0.08203669956752233, 'compression_ratio': 1.7022222222222223, 'no_speech_prob': 0.002358471043407917}, {'id': 547, 'seek': 308812, 'start': 3107.48, 'end': 3111.72, 'text': \" we want to build a neural network, or let's say in general, we want to build a machine learning\", 'tokens': [51332, 321, 528, 281, 1322, 257, 18161, 3209, 11, 420, 718, 311, 584, 294, 2674, 11, 321, 528, 281, 1322, 257, 3479, 2539, 51544], 'temperature': 0.0, 'avg_logprob': -0.08203669956752233, 'compression_ratio': 1.7022222222222223, 'no_speech_prob': 0.002358471043407917}, {'id': 548, 'seek': 311172, 'start': 3111.72, 'end': 3118.12, 'text': \" model that can accurately describe some patterns in our data, but remember, we're ultimately,\", 'tokens': [50364, 2316, 300, 393, 20095, 6786, 512, 8294, 294, 527, 1412, 11, 457, 1604, 11, 321, 434, 6284, 11, 50684], 'temperature': 0.0, 'avg_logprob': -0.08233569349561419, 'compression_ratio': 2.0204918032786887, 'no_speech_prob': 0.012615551240742207}, {'id': 549, 'seek': 311172, 'start': 3118.12, 'end': 3122.9199999999996, 'text': \" we don't want to describe the patterns in our training data. Ideally, we want to define the patterns\", 'tokens': [50684, 321, 500, 380, 528, 281, 6786, 264, 8294, 294, 527, 3097, 1412, 13, 40817, 11, 321, 528, 281, 6964, 264, 8294, 50924], 'temperature': 0.0, 'avg_logprob': -0.08233569349561419, 'compression_ratio': 2.0204918032786887, 'no_speech_prob': 0.012615551240742207}, {'id': 550, 'seek': 311172, 'start': 3122.9199999999996, 'end': 3127.7999999999997, 'text': \" in our test data. Of course, we don't observe test data. We only observe training data. So we have\", 'tokens': [50924, 294, 527, 1500, 1412, 13, 2720, 1164, 11, 321, 500, 380, 11441, 1500, 1412, 13, 492, 787, 11441, 3097, 1412, 13, 407, 321, 362, 51168], 'temperature': 0.0, 'avg_logprob': -0.08233569349561419, 'compression_ratio': 2.0204918032786887, 'no_speech_prob': 0.012615551240742207}, {'id': 551, 'seek': 311172, 'start': 3127.7999999999997, 'end': 3133.0, 'text': ' this challenge of extracting patterns from training data and hoping that they generalize to our test', 'tokens': [51168, 341, 3430, 295, 49844, 8294, 490, 3097, 1412, 293, 7159, 300, 436, 2674, 1125, 281, 527, 1500, 51428], 'temperature': 0.0, 'avg_logprob': -0.08233569349561419, 'compression_ratio': 2.0204918032786887, 'no_speech_prob': 0.012615551240742207}, {'id': 552, 'seek': 311172, 'start': 3133.0, 'end': 3138.4399999999996, 'text': ' data. So set in one different way, we want to build models that can learn representations from our', 'tokens': [51428, 1412, 13, 407, 992, 294, 472, 819, 636, 11, 321, 528, 281, 1322, 5245, 300, 393, 1466, 33358, 490, 527, 51700], 'temperature': 0.0, 'avg_logprob': -0.08233569349561419, 'compression_ratio': 2.0204918032786887, 'no_speech_prob': 0.012615551240742207}, {'id': 553, 'seek': 313844, 'start': 3138.44, 'end': 3145.08, 'text': ' training data that can still generalize even when we show them brand new unseen pieces of test data.', 'tokens': [50364, 3097, 1412, 300, 393, 920, 2674, 1125, 754, 562, 321, 855, 552, 3360, 777, 40608, 3755, 295, 1500, 1412, 13, 50696], 'temperature': 0.0, 'avg_logprob': -0.08710828423500061, 'compression_ratio': 1.6512605042016806, 'no_speech_prob': 0.0004372166295070201}, {'id': 554, 'seek': 313844, 'start': 3145.64, 'end': 3151.16, 'text': ' So assume that you want to build a line that can describe or find the patterns in these points', 'tokens': [50724, 407, 6552, 300, 291, 528, 281, 1322, 257, 1622, 300, 393, 6786, 420, 915, 264, 8294, 294, 613, 2793, 51000], 'temperature': 0.0, 'avg_logprob': -0.08710828423500061, 'compression_ratio': 1.6512605042016806, 'no_speech_prob': 0.0004372166295070201}, {'id': 555, 'seek': 313844, 'start': 3151.16, 'end': 3156.36, 'text': ' that you can see on the slide, right? If you have a very simple neural network, which is just a single', 'tokens': [51000, 300, 291, 393, 536, 322, 264, 4137, 11, 558, 30, 759, 291, 362, 257, 588, 2199, 18161, 3209, 11, 597, 307, 445, 257, 2167, 51260], 'temperature': 0.0, 'avg_logprob': -0.08710828423500061, 'compression_ratio': 1.6512605042016806, 'no_speech_prob': 0.0004372166295070201}, {'id': 556, 'seek': 313844, 'start': 3156.36, 'end': 3163.96, 'text': ' line, straight line, you can describe this data sub-optimally, right? Because the data here is', 'tokens': [51260, 1622, 11, 2997, 1622, 11, 291, 393, 6786, 341, 1412, 1422, 12, 5747, 332, 379, 11, 558, 30, 1436, 264, 1412, 510, 307, 51640], 'temperature': 0.0, 'avg_logprob': -0.08710828423500061, 'compression_ratio': 1.6512605042016806, 'no_speech_prob': 0.0004372166295070201}, {'id': 557, 'seek': 316396, 'start': 3163.96, 'end': 3169.7200000000003, 'text': \" nonlinear, you're not going to accurately capture all of the nuances and subtleties in this data set.\", 'tokens': [50364, 2107, 28263, 11, 291, 434, 406, 516, 281, 20095, 7983, 439, 295, 264, 38775, 293, 7257, 2631, 530, 294, 341, 1412, 992, 13, 50652], 'temperature': 0.0, 'avg_logprob': -0.10487216081076521, 'compression_ratio': 1.8345323741007193, 'no_speech_prob': 0.011146606877446175}, {'id': 558, 'seek': 316396, 'start': 3169.7200000000003, 'end': 3174.36, 'text': \" That's on the left hand side. If you move to the right hand side, you can see a much more complicated\", 'tokens': [50652, 663, 311, 322, 264, 1411, 1011, 1252, 13, 759, 291, 1286, 281, 264, 558, 1011, 1252, 11, 291, 393, 536, 257, 709, 544, 6179, 50884], 'temperature': 0.0, 'avg_logprob': -0.10487216081076521, 'compression_ratio': 1.8345323741007193, 'no_speech_prob': 0.011146606877446175}, {'id': 559, 'seek': 316396, 'start': 3174.36, 'end': 3179.56, 'text': \" model, but here you're actually overexpressive. You're too expressive and you're capturing kind of the\", 'tokens': [50884, 2316, 11, 457, 510, 291, 434, 767, 38657, 87, 11637, 488, 13, 509, 434, 886, 40189, 293, 291, 434, 23384, 733, 295, 264, 51144], 'temperature': 0.0, 'avg_logprob': -0.10487216081076521, 'compression_ratio': 1.8345323741007193, 'no_speech_prob': 0.011146606877446175}, {'id': 560, 'seek': 316396, 'start': 3179.56, 'end': 3186.6, 'text': ' nuances, the spurious nuances in your training data that are actually not representative of your test', 'tokens': [51144, 38775, 11, 264, 637, 24274, 38775, 294, 428, 3097, 1412, 300, 366, 767, 406, 12424, 295, 428, 1500, 51496], 'temperature': 0.0, 'avg_logprob': -0.10487216081076521, 'compression_ratio': 1.8345323741007193, 'no_speech_prob': 0.011146606877446175}, {'id': 561, 'seek': 316396, 'start': 3186.6, 'end': 3191.8, 'text': ' data. Ideally, you want to end up with the model in the middle, which is basically the middle ground,', 'tokens': [51496, 1412, 13, 40817, 11, 291, 528, 281, 917, 493, 365, 264, 2316, 294, 264, 2808, 11, 597, 307, 1936, 264, 2808, 2727, 11, 51756], 'temperature': 0.0, 'avg_logprob': -0.10487216081076521, 'compression_ratio': 1.8345323741007193, 'no_speech_prob': 0.011146606877446175}, {'id': 562, 'seek': 319180, 'start': 3191.8, 'end': 3196.6800000000003, 'text': \" right? It's not too complex and it's not too simple. It still gives you what you want to perform\", 'tokens': [50364, 558, 30, 467, 311, 406, 886, 3997, 293, 309, 311, 406, 886, 2199, 13, 467, 920, 2709, 291, 437, 291, 528, 281, 2042, 50608], 'temperature': 0.0, 'avg_logprob': -0.06460349749674839, 'compression_ratio': 1.688135593220339, 'no_speech_prob': 0.0013661120319738984}, {'id': 563, 'seek': 319180, 'start': 3196.6800000000003, 'end': 3203.0800000000004, 'text': \" well and even when you give it brand new data. So to address this problem, let's briefly talk about\", 'tokens': [50608, 731, 293, 754, 562, 291, 976, 309, 3360, 777, 1412, 13, 407, 281, 2985, 341, 1154, 11, 718, 311, 10515, 751, 466, 50928], 'temperature': 0.0, 'avg_logprob': -0.06460349749674839, 'compression_ratio': 1.688135593220339, 'no_speech_prob': 0.0013661120319738984}, {'id': 564, 'seek': 319180, 'start': 3203.0800000000004, 'end': 3208.6800000000003, 'text': \" what's called regularization. Regularization is a technique that you can introduce to your training\", 'tokens': [50928, 437, 311, 1219, 3890, 2144, 13, 45659, 2144, 307, 257, 6532, 300, 291, 393, 5366, 281, 428, 3097, 51208], 'temperature': 0.0, 'avg_logprob': -0.06460349749674839, 'compression_ratio': 1.688135593220339, 'no_speech_prob': 0.0013661120319738984}, {'id': 565, 'seek': 319180, 'start': 3208.6800000000003, 'end': 3215.6400000000003, 'text': \" pipeline to discourage complex models from being learned. Now, as we've seen before, this is really\", 'tokens': [51208, 15517, 281, 21497, 609, 3997, 5245, 490, 885, 3264, 13, 823, 11, 382, 321, 600, 1612, 949, 11, 341, 307, 534, 51556], 'temperature': 0.0, 'avg_logprob': -0.06460349749674839, 'compression_ratio': 1.688135593220339, 'no_speech_prob': 0.0013661120319738984}, {'id': 566, 'seek': 319180, 'start': 3215.6400000000003, 'end': 3221.7200000000003, 'text': ' critical because neural networks are extremely large models, they are extremely prone to overfitting.', 'tokens': [51556, 4924, 570, 18161, 9590, 366, 4664, 2416, 5245, 11, 436, 366, 4664, 25806, 281, 670, 69, 2414, 13, 51860], 'temperature': 0.0, 'avg_logprob': -0.06460349749674839, 'compression_ratio': 1.688135593220339, 'no_speech_prob': 0.0013661120319738984}, {'id': 567, 'seek': 322180, 'start': 3222.2000000000003, 'end': 3227.4, 'text': ' So regularization and having techniques for regularization has extreme implications towards the', 'tokens': [50384, 407, 3890, 2144, 293, 1419, 7512, 337, 3890, 2144, 575, 8084, 16602, 3030, 264, 50644], 'temperature': 0.0, 'avg_logprob': -0.09905800016799776, 'compression_ratio': 1.8161764705882353, 'no_speech_prob': 0.0005609836080111563}, {'id': 568, 'seek': 322180, 'start': 3227.4, 'end': 3232.84, 'text': ' success of neural networks and having them generalize beyond training data far into our testing domain.', 'tokens': [50644, 2245, 295, 18161, 9590, 293, 1419, 552, 2674, 1125, 4399, 3097, 1412, 1400, 666, 527, 4997, 9274, 13, 50916], 'temperature': 0.0, 'avg_logprob': -0.09905800016799776, 'compression_ratio': 1.8161764705882353, 'no_speech_prob': 0.0005609836080111563}, {'id': 569, 'seek': 322180, 'start': 3233.8, 'end': 3239.0, 'text': ' The most popular technique for regularization in deep learning is called dropout and the idea of', 'tokens': [50964, 440, 881, 3743, 6532, 337, 3890, 2144, 294, 2452, 2539, 307, 1219, 3270, 346, 293, 264, 1558, 295, 51224], 'temperature': 0.0, 'avg_logprob': -0.09905800016799776, 'compression_ratio': 1.8161764705882353, 'no_speech_prob': 0.0005609836080111563}, {'id': 570, 'seek': 322180, 'start': 3239.0, 'end': 3244.6000000000004, 'text': \" dropout is actually very simple. Let's revisit it by drawing this picture of deep neural networks\", 'tokens': [51224, 3270, 346, 307, 767, 588, 2199, 13, 961, 311, 32676, 309, 538, 6316, 341, 3036, 295, 2452, 18161, 9590, 51504], 'temperature': 0.0, 'avg_logprob': -0.09905800016799776, 'compression_ratio': 1.8161764705882353, 'no_speech_prob': 0.0005609836080111563}, {'id': 571, 'seek': 322180, 'start': 3244.6000000000004, 'end': 3250.52, 'text': \" that we saw earlier in today's lecture. In dropout, during training, we essentially randomly select\", 'tokens': [51504, 300, 321, 1866, 3071, 294, 965, 311, 7991, 13, 682, 3270, 346, 11, 1830, 3097, 11, 321, 4476, 16979, 3048, 51800], 'temperature': 0.0, 'avg_logprob': -0.09905800016799776, 'compression_ratio': 1.8161764705882353, 'no_speech_prob': 0.0005609836080111563}, {'id': 572, 'seek': 325052, 'start': 3250.52, 'end': 3257.48, 'text': ' some subset of the neurons in this neural network and we try to prune them out with some random', 'tokens': [50364, 512, 25993, 295, 264, 22027, 294, 341, 18161, 3209, 293, 321, 853, 281, 582, 2613, 552, 484, 365, 512, 4974, 50712], 'temperature': 0.0, 'avg_logprob': -0.09834471203031994, 'compression_ratio': 1.819047619047619, 'no_speech_prob': 0.0015715495683252811}, {'id': 573, 'seek': 325052, 'start': 3257.48, 'end': 3262.6, 'text': ' probability. So for example, we can select this subset of neurons. We can randomly select them', 'tokens': [50712, 8482, 13, 407, 337, 1365, 11, 321, 393, 3048, 341, 25993, 295, 22027, 13, 492, 393, 16979, 3048, 552, 50968], 'temperature': 0.0, 'avg_logprob': -0.09834471203031994, 'compression_ratio': 1.819047619047619, 'no_speech_prob': 0.0015715495683252811}, {'id': 574, 'seek': 325052, 'start': 3262.6, 'end': 3268.92, 'text': ' with a probability of 50 percent and with that probability, we randomly turn them off or on', 'tokens': [50968, 365, 257, 8482, 295, 2625, 3043, 293, 365, 300, 8482, 11, 321, 16979, 1261, 552, 766, 420, 322, 51284], 'temperature': 0.0, 'avg_logprob': -0.09834471203031994, 'compression_ratio': 1.819047619047619, 'no_speech_prob': 0.0015715495683252811}, {'id': 575, 'seek': 325052, 'start': 3268.92, 'end': 3276.36, 'text': ' on different iterations of our training. So this is essentially forcing the neural network to learn', 'tokens': [51284, 322, 819, 36540, 295, 527, 3097, 13, 407, 341, 307, 4476, 19030, 264, 18161, 3209, 281, 1466, 51656], 'temperature': 0.0, 'avg_logprob': -0.09834471203031994, 'compression_ratio': 1.819047619047619, 'no_speech_prob': 0.0015715495683252811}, {'id': 576, 'seek': 327636, 'start': 3276.44, 'end': 3282.52, 'text': \" you can think of an ensemble of different models. On every iteration, it's going to be exposed to\", 'tokens': [50368, 291, 393, 519, 295, 364, 19492, 295, 819, 5245, 13, 1282, 633, 24784, 11, 309, 311, 516, 281, 312, 9495, 281, 50672], 'temperature': 0.0, 'avg_logprob': -0.08714095751444499, 'compression_ratio': 1.850187265917603, 'no_speech_prob': 0.0030727172270417213}, {'id': 577, 'seek': 327636, 'start': 3282.52, 'end': 3286.92, 'text': ' kind of a different model internally than the one it had on the last iteration. So it has to learn', 'tokens': [50672, 733, 295, 257, 819, 2316, 19501, 813, 264, 472, 309, 632, 322, 264, 1036, 24784, 13, 407, 309, 575, 281, 1466, 50892], 'temperature': 0.0, 'avg_logprob': -0.08714095751444499, 'compression_ratio': 1.850187265917603, 'no_speech_prob': 0.0030727172270417213}, {'id': 578, 'seek': 327636, 'start': 3286.92, 'end': 3292.76, 'text': \" how to build internal pathways to process the same information and it can't rely on information\", 'tokens': [50892, 577, 281, 1322, 6920, 22988, 281, 1399, 264, 912, 1589, 293, 309, 393, 380, 10687, 322, 1589, 51184], 'temperature': 0.0, 'avg_logprob': -0.08714095751444499, 'compression_ratio': 1.850187265917603, 'no_speech_prob': 0.0030727172270417213}, {'id': 579, 'seek': 327636, 'start': 3292.76, 'end': 3298.6, 'text': ' that it learned on previous iterations. So it forces it to kind of capture some deeper meaning within', 'tokens': [51184, 300, 309, 3264, 322, 3894, 36540, 13, 407, 309, 5874, 309, 281, 733, 295, 7983, 512, 7731, 3620, 1951, 51476], 'temperature': 0.0, 'avg_logprob': -0.08714095751444499, 'compression_ratio': 1.850187265917603, 'no_speech_prob': 0.0030727172270417213}, {'id': 580, 'seek': 327636, 'start': 3298.6, 'end': 3303.1600000000003, 'text': ' the pathways of the neural network and this can be extremely powerful because number one, it lowers', 'tokens': [51476, 264, 22988, 295, 264, 18161, 3209, 293, 341, 393, 312, 4664, 4005, 570, 1230, 472, 11, 309, 44936, 51704], 'temperature': 0.0, 'avg_logprob': -0.08714095751444499, 'compression_ratio': 1.850187265917603, 'no_speech_prob': 0.0030727172270417213}, {'id': 581, 'seek': 330316, 'start': 3303.24, 'end': 3308.68, 'text': \" the capacity of the neural network significantly. You're lowering it by roughly 50 percent in this\", 'tokens': [50368, 264, 6042, 295, 264, 18161, 3209, 10591, 13, 509, 434, 28124, 309, 538, 9810, 2625, 3043, 294, 341, 50640], 'temperature': 0.0, 'avg_logprob': -0.128396185759072, 'compression_ratio': 1.5864406779661018, 'no_speech_prob': 0.0001634252694202587}, {'id': 582, 'seek': 330316, 'start': 3308.68, 'end': 3314.68, 'text': ' example, but also because it makes it easier to train because the number of weights that have', 'tokens': [50640, 1365, 11, 457, 611, 570, 309, 1669, 309, 3571, 281, 3847, 570, 264, 1230, 295, 17443, 300, 362, 50940], 'temperature': 0.0, 'avg_logprob': -0.128396185759072, 'compression_ratio': 1.5864406779661018, 'no_speech_prob': 0.0001634252694202587}, {'id': 583, 'seek': 330316, 'start': 3314.68, 'end': 3318.92, 'text': \" gradients in this case is also reduced. So it's actually much faster to train them as well.\", 'tokens': [50940, 2771, 2448, 294, 341, 1389, 307, 611, 9212, 13, 407, 309, 311, 767, 709, 4663, 281, 3847, 552, 382, 731, 13, 51152], 'temperature': 0.0, 'avg_logprob': -0.128396185759072, 'compression_ratio': 1.5864406779661018, 'no_speech_prob': 0.0001634252694202587}, {'id': 584, 'seek': 330316, 'start': 3320.2799999999997, 'end': 3326.04, 'text': ' Now, like I mentioned, on every iteration, we randomly drop out a different set of neurons,', 'tokens': [51220, 823, 11, 411, 286, 2835, 11, 322, 633, 24784, 11, 321, 16979, 3270, 484, 257, 819, 992, 295, 22027, 11, 51508], 'temperature': 0.0, 'avg_logprob': -0.128396185759072, 'compression_ratio': 1.5864406779661018, 'no_speech_prob': 0.0001634252694202587}, {'id': 585, 'seek': 330316, 'start': 3326.04, 'end': 3331.3199999999997, 'text': ' right, and that helps the data generalize better. And the second regularization techniques,', 'tokens': [51508, 558, 11, 293, 300, 3665, 264, 1412, 2674, 1125, 1101, 13, 400, 264, 1150, 3890, 2144, 7512, 11, 51772], 'temperature': 0.0, 'avg_logprob': -0.128396185759072, 'compression_ratio': 1.5864406779661018, 'no_speech_prob': 0.0001634252694202587}, {'id': 586, 'seek': 333132, 'start': 3331.32, 'end': 3336.52, 'text': ' which is actually a very broad regularization technique far beyond neural networks, is simply', 'tokens': [50364, 597, 307, 767, 257, 588, 4152, 3890, 2144, 6532, 1400, 4399, 18161, 9590, 11, 307, 2935, 50624], 'temperature': 0.0, 'avg_logprob': -0.07762890300531497, 'compression_ratio': 1.6553191489361703, 'no_speech_prob': 0.0008964719600044191}, {'id': 587, 'seek': 333132, 'start': 3336.52, 'end': 3344.76, 'text': ' called early stopping. Now, we know the definition of overfitting is simply when our model starts to', 'tokens': [50624, 1219, 2440, 12767, 13, 823, 11, 321, 458, 264, 7123, 295, 670, 69, 2414, 307, 2935, 562, 527, 2316, 3719, 281, 51036], 'temperature': 0.0, 'avg_logprob': -0.07762890300531497, 'compression_ratio': 1.6553191489361703, 'no_speech_prob': 0.0008964719600044191}, {'id': 588, 'seek': 333132, 'start': 3344.76, 'end': 3349.7200000000003, 'text': \" represent basically the training data more than the testing data. That's really what overfitting\", 'tokens': [51036, 2906, 1936, 264, 3097, 1412, 544, 813, 264, 4997, 1412, 13, 663, 311, 534, 437, 670, 69, 2414, 51284], 'temperature': 0.0, 'avg_logprob': -0.07762890300531497, 'compression_ratio': 1.6553191489361703, 'no_speech_prob': 0.0008964719600044191}, {'id': 589, 'seek': 333132, 'start': 3349.7200000000003, 'end': 3355.48, 'text': \" comes down to its core. If we set aside some of the training data to use separately that we don't\", 'tokens': [51284, 1487, 760, 281, 1080, 4965, 13, 759, 321, 992, 7359, 512, 295, 264, 3097, 1412, 281, 764, 14759, 300, 321, 500, 380, 51572], 'temperature': 0.0, 'avg_logprob': -0.07762890300531497, 'compression_ratio': 1.6553191489361703, 'no_speech_prob': 0.0008964719600044191}, {'id': 590, 'seek': 335548, 'start': 3355.48, 'end': 3362.04, 'text': ' train on it, we can use a kind of a testing, a data set, synthetic testing data set in some ways.', 'tokens': [50364, 3847, 322, 309, 11, 321, 393, 764, 257, 733, 295, 257, 4997, 11, 257, 1412, 992, 11, 23420, 4997, 1412, 992, 294, 512, 2098, 13, 50692], 'temperature': 0.0, 'avg_logprob': -0.12112453119541572, 'compression_ratio': 1.7794117647058822, 'no_speech_prob': 0.0011329392436891794}, {'id': 591, 'seek': 335548, 'start': 3362.84, 'end': 3368.2, 'text': ' We can monitor how our network is learning on this unseen portion of data. So for example,', 'tokens': [50732, 492, 393, 6002, 577, 527, 3209, 307, 2539, 322, 341, 40608, 8044, 295, 1412, 13, 407, 337, 1365, 11, 51000], 'temperature': 0.0, 'avg_logprob': -0.12112453119541572, 'compression_ratio': 1.7794117647058822, 'no_speech_prob': 0.0011329392436891794}, {'id': 592, 'seek': 335548, 'start': 3368.2, 'end': 3373.4, 'text': ' we can, over the course of training, we can basically plot the performance of our network on both', 'tokens': [51000, 321, 393, 11, 670, 264, 1164, 295, 3097, 11, 321, 393, 1936, 7542, 264, 3389, 295, 527, 3209, 322, 1293, 51260], 'temperature': 0.0, 'avg_logprob': -0.12112453119541572, 'compression_ratio': 1.7794117647058822, 'no_speech_prob': 0.0011329392436891794}, {'id': 593, 'seek': 335548, 'start': 3373.4, 'end': 3378.6, 'text': \" the training set as well as our held out test set. And as the network is trained, we're going to see\", 'tokens': [51260, 264, 3097, 992, 382, 731, 382, 527, 5167, 484, 1500, 992, 13, 400, 382, 264, 3209, 307, 8895, 11, 321, 434, 516, 281, 536, 51520], 'temperature': 0.0, 'avg_logprob': -0.12112453119541572, 'compression_ratio': 1.7794117647058822, 'no_speech_prob': 0.0011329392436891794}, {'id': 594, 'seek': 335548, 'start': 3378.6, 'end': 3383.96, 'text': \" that, first of all, these both decrease, but there's going to be a point where the loss plateaus\", 'tokens': [51520, 300, 11, 700, 295, 439, 11, 613, 1293, 11514, 11, 457, 456, 311, 516, 281, 312, 257, 935, 689, 264, 4470, 5924, 8463, 51788], 'temperature': 0.0, 'avg_logprob': -0.12112453119541572, 'compression_ratio': 1.7794117647058822, 'no_speech_prob': 0.0011329392436891794}, {'id': 595, 'seek': 338396, 'start': 3384.6, 'end': 3388.76, 'text': ' and starts to increase. The training loss will actually start to increase. This is exactly the', 'tokens': [50396, 293, 3719, 281, 3488, 13, 440, 3097, 4470, 486, 767, 722, 281, 3488, 13, 639, 307, 2293, 264, 50604], 'temperature': 0.0, 'avg_logprob': -0.10592311251479967, 'compression_ratio': 2.016460905349794, 'no_speech_prob': 0.0017535428050905466}, {'id': 596, 'seek': 338396, 'start': 3388.76, 'end': 3394.28, 'text': \" point where you start to overfit, right? Because now you're starting to have, sorry, that was the test\", 'tokens': [50604, 935, 689, 291, 722, 281, 670, 6845, 11, 558, 30, 1436, 586, 291, 434, 2891, 281, 362, 11, 2597, 11, 300, 390, 264, 1500, 50880], 'temperature': 0.0, 'avg_logprob': -0.10592311251479967, 'compression_ratio': 2.016460905349794, 'no_speech_prob': 0.0017535428050905466}, {'id': 597, 'seek': 338396, 'start': 3394.28, 'end': 3398.04, 'text': \" loss. The test loss actually starts to increase because now you're starting to overfit on your\", 'tokens': [50880, 4470, 13, 440, 1500, 4470, 767, 3719, 281, 3488, 570, 586, 291, 434, 2891, 281, 670, 6845, 322, 428, 51068], 'temperature': 0.0, 'avg_logprob': -0.10592311251479967, 'compression_ratio': 2.016460905349794, 'no_speech_prob': 0.0017535428050905466}, {'id': 598, 'seek': 338396, 'start': 3398.04, 'end': 3404.2, 'text': ' training data. This pattern basically continues for the rest of training. And this is the point', 'tokens': [51068, 3097, 1412, 13, 639, 5102, 1936, 6515, 337, 264, 1472, 295, 3097, 13, 400, 341, 307, 264, 935, 51376], 'temperature': 0.0, 'avg_logprob': -0.10592311251479967, 'compression_ratio': 2.016460905349794, 'no_speech_prob': 0.0017535428050905466}, {'id': 599, 'seek': 338396, 'start': 3404.2, 'end': 3408.44, 'text': ' that I want you to focus on, right? This middle point is where we need to stop training because after', 'tokens': [51376, 300, 286, 528, 291, 281, 1879, 322, 11, 558, 30, 639, 2808, 935, 307, 689, 321, 643, 281, 1590, 3097, 570, 934, 51588], 'temperature': 0.0, 'avg_logprob': -0.10592311251479967, 'compression_ratio': 2.016460905349794, 'no_speech_prob': 0.0017535428050905466}, {'id': 600, 'seek': 340844, 'start': 3408.44, 'end': 3414.68, 'text': ' this point, assuming that this test set is a valid representation of the true test set,', 'tokens': [50364, 341, 935, 11, 11926, 300, 341, 1500, 992, 307, 257, 7363, 10290, 295, 264, 2074, 1500, 992, 11, 50676], 'temperature': 0.0, 'avg_logprob': -0.0694785558876871, 'compression_ratio': 1.725, 'no_speech_prob': 0.0007206593872979283}, {'id': 601, 'seek': 340844, 'start': 3414.68, 'end': 3418.76, 'text': ' this is the place where the accuracy of the model will only get worse, right? So this is where we', 'tokens': [50676, 341, 307, 264, 1081, 689, 264, 14170, 295, 264, 2316, 486, 787, 483, 5324, 11, 558, 30, 407, 341, 307, 689, 321, 50880], 'temperature': 0.0, 'avg_logprob': -0.0694785558876871, 'compression_ratio': 1.725, 'no_speech_prob': 0.0007206593872979283}, {'id': 602, 'seek': 340844, 'start': 3418.76, 'end': 3424.12, 'text': ' would want to early stop our model and regularize the performance. And we can see that stopping', 'tokens': [50880, 576, 528, 281, 2440, 1590, 527, 2316, 293, 3890, 1125, 264, 3389, 13, 400, 321, 393, 536, 300, 12767, 51148], 'temperature': 0.0, 'avg_logprob': -0.0694785558876871, 'compression_ratio': 1.725, 'no_speech_prob': 0.0007206593872979283}, {'id': 603, 'seek': 340844, 'start': 3424.12, 'end': 3429.48, 'text': \" any time before this point is also not good. We're going to produce an underfit model where we could\", 'tokens': [51148, 604, 565, 949, 341, 935, 307, 611, 406, 665, 13, 492, 434, 516, 281, 5258, 364, 833, 6845, 2316, 689, 321, 727, 51416], 'temperature': 0.0, 'avg_logprob': -0.0694785558876871, 'compression_ratio': 1.725, 'no_speech_prob': 0.0007206593872979283}, {'id': 604, 'seek': 340844, 'start': 3429.48, 'end': 3433.96, 'text': \" have had a better model on the test data, but it's this tradeoff, right? You can't stop too late and\", 'tokens': [51416, 362, 632, 257, 1101, 2316, 322, 264, 1500, 1412, 11, 457, 309, 311, 341, 4923, 4506, 11, 558, 30, 509, 393, 380, 1590, 886, 3469, 293, 51640], 'temperature': 0.0, 'avg_logprob': -0.0694785558876871, 'compression_ratio': 1.725, 'no_speech_prob': 0.0007206593872979283}, {'id': 605, 'seek': 343396, 'start': 3433.96, 'end': 3440.84, 'text': \" you can't stop too early as well. So I'll conclude this lecture by just summarizing these three\", 'tokens': [50364, 291, 393, 380, 1590, 886, 2440, 382, 731, 13, 407, 286, 603, 16886, 341, 7991, 538, 445, 14611, 3319, 613, 1045, 50708], 'temperature': 0.0, 'avg_logprob': -0.09776176662620055, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 0.0007787907961755991}, {'id': 606, 'seek': 343396, 'start': 3440.84, 'end': 3446.2, 'text': \" key points that we've covered in today's lecture so far. So we first covered these fundamental\", 'tokens': [50708, 2141, 2793, 300, 321, 600, 5343, 294, 965, 311, 7991, 370, 1400, 13, 407, 321, 700, 5343, 613, 8088, 50976], 'temperature': 0.0, 'avg_logprob': -0.09776176662620055, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 0.0007787907961755991}, {'id': 607, 'seek': 343396, 'start': 3446.2, 'end': 3450.84, 'text': \" building blocks of all neural networks, which is the single neuron, the perceptron. We've built\", 'tokens': [50976, 2390, 8474, 295, 439, 18161, 9590, 11, 597, 307, 264, 2167, 34090, 11, 264, 43276, 2044, 13, 492, 600, 3094, 51208], 'temperature': 0.0, 'avg_logprob': -0.09776176662620055, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 0.0007787907961755991}, {'id': 608, 'seek': 343396, 'start': 3450.84, 'end': 3456.76, 'text': ' these up into larger neural layers and then from their neural networks and deep neural networks,', 'tokens': [51208, 613, 493, 666, 4833, 18161, 7914, 293, 550, 490, 641, 18161, 9590, 293, 2452, 18161, 9590, 11, 51504], 'temperature': 0.0, 'avg_logprob': -0.09776176662620055, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 0.0007787907961755991}, {'id': 609, 'seek': 343396, 'start': 3457.08, 'end': 3461.8, 'text': \" we've learned how we can train these, apply them to data sets, back propagate through them,\", 'tokens': [51520, 321, 600, 3264, 577, 321, 393, 3847, 613, 11, 3079, 552, 281, 1412, 6352, 11, 646, 48256, 807, 552, 11, 51756], 'temperature': 0.0, 'avg_logprob': -0.09776176662620055, 'compression_ratio': 1.7272727272727273, 'no_speech_prob': 0.0007787907961755991}, {'id': 610, 'seek': 346180, 'start': 3461.8, 'end': 3467.0800000000004, 'text': \" and we've seen some tricks and tricks for optimizing these systems end to end.\", 'tokens': [50364, 293, 321, 600, 1612, 512, 11733, 293, 11733, 337, 40425, 613, 3652, 917, 281, 917, 13, 50628], 'temperature': 0.0, 'avg_logprob': -0.1482161156674649, 'compression_ratio': 1.569811320754717, 'no_speech_prob': 0.00019693707872647792}, {'id': 611, 'seek': 346180, 'start': 3468.04, 'end': 3473.0800000000004, 'text': \" In the next lecture, we'll hear from AVA on deep sequence modeling using RNNs,\", 'tokens': [50676, 682, 264, 958, 7991, 11, 321, 603, 1568, 490, 316, 20914, 322, 2452, 8310, 15983, 1228, 45702, 45, 82, 11, 50928], 'temperature': 0.0, 'avg_logprob': -0.1482161156674649, 'compression_ratio': 1.569811320754717, 'no_speech_prob': 0.00019693707872647792}, {'id': 612, 'seek': 346180, 'start': 3473.0800000000004, 'end': 3478.6000000000004, 'text': ' and specifically this very exciting new type of model called the transformer', 'tokens': [50928, 293, 4682, 341, 588, 4670, 777, 2010, 295, 2316, 1219, 264, 31782, 51204], 'temperature': 0.0, 'avg_logprob': -0.1482161156674649, 'compression_ratio': 1.569811320754717, 'no_speech_prob': 0.00019693707872647792}, {'id': 613, 'seek': 346180, 'start': 3478.6000000000004, 'end': 3484.1200000000003, 'text': \" architecture and attention mechanisms. So maybe let's resume the class in about five minutes after\", 'tokens': [51204, 9482, 293, 3202, 15902, 13, 407, 1310, 718, 311, 15358, 264, 1508, 294, 466, 1732, 2077, 934, 51480], 'temperature': 0.0, 'avg_logprob': -0.1482161156674649, 'compression_ratio': 1.569811320754717, 'no_speech_prob': 0.00019693707872647792}, {'id': 614, 'seek': 346180, 'start': 3484.1200000000003, 'end': 3487.88, 'text': ' we have a chance to swap speakers and thank you so much for all of your attention.', 'tokens': [51480, 321, 362, 257, 2931, 281, 18135, 9518, 293, 1309, 291, 370, 709, 337, 439, 295, 428, 3202, 13, 51668], 'temperature': 0.0, 'avg_logprob': -0.1482161156674649, 'compression_ratio': 1.569811320754717, 'no_speech_prob': 0.00019693707872647792}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdlpSiqyfMxs",
        "outputId": "3f93125a-f9e9-4c9e-8b6c-b4ef409f9b3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 2,\n",
              " 'seek': 0,\n",
              " 'start': 19.88,\n",
              " 'end': 26.16,\n",
              " 'text': ' super excited to introduce you all to introduction to deep learning. Now, MIT Inter to Deep',\n",
              " 'tokens': [51358,\n",
              "  1687,\n",
              "  2919,\n",
              "  281,\n",
              "  5366,\n",
              "  291,\n",
              "  439,\n",
              "  281,\n",
              "  9339,\n",
              "  281,\n",
              "  2452,\n",
              "  2539,\n",
              "  13,\n",
              "  823,\n",
              "  11,\n",
              "  13100,\n",
              "  5751,\n",
              "  281,\n",
              "  14895,\n",
              "  51672],\n",
              " 'temperature': 0.0,\n",
              " 'avg_logprob': -0.2659889050384066,\n",
              " 'compression_ratio': 1.4126984126984128,\n",
              " 'no_speech_prob': 0.10993754118680954}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def store_segments(segments):\n",
        "  texts = []\n",
        "  start_times = []\n",
        "\n",
        "  for segment in segments:\n",
        "    text = segment['text']\n",
        "    start = segment['start']\n",
        "\n",
        "    # Convert the starting time to a datetime object\n",
        "    start_datetime = datetime.fromtimestamp(start)\n",
        "\n",
        "    # Format the starting time as a string in the format \"00:00:00\"\n",
        "    formatted_start_time = start_datetime.strftime('%H:%M:%S')\n",
        "\n",
        "    texts.append(\"\".join(text))\n",
        "    start_times.append(formatted_start_time)\n",
        "\n",
        "  return texts, start_times"
      ],
      "metadata": {
        "id": "G43zeNwUiSkv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_segments(res)"
      ],
      "metadata": {
        "id": "yaqzfco_iVOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts, start_times = store_segments(res)"
      ],
      "metadata": {
        "id": "-ZYECiqNgZ8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "qBBaaahmqCWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "DLAEaZSYqInM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-gpu"
      ],
      "metadata": {
        "id": "AbdW-kuUqM2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.chains import VectorDBQAWithSourcesChain\n",
        "from langchain import OpenAI\n",
        "import openai\n",
        "import faiss"
      ],
      "metadata": {
        "id": "lqQNXEvAqT62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "vgN9CeSaqhUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
        "docs = []\n",
        "metadatas = []\n",
        "for i, d in enumerate(texts):\n",
        "    splits = text_splitter.split_text(d)\n",
        "    docs.extend(splits)\n",
        "    metadatas.extend([{\"source\": start_times[i]}] * len(splits))\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "tjMXHVwRqqTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)\n",
        "faiss.write_index(store.index, \"docs.index\")"
      ],
      "metadata": {
        "id": "iFri6vBTqutR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain"
      ],
      "metadata": {
        "id": "bUOIpXCFYg_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = VectorDBQAWithSourcesChain.from_llm(llm=OpenAI(temperature=0.9), vectorstore=store)"
      ],
      "metadata": {
        "id": "dZObHNoxqx5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890b4648-f59c-468a-d7aa-a30224ff1ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/qa_with_sources/vector_db.py:67: UserWarning: `VectorDBQAWithSourcesChain` is deprecated - please use `from langchain.chains import RetrievalQAWithSourcesChain`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain({\"question\": \"What is Backpropagation?\"})"
      ],
      "metadata": {
        "id": "j9i9whWUYlJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Answer: {result['answer']}  Sources: {result['sources']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tKd-k8aa_xZ",
        "outputId": "70718e65-e99c-466f-edc4-89f56e2fbcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  Backpropagation is an algorithm used for training neural networks by propagating errors from the output back to the input.\n",
            "  Sources: 00:42:28, 00:43:02, 00:43:21\n"
          ]
        }
      ]
    }
  ]
}