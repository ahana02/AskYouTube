{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN37sQfYHScXyJdIX98+KZ+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Use GPU runtime"
      ],
      "metadata": {
        "id": "MtKoKnp63RqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyzEXeRkYGIb",
        "outputId": "b5b23e01-1d49-4094-88fe-974b5c7db367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/whisper.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEuh12CQYcyi",
        "outputId": "b448d432-6234-485f-96c0-c39e378dcfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import pytube"
      ],
      "metadata": {
        "id": "iqQXWexQYgBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the taken Youtube link\n",
        "url = \"https://www.youtube.com/watch?v=QDX-1M5Nj7s\"\n",
        "video = pytube.YouTube(url,use_oauth=False,\n",
        "        allow_oauth_cache=True)\n",
        "video.streams.get_highest_resolution().filesize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FUhWKz1Y0HB",
        "outputId": "d449e25f-27ff-41b5-ce04-3c3d32359276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93449491"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting and downloading as 'MP4' file\n",
        "audio = video.streams.get_audio_only()\n",
        "fn = audio.download(output_path=\"tmp.mp3\")"
      ],
      "metadata": {
        "id": "CAQzzx3VY6Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the 'base' model of Whisper\n",
        "model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "id": "q-fKBf3PaG4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = model.transcribe(\"/content/tmp.mp3/MIT Introduction to Deep Learning  6S191.mp4\")"
      ],
      "metadata": {
        "id": "YkKvgw6eZ5HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = transcription['segments']\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_jyeEqJaRiI",
        "outputId": "e98c66c9-a920-49ea-c516-f9e84b2608eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'id': 0, 'seek': 0, 'start': 0.0, 'end': 14.64, 'text': ' Good afternoon, everyone. Thank you all for joining today. My name is Alexander Amini,', 'tokens': [50364, 2205, 6499, 11, 1518, 13, 1044, 291, 439, 337, 5549, 965, 13, 1222, 1315, 307, 14845, 2012, 3812, 11, 51096], 'temperature': 0.0, 'avg_logprob': -0.2530507903168167, 'compression_ratio': 1.4450261780104712, 'no_speech_prob': 0.10994959622621536}, {'id': 1, 'seek': 0, 'start': 14.64, 'end': 19.88, 'text': \" and I'll be one of your course organizers this year, along with Ava. And together, we're\", 'tokens': [51096, 293, 286, 603, 312, 472, 295, 428, 1164, 35071, 341, 1064, 11, 2051, 365, 316, 2757, 13, 400, 1214, 11, 321, 434, 51358], 'temperature': 0.0, 'avg_logprob': -0.2530507903168167, 'compression_ratio': 1.4450261780104712, 'no_speech_prob': 0.10994959622621536}, {'id': 2, 'seek': 0, 'start': 19.88, 'end': 26.76, 'text': ' super excited to introduce you all to Introduction to Deep Learning. Now, MIT Inter to Deep Learning', 'tokens': [51358, 1687, 2919, 281, 5366, 291, 439, 281, 27193, 882, 281, 14895, 15205, 13, 823, 11, 13100, 5751, 281, 14895, 15205, 51702], 'temperature': 0.0, 'avg_logprob': -0.2530507903168167, 'compression_ratio': 1.4450261780104712, 'no_speech_prob': 0.10994959622621536}, {'id': 3, 'seek': 2676, 'start': 26.76, 'end': 32.84, 'text': ' is a really, really fun, exciting and fast-paced program here at MIT. And let me start by just', 'tokens': [50364, 307, 257, 534, 11, 534, 1019, 11, 4670, 293, 2370, 12, 47038, 1461, 510, 412, 13100, 13, 400, 718, 385, 722, 538, 445, 50668], 'temperature': 0.0, 'avg_logprob': -0.11671413694109235, 'compression_ratio': 1.703971119133574, 'no_speech_prob': 0.003118266351521015}, {'id': 4, 'seek': 2676, 'start': 32.84, 'end': 37.24, 'text': \" first of all, giving you a little bit of background into what we do and what you're going to\", 'tokens': [50668, 700, 295, 439, 11, 2902, 291, 257, 707, 857, 295, 3678, 666, 437, 321, 360, 293, 437, 291, 434, 516, 281, 50888], 'temperature': 0.0, 'avg_logprob': -0.11671413694109235, 'compression_ratio': 1.703971119133574, 'no_speech_prob': 0.003118266351521015}, {'id': 5, 'seek': 2676, 'start': 37.24, 'end': 42.72, 'text': \" learn about this year. So this week of Introduction to Deep Learning, we're going to cover a ton\", 'tokens': [50888, 1466, 466, 341, 1064, 13, 407, 341, 1243, 295, 27193, 882, 281, 14895, 15205, 11, 321, 434, 516, 281, 2060, 257, 2952, 51162], 'temperature': 0.0, 'avg_logprob': -0.11671413694109235, 'compression_ratio': 1.703971119133574, 'no_speech_prob': 0.003118266351521015}, {'id': 6, 'seek': 2676, 'start': 42.72, 'end': 47.88, 'text': \" of material in just one week. You'll learn the foundations of this really, really fascinating\", 'tokens': [51162, 295, 2527, 294, 445, 472, 1243, 13, 509, 603, 1466, 264, 22467, 295, 341, 534, 11, 534, 10343, 51420], 'temperature': 0.0, 'avg_logprob': -0.11671413694109235, 'compression_ratio': 1.703971119133574, 'no_speech_prob': 0.003118266351521015}, {'id': 7, 'seek': 2676, 'start': 47.88, 'end': 53.32000000000001, 'text': \" and exciting field of deep learning and artificial intelligence. And more importantly, you're\", 'tokens': [51420, 293, 4670, 2519, 295, 2452, 2539, 293, 11677, 7599, 13, 400, 544, 8906, 11, 291, 434, 51692], 'temperature': 0.0, 'avg_logprob': -0.11671413694109235, 'compression_ratio': 1.703971119133574, 'no_speech_prob': 0.003118266351521015}, {'id': 8, 'seek': 5332, 'start': 53.32, 'end': 59.36, 'text': ' going to get hands-on experience actually reinforcing what you learn in the lectures as part of', 'tokens': [50364, 516, 281, 483, 2377, 12, 266, 1752, 767, 48262, 437, 291, 1466, 294, 264, 16564, 382, 644, 295, 50666], 'temperature': 0.0, 'avg_logprob': -0.1361892819404602, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0007429471588693559}, {'id': 9, 'seek': 5332, 'start': 59.36, 'end': 65.64, 'text': ' hands-on software labs. Now, over the past decade, AI and deep learning have really had a huge', 'tokens': [50666, 2377, 12, 266, 4722, 20339, 13, 823, 11, 670, 264, 1791, 10378, 11, 7318, 293, 2452, 2539, 362, 534, 632, 257, 2603, 50980], 'temperature': 0.0, 'avg_logprob': -0.1361892819404602, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0007429471588693559}, {'id': 10, 'seek': 5332, 'start': 65.64, 'end': 71.24000000000001, 'text': ' resurgence and had incredible successes. And a lot of problems that even just a decade ago, we', 'tokens': [50980, 725, 44607, 293, 632, 4651, 26101, 13, 400, 257, 688, 295, 2740, 300, 754, 445, 257, 10378, 2057, 11, 321, 51260], 'temperature': 0.0, 'avg_logprob': -0.1361892819404602, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0007429471588693559}, {'id': 11, 'seek': 5332, 'start': 71.24000000000001, 'end': 75.96000000000001, 'text': \" thought we're not really even solvable in the near future. Now, we're solving with deep learning\", 'tokens': [51260, 1194, 321, 434, 406, 534, 754, 1404, 17915, 294, 264, 2651, 2027, 13, 823, 11, 321, 434, 12606, 365, 2452, 2539, 51496], 'temperature': 0.0, 'avg_logprob': -0.1361892819404602, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0007429471588693559}, {'id': 12, 'seek': 5332, 'start': 75.96000000000001, 'end': 83.08, 'text': ' with incredible ease. Now, this past year in particular of 2022 has been an incredible year for', 'tokens': [51496, 365, 4651, 12708, 13, 823, 11, 341, 1791, 1064, 294, 1729, 295, 20229, 575, 668, 364, 4651, 1064, 337, 51852], 'temperature': 0.0, 'avg_logprob': -0.1361892819404602, 'compression_ratio': 1.7638376383763839, 'no_speech_prob': 0.0007429471588693559}, {'id': 13, 'seek': 8308, 'start': 83.08, 'end': 88.12, 'text': \" deep learning progress. And I'd like to say that actually this past year in particular has been the\", 'tokens': [50364, 2452, 2539, 4205, 13, 400, 286, 1116, 411, 281, 584, 300, 767, 341, 1791, 1064, 294, 1729, 575, 668, 264, 50616], 'temperature': 0.0, 'avg_logprob': -0.06966872726167951, 'compression_ratio': 1.828358208955224, 'no_speech_prob': 0.0003458322025835514}, {'id': 14, 'seek': 8308, 'start': 88.12, 'end': 93.64, 'text': ' year of generative deep learning, using deep learning to generate brand new types of data that', 'tokens': [50616, 1064, 295, 1337, 1166, 2452, 2539, 11, 1228, 2452, 2539, 281, 8460, 3360, 777, 3467, 295, 1412, 300, 50892], 'temperature': 0.0, 'avg_logprob': -0.06966872726167951, 'compression_ratio': 1.828358208955224, 'no_speech_prob': 0.0003458322025835514}, {'id': 15, 'seek': 8308, 'start': 93.64, 'end': 99.72, 'text': ' have never been seen before and never existed in reality. In fact, I want to start this class by', 'tokens': [50892, 362, 1128, 668, 1612, 949, 293, 1128, 13135, 294, 4103, 13, 682, 1186, 11, 286, 528, 281, 722, 341, 1508, 538, 51196], 'temperature': 0.0, 'avg_logprob': -0.06966872726167951, 'compression_ratio': 1.828358208955224, 'no_speech_prob': 0.0003458322025835514}, {'id': 16, 'seek': 8308, 'start': 99.72, 'end': 104.68, 'text': ' actually showing you how we started this class several years ago, which was by playing this video', 'tokens': [51196, 767, 4099, 291, 577, 321, 1409, 341, 1508, 2940, 924, 2057, 11, 597, 390, 538, 2433, 341, 960, 51444], 'temperature': 0.0, 'avg_logprob': -0.06966872726167951, 'compression_ratio': 1.828358208955224, 'no_speech_prob': 0.0003458322025835514}, {'id': 17, 'seek': 8308, 'start': 104.68, 'end': 110.75999999999999, 'text': \" that I'll play in a second. Now, this video actually was an introductory video for the class. It was\", 'tokens': [51444, 300, 286, 603, 862, 294, 257, 1150, 13, 823, 11, 341, 960, 767, 390, 364, 39048, 960, 337, 264, 1508, 13, 467, 390, 51748], 'temperature': 0.0, 'avg_logprob': -0.06966872726167951, 'compression_ratio': 1.828358208955224, 'no_speech_prob': 0.0003458322025835514}, {'id': 18, 'seek': 11076, 'start': 111.56, 'end': 116.68, 'text': \" and kind of exemplifies this idea that I'm talking about. So let me just start there and play\", 'tokens': [50404, 293, 733, 295, 24112, 11221, 341, 1558, 300, 286, 478, 1417, 466, 13, 407, 718, 385, 445, 722, 456, 293, 862, 50660], 'temperature': 0.0, 'avg_logprob': -0.45643162727355957, 'compression_ratio': 1.2335766423357664, 'no_speech_prob': 0.006934036500751972}, {'id': 19, 'seek': 11076, 'start': 116.68, 'end': 127.32000000000001, 'text': ' this video first of all. Hi everybody and welcome to MIT, six S-191. We are', 'tokens': [50660, 341, 960, 700, 295, 439, 13, 2421, 2201, 293, 2928, 281, 13100, 11, 2309, 318, 12, 3405, 16, 13, 492, 366, 51192], 'temperature': 0.0, 'avg_logprob': -0.45643162727355957, 'compression_ratio': 1.2335766423357664, 'no_speech_prob': 0.006934036500751972}, {'id': 20, 'seek': 12732, 'start': 128.12, 'end': 135.88, 'text': \" conducting course on deep learning to talk here at MIT. We've learned to revolutionize\", 'tokens': [50404, 21749, 1164, 322, 2452, 2539, 281, 751, 510, 412, 13100, 13, 492, 600, 3264, 281, 8894, 1125, 50792], 'temperature': 0.0, 'avg_logprob': -0.6025000400230532, 'compression_ratio': 1.5055555555555555, 'no_speech_prob': 0.46394476294517517}, {'id': 21, 'seek': 12732, 'start': 135.88, 'end': 144.6, 'text': \" things so many things from the bodies, the medicine and everything in the field. You'll learn\", 'tokens': [50792, 721, 370, 867, 721, 490, 264, 7510, 11, 264, 7195, 293, 1203, 294, 264, 2519, 13, 509, 603, 1466, 51228], 'temperature': 0.0, 'avg_logprob': -0.6025000400230532, 'compression_ratio': 1.5055555555555555, 'no_speech_prob': 0.46394476294517517}, {'id': 22, 'seek': 12732, 'start': 144.6, 'end': 151.64, 'text': ' upon the levels of this field and how you can build some of these incredible alternatives.', 'tokens': [51228, 3564, 264, 4358, 295, 341, 2519, 293, 577, 291, 393, 1322, 512, 295, 613, 4651, 20478, 13, 51580], 'temperature': 0.0, 'avg_logprob': -0.6025000400230532, 'compression_ratio': 1.5055555555555555, 'no_speech_prob': 0.46394476294517517}, {'id': 23, 'seek': 15164, 'start': 151.64, 'end': 161.79999999999998, 'text': \" In fact, this is where our students and video are not real and we're created to be deep learning\", 'tokens': [50364, 682, 1186, 11, 341, 307, 689, 527, 1731, 293, 960, 366, 406, 957, 293, 321, 434, 2942, 281, 312, 2452, 2539, 50872], 'temperature': 0.0, 'avg_logprob': -0.5536471170092386, 'compression_ratio': 1.5093167701863355, 'no_speech_prob': 0.01316524762660265}, {'id': 24, 'seek': 15164, 'start': 161.79999999999998, 'end': 170.11999999999998, 'text': \" and our visual intelligence. And in this class, you'll learn how to have been honored\", 'tokens': [50872, 293, 527, 5056, 7599, 13, 400, 294, 341, 1508, 11, 291, 603, 1466, 577, 281, 362, 668, 14556, 51288], 'temperature': 0.0, 'avg_logprob': -0.5536471170092386, 'compression_ratio': 1.5093167701863355, 'no_speech_prob': 0.01316524762660265}, {'id': 25, 'seek': 15164, 'start': 170.11999999999998, 'end': 173.56, 'text': \" with me today and I hope to be in the way that we're seeing.\", 'tokens': [51288, 365, 385, 965, 293, 286, 1454, 281, 312, 294, 264, 636, 300, 321, 434, 2577, 13, 51460], 'temperature': 0.0, 'avg_logprob': -0.5536471170092386, 'compression_ratio': 1.5093167701863355, 'no_speech_prob': 0.01316524762660265}, {'id': 26, 'seek': 17356, 'start': 173.8, 'end': 183.32, 'text': \" So in case you couldn't tell, this video and its entire audio was actually not real. It was\", 'tokens': [50376, 407, 294, 1389, 291, 2809, 380, 980, 11, 341, 960, 293, 1080, 2302, 6278, 390, 767, 406, 957, 13, 467, 390, 50852], 'temperature': 0.0, 'avg_logprob': -0.13923942611878176, 'compression_ratio': 1.7177033492822966, 'no_speech_prob': 0.029638826847076416}, {'id': 27, 'seek': 17356, 'start': 183.32, 'end': 188.76, 'text': ' synthetically generated by a deep learning algorithm and when we introduced this class a few years', 'tokens': [50852, 10657, 22652, 10833, 538, 257, 2452, 2539, 9284, 293, 562, 321, 7268, 341, 1508, 257, 1326, 924, 51124], 'temperature': 0.0, 'avg_logprob': -0.13923942611878176, 'compression_ratio': 1.7177033492822966, 'no_speech_prob': 0.029638826847076416}, {'id': 28, 'seek': 17356, 'start': 188.76, 'end': 193.16, 'text': ' ago, this video was created several years ago. But even several years ago,', 'tokens': [51124, 2057, 11, 341, 960, 390, 2942, 2940, 924, 2057, 13, 583, 754, 2940, 924, 2057, 11, 51344], 'temperature': 0.0, 'avg_logprob': -0.13923942611878176, 'compression_ratio': 1.7177033492822966, 'no_speech_prob': 0.029638826847076416}, {'id': 29, 'seek': 17356, 'start': 193.88, 'end': 198.36, 'text': ' when we introduced this and put it on YouTube, when some were viral, people really loved this', 'tokens': [51380, 562, 321, 7268, 341, 293, 829, 309, 322, 3088, 11, 562, 512, 645, 16132, 11, 561, 534, 4333, 341, 51604], 'temperature': 0.0, 'avg_logprob': -0.13923942611878176, 'compression_ratio': 1.7177033492822966, 'no_speech_prob': 0.029638826847076416}, {'id': 30, 'seek': 19836, 'start': 198.36, 'end': 206.04000000000002, 'text': ' video. They were intrigued by how real the video and audio felt and looked entirely generated by', 'tokens': [50364, 960, 13, 814, 645, 35140, 538, 577, 957, 264, 960, 293, 6278, 2762, 293, 2956, 7696, 10833, 538, 50748], 'temperature': 0.0, 'avg_logprob': -0.10352805565143454, 'compression_ratio': 1.6608695652173913, 'no_speech_prob': 0.0005879112868569791}, {'id': 31, 'seek': 19836, 'start': 206.04000000000002, 'end': 211.16000000000003, 'text': ' an algorithm, by a computer. And people were shocked with the power and the realism of these', 'tokens': [50748, 364, 9284, 11, 538, 257, 3820, 13, 400, 561, 645, 12763, 365, 264, 1347, 293, 264, 38484, 295, 613, 51004], 'temperature': 0.0, 'avg_logprob': -0.10352805565143454, 'compression_ratio': 1.6608695652173913, 'no_speech_prob': 0.0005879112868569791}, {'id': 32, 'seek': 19836, 'start': 211.16000000000003, 'end': 217.24, 'text': ' types of approaches and this was a few years ago. Now, fast forward to today and the state of deep', 'tokens': [51004, 3467, 295, 11587, 293, 341, 390, 257, 1326, 924, 2057, 13, 823, 11, 2370, 2128, 281, 965, 293, 264, 1785, 295, 2452, 51308], 'temperature': 0.0, 'avg_logprob': -0.10352805565143454, 'compression_ratio': 1.6608695652173913, 'no_speech_prob': 0.0005879112868569791}, {'id': 33, 'seek': 19836, 'start': 217.24, 'end': 224.92000000000002, 'text': \" learning today, we have seen deep learning accelerating at a rate faster than we've ever seen\", 'tokens': [51308, 2539, 965, 11, 321, 362, 1612, 2452, 2539, 34391, 412, 257, 3314, 4663, 813, 321, 600, 1562, 1612, 51692], 'temperature': 0.0, 'avg_logprob': -0.10352805565143454, 'compression_ratio': 1.6608695652173913, 'no_speech_prob': 0.0005879112868569791}, {'id': 34, 'seek': 22492, 'start': 225.07999999999998, 'end': 232.2, 'text': ' before. In fact, we can use deep learning now to generate not just images of faces, but generate', 'tokens': [50372, 949, 13, 682, 1186, 11, 321, 393, 764, 2452, 2539, 586, 281, 8460, 406, 445, 5267, 295, 8475, 11, 457, 8460, 50728], 'temperature': 0.0, 'avg_logprob': -0.10844936557844573, 'compression_ratio': 1.7204301075268817, 'no_speech_prob': 0.0060876174829900265}, {'id': 35, 'seek': 22492, 'start': 232.2, 'end': 237.39999999999998, 'text': ' full synthetic environments where we can train autonomous vehicles entirely in simulation and', 'tokens': [50728, 1577, 23420, 12388, 689, 321, 393, 3847, 23797, 8948, 7696, 294, 16575, 293, 50988], 'temperature': 0.0, 'avg_logprob': -0.10844936557844573, 'compression_ratio': 1.7204301075268817, 'no_speech_prob': 0.0060876174829900265}, {'id': 36, 'seek': 22492, 'start': 237.39999999999998, 'end': 242.67999999999998, 'text': ' deploy them on full scale vehicles in the real world seamlessly. The videos here you see are', 'tokens': [50988, 7274, 552, 322, 1577, 4373, 8948, 294, 264, 957, 1002, 38083, 13, 440, 2145, 510, 291, 536, 366, 51252], 'temperature': 0.0, 'avg_logprob': -0.10844936557844573, 'compression_ratio': 1.7204301075268817, 'no_speech_prob': 0.0060876174829900265}, {'id': 37, 'seek': 22492, 'start': 242.67999999999998, 'end': 247.79999999999998, 'text': ' actually from a data driven simulator from neural networks generated called Vista that we actually', 'tokens': [51252, 767, 490, 257, 1412, 9555, 32974, 490, 18161, 9590, 10833, 1219, 691, 5236, 300, 321, 767, 51508], 'temperature': 0.0, 'avg_logprob': -0.10844936557844573, 'compression_ratio': 1.7204301075268817, 'no_speech_prob': 0.0060876174829900265}, {'id': 38, 'seek': 22492, 'start': 247.79999999999998, 'end': 253.39999999999998, 'text': ' built here at MIT and have open sourced to the public. So all of you can actually train and build', 'tokens': [51508, 3094, 510, 412, 13100, 293, 362, 1269, 11006, 1232, 281, 264, 1908, 13, 407, 439, 295, 291, 393, 767, 3847, 293, 1322, 51788], 'temperature': 0.0, 'avg_logprob': -0.10844936557844573, 'compression_ratio': 1.7204301075268817, 'no_speech_prob': 0.0060876174829900265}, {'id': 39, 'seek': 25340, 'start': 253.4, 'end': 258.36, 'text': ' the future of autonomy and self-driving cars. And of course, it goes far beyond this as well.', 'tokens': [50364, 264, 2027, 295, 27278, 293, 2698, 12, 47094, 5163, 13, 400, 295, 1164, 11, 309, 1709, 1400, 4399, 341, 382, 731, 13, 50612], 'temperature': 0.0, 'avg_logprob': -0.07273636307827262, 'compression_ratio': 1.6828193832599119, 'no_speech_prob': 0.0005970129859633744}, {'id': 40, 'seek': 25340, 'start': 258.36, 'end': 264.2, 'text': ' Deep learning can be used to generate content directly from how we speak and the language that', 'tokens': [50612, 14895, 2539, 393, 312, 1143, 281, 8460, 2701, 3838, 490, 577, 321, 1710, 293, 264, 2856, 300, 50904], 'temperature': 0.0, 'avg_logprob': -0.07273636307827262, 'compression_ratio': 1.6828193832599119, 'no_speech_prob': 0.0005970129859633744}, {'id': 41, 'seek': 25340, 'start': 264.2, 'end': 270.12, 'text': ' we convey to it from prompts that we say. Deep learning can reason about the prompts in natural', 'tokens': [50904, 321, 16965, 281, 309, 490, 41095, 300, 321, 584, 13, 14895, 2539, 393, 1778, 466, 264, 41095, 294, 3303, 51200], 'temperature': 0.0, 'avg_logprob': -0.07273636307827262, 'compression_ratio': 1.6828193832599119, 'no_speech_prob': 0.0005970129859633744}, {'id': 42, 'seek': 25340, 'start': 270.12, 'end': 276.52, 'text': ' language and English, for example, and then guide and control what is generated according to what', 'tokens': [51200, 2856, 293, 3669, 11, 337, 1365, 11, 293, 550, 5934, 293, 1969, 437, 307, 10833, 4650, 281, 437, 51520], 'temperature': 0.0, 'avg_logprob': -0.07273636307827262, 'compression_ratio': 1.6828193832599119, 'no_speech_prob': 0.0005970129859633744}, {'id': 43, 'seek': 27652, 'start': 276.52, 'end': 283.24, 'text': \" we specify. We've seen examples of where we can generate, for example, things that, again,\", 'tokens': [50364, 321, 16500, 13, 492, 600, 1612, 5110, 295, 689, 321, 393, 8460, 11, 337, 1365, 11, 721, 300, 11, 797, 11, 50700], 'temperature': 0.0, 'avg_logprob': -0.08812428364711525, 'compression_ratio': 1.8143939393939394, 'no_speech_prob': 0.004749222658574581}, {'id': 44, 'seek': 27652, 'start': 283.24, 'end': 288.28, 'text': ' have never existed in reality. We can ask a neural network to generate a photo of an astronaut', 'tokens': [50700, 362, 1128, 13135, 294, 4103, 13, 492, 393, 1029, 257, 18161, 3209, 281, 8460, 257, 5052, 295, 364, 18516, 50952], 'temperature': 0.0, 'avg_logprob': -0.08812428364711525, 'compression_ratio': 1.8143939393939394, 'no_speech_prob': 0.004749222658574581}, {'id': 45, 'seek': 27652, 'start': 288.28, 'end': 294.44, 'text': ' writing a horse. And it actually can imagine, hallucinate what this might look like even though,', 'tokens': [50952, 3579, 257, 6832, 13, 400, 309, 767, 393, 3811, 11, 35212, 13923, 437, 341, 1062, 574, 411, 754, 1673, 11, 51260], 'temperature': 0.0, 'avg_logprob': -0.08812428364711525, 'compression_ratio': 1.8143939393939394, 'no_speech_prob': 0.004749222658574581}, {'id': 46, 'seek': 27652, 'start': 294.44, 'end': 299.32, 'text': \" of course, this photo, not only this photo has never occurred before, but I don't think any photo\", 'tokens': [51260, 295, 1164, 11, 341, 5052, 11, 406, 787, 341, 5052, 575, 1128, 11068, 949, 11, 457, 286, 500, 380, 519, 604, 5052, 51504], 'temperature': 0.0, 'avg_logprob': -0.08812428364711525, 'compression_ratio': 1.8143939393939394, 'no_speech_prob': 0.004749222658574581}, {'id': 47, 'seek': 27652, 'start': 299.32, 'end': 303.56, 'text': \" of an astronaut writing a horse has ever occurred before. So there's not really even training data\", 'tokens': [51504, 295, 364, 18516, 3579, 257, 6832, 575, 1562, 11068, 949, 13, 407, 456, 311, 406, 534, 754, 3097, 1412, 51716], 'temperature': 0.0, 'avg_logprob': -0.08812428364711525, 'compression_ratio': 1.8143939393939394, 'no_speech_prob': 0.004749222658574581}, {'id': 48, 'seek': 30356, 'start': 303.56, 'end': 308.2, 'text': ' that you could go off in this case. And my personal favorite is actually how we can not only build', 'tokens': [50364, 300, 291, 727, 352, 766, 294, 341, 1389, 13, 400, 452, 2973, 2954, 307, 767, 577, 321, 393, 406, 787, 1322, 50596], 'temperature': 0.0, 'avg_logprob': -0.08099890200891227, 'compression_ratio': 1.8488372093023255, 'no_speech_prob': 0.009255751967430115}, {'id': 49, 'seek': 30356, 'start': 308.2, 'end': 314.12, 'text': ' software that can generate images and videos, but build software that can generate software', 'tokens': [50596, 4722, 300, 393, 8460, 5267, 293, 2145, 11, 457, 1322, 4722, 300, 393, 8460, 4722, 50892], 'temperature': 0.0, 'avg_logprob': -0.08099890200891227, 'compression_ratio': 1.8488372093023255, 'no_speech_prob': 0.009255751967430115}, {'id': 50, 'seek': 30356, 'start': 314.12, 'end': 319.24, 'text': ' as well. We can also have algorithms that can take language prompts, for example, a prompt like', 'tokens': [50892, 382, 731, 13, 492, 393, 611, 362, 14642, 300, 393, 747, 2856, 41095, 11, 337, 1365, 11, 257, 12391, 411, 51148], 'temperature': 0.0, 'avg_logprob': -0.08099890200891227, 'compression_ratio': 1.8488372093023255, 'no_speech_prob': 0.009255751967430115}, {'id': 51, 'seek': 30356, 'start': 319.24, 'end': 325.08, 'text': ' this, write code and TensorFlow to generate or to train a neural network. And not only will it', 'tokens': [51148, 341, 11, 2464, 3089, 293, 37624, 281, 8460, 420, 281, 3847, 257, 18161, 3209, 13, 400, 406, 787, 486, 309, 51440], 'temperature': 0.0, 'avg_logprob': -0.08099890200891227, 'compression_ratio': 1.8488372093023255, 'no_speech_prob': 0.009255751967430115}, {'id': 52, 'seek': 30356, 'start': 325.08, 'end': 331.32, 'text': ' write the code and create that neural network, but it will have the ability to reason about the', 'tokens': [51440, 2464, 264, 3089, 293, 1884, 300, 18161, 3209, 11, 457, 309, 486, 362, 264, 3485, 281, 1778, 466, 264, 51752], 'temperature': 0.0, 'avg_logprob': -0.08099890200891227, 'compression_ratio': 1.8488372093023255, 'no_speech_prob': 0.009255751967430115}, {'id': 53, 'seek': 33132, 'start': 331.32, 'end': 335.96, 'text': \" code that it's generated and walk you through. Step by step, explaining the process and procedure\", 'tokens': [50364, 3089, 300, 309, 311, 10833, 293, 1792, 291, 807, 13, 5470, 538, 1823, 11, 13468, 264, 1399, 293, 10747, 50596], 'temperature': 0.0, 'avg_logprob': -0.0881529466821513, 'compression_ratio': 1.6701388888888888, 'no_speech_prob': 0.0010478495387360454}, {'id': 54, 'seek': 33132, 'start': 335.96, 'end': 340.68, 'text': ' all the way from the ground up to you so that you can actually learn how to do this process as well.', 'tokens': [50596, 439, 264, 636, 490, 264, 2727, 493, 281, 291, 370, 300, 291, 393, 767, 1466, 577, 281, 360, 341, 1399, 382, 731, 13, 50832], 'temperature': 0.0, 'avg_logprob': -0.0881529466821513, 'compression_ratio': 1.6701388888888888, 'no_speech_prob': 0.0010478495387360454}, {'id': 55, 'seek': 33132, 'start': 341.56, 'end': 347.0, 'text': ' Now, I think some of these examples really just highlight how far deep learning and these', 'tokens': [50876, 823, 11, 286, 519, 512, 295, 613, 5110, 534, 445, 5078, 577, 1400, 2452, 2539, 293, 613, 51148], 'temperature': 0.0, 'avg_logprob': -0.0881529466821513, 'compression_ratio': 1.6701388888888888, 'no_speech_prob': 0.0010478495387360454}, {'id': 56, 'seek': 33132, 'start': 347.0, 'end': 351.88, 'text': ' methods have come in the past six years since we started this course. And you saw that example', 'tokens': [51148, 7150, 362, 808, 294, 264, 1791, 2309, 924, 1670, 321, 1409, 341, 1164, 13, 400, 291, 1866, 300, 1365, 51392], 'temperature': 0.0, 'avg_logprob': -0.0881529466821513, 'compression_ratio': 1.6701388888888888, 'no_speech_prob': 0.0010478495387360454}, {'id': 57, 'seek': 33132, 'start': 351.88, 'end': 356.52, 'text': \" just a few years ago from that introductory video. But now we're seeing such incredible advances.\", 'tokens': [51392, 445, 257, 1326, 924, 2057, 490, 300, 39048, 960, 13, 583, 586, 321, 434, 2577, 1270, 4651, 25297, 13, 51624], 'temperature': 0.0, 'avg_logprob': -0.0881529466821513, 'compression_ratio': 1.6701388888888888, 'no_speech_prob': 0.0010478495387360454}, {'id': 58, 'seek': 35652, 'start': 356.91999999999996, 'end': 361.64, 'text': ' The most amazing part of this course, in my opinion, is actually that within this one week,', 'tokens': [50384, 440, 881, 2243, 644, 295, 341, 1164, 11, 294, 452, 4800, 11, 307, 767, 300, 1951, 341, 472, 1243, 11, 50620], 'temperature': 0.0, 'avg_logprob': -0.1354101527820934, 'compression_ratio': 1.7062937062937062, 'no_speech_prob': 0.01792885921895504}, {'id': 59, 'seek': 35652, 'start': 361.64, 'end': 366.91999999999996, 'text': \" we're going to take you through from the ground up, starting from today, all of the foundational\", 'tokens': [50620, 321, 434, 516, 281, 747, 291, 807, 490, 264, 2727, 493, 11, 2891, 490, 965, 11, 439, 295, 264, 32195, 50884], 'temperature': 0.0, 'avg_logprob': -0.1354101527820934, 'compression_ratio': 1.7062937062937062, 'no_speech_prob': 0.01792885921895504}, {'id': 60, 'seek': 35652, 'start': 366.91999999999996, 'end': 372.52, 'text': ' building blocks that will allow you to understand and make all of this amazing advances possible.', 'tokens': [50884, 2390, 8474, 300, 486, 2089, 291, 281, 1223, 293, 652, 439, 295, 341, 2243, 25297, 1944, 13, 51164], 'temperature': 0.0, 'avg_logprob': -0.1354101527820934, 'compression_ratio': 1.7062937062937062, 'no_speech_prob': 0.01792885921895504}, {'id': 61, 'seek': 35652, 'start': 373.71999999999997, 'end': 379.32, 'text': \" So with that, hopefully now you're all super excited about what this class will teach. And I want to\", 'tokens': [51224, 407, 365, 300, 11, 4696, 586, 291, 434, 439, 1687, 2919, 466, 437, 341, 1508, 486, 2924, 13, 400, 286, 528, 281, 51504], 'temperature': 0.0, 'avg_logprob': -0.1354101527820934, 'compression_ratio': 1.7062937062937062, 'no_speech_prob': 0.01792885921895504}, {'id': 62, 'seek': 35652, 'start': 379.32, 'end': 385.08, 'text': \" basically now just start by taking a step back and introducing some of these terminologies that I've\", 'tokens': [51504, 1936, 586, 445, 722, 538, 1940, 257, 1823, 646, 293, 15424, 512, 295, 613, 10761, 6204, 300, 286, 600, 51792], 'temperature': 0.0, 'avg_logprob': -0.1354101527820934, 'compression_ratio': 1.7062937062937062, 'no_speech_prob': 0.01792885921895504}, {'id': 63, 'seek': 38508, 'start': 385.08, 'end': 389.15999999999997, 'text': ' kind of been throwing around so far, the deep learning, artificial intelligence, what do these', 'tokens': [50364, 733, 295, 668, 10238, 926, 370, 1400, 11, 264, 2452, 2539, 11, 11677, 7599, 11, 437, 360, 613, 50568], 'temperature': 0.0, 'avg_logprob': -0.09727614907657399, 'compression_ratio': 1.6724890829694323, 'no_speech_prob': 0.00016338574641849846}, {'id': 64, 'seek': 38508, 'start': 389.15999999999997, 'end': 396.03999999999996, 'text': ' things actually mean? So first of all, I want to maybe just take a second to speak a little bit', 'tokens': [50568, 721, 767, 914, 30, 407, 700, 295, 439, 11, 286, 528, 281, 1310, 445, 747, 257, 1150, 281, 1710, 257, 707, 857, 50912], 'temperature': 0.0, 'avg_logprob': -0.09727614907657399, 'compression_ratio': 1.6724890829694323, 'no_speech_prob': 0.00016338574641849846}, {'id': 65, 'seek': 38508, 'start': 396.03999999999996, 'end': 402.2, 'text': ' about intelligence and what intelligence means at its core. So to me, intelligence is simply the', 'tokens': [50912, 466, 7599, 293, 437, 7599, 1355, 412, 1080, 4965, 13, 407, 281, 385, 11, 7599, 307, 2935, 264, 51220], 'temperature': 0.0, 'avg_logprob': -0.09727614907657399, 'compression_ratio': 1.6724890829694323, 'no_speech_prob': 0.00016338574641849846}, {'id': 66, 'seek': 38508, 'start': 402.2, 'end': 408.44, 'text': ' ability to process information such that we can use it to inform some future decision or action', 'tokens': [51220, 3485, 281, 1399, 1589, 1270, 300, 321, 393, 764, 309, 281, 1356, 512, 2027, 3537, 420, 3069, 51532], 'temperature': 0.0, 'avg_logprob': -0.09727614907657399, 'compression_ratio': 1.6724890829694323, 'no_speech_prob': 0.00016338574641849846}, {'id': 67, 'seek': 40844, 'start': 408.44, 'end': 414.6, 'text': ' that we take. Now, the field of artificial intelligence is simply the ability for us to build', 'tokens': [50364, 300, 321, 747, 13, 823, 11, 264, 2519, 295, 11677, 7599, 307, 2935, 264, 3485, 337, 505, 281, 1322, 50672], 'temperature': 0.0, 'avg_logprob': -0.09689633051554362, 'compression_ratio': 1.7130044843049328, 'no_speech_prob': 0.0007318749558180571}, {'id': 68, 'seek': 40844, 'start': 414.6, 'end': 419.96, 'text': ' algorithms, artificial algorithms that can do exactly this, process information to inform some', 'tokens': [50672, 14642, 11, 11677, 14642, 300, 393, 360, 2293, 341, 11, 1399, 1589, 281, 1356, 512, 50940], 'temperature': 0.0, 'avg_logprob': -0.09689633051554362, 'compression_ratio': 1.7130044843049328, 'no_speech_prob': 0.0007318749558180571}, {'id': 69, 'seek': 40844, 'start': 419.96, 'end': 425.88, 'text': ' future decision. Now, machine learning is simply a subset of AI, which focuses specifically on how', 'tokens': [50940, 2027, 3537, 13, 823, 11, 3479, 2539, 307, 2935, 257, 25993, 295, 7318, 11, 597, 16109, 4682, 322, 577, 51236], 'temperature': 0.0, 'avg_logprob': -0.09689633051554362, 'compression_ratio': 1.7130044843049328, 'no_speech_prob': 0.0007318749558180571}, {'id': 70, 'seek': 40844, 'start': 425.88, 'end': 433.8, 'text': ' we can build a machine to or teach a machine how to do this from some experiences or data, for', 'tokens': [51236, 321, 393, 1322, 257, 3479, 281, 420, 2924, 257, 3479, 577, 281, 360, 341, 490, 512, 5235, 420, 1412, 11, 337, 51632], 'temperature': 0.0, 'avg_logprob': -0.09689633051554362, 'compression_ratio': 1.7130044843049328, 'no_speech_prob': 0.0007318749558180571}, {'id': 71, 'seek': 43380, 'start': 433.8, 'end': 439.08, 'text': ' example. Now, deep learning goes one step beyond this and is a subset of machine learning, which', 'tokens': [50364, 1365, 13, 823, 11, 2452, 2539, 1709, 472, 1823, 4399, 341, 293, 307, 257, 25993, 295, 3479, 2539, 11, 597, 50628], 'temperature': 0.0, 'avg_logprob': -0.0599158763885498, 'compression_ratio': 1.7647058823529411, 'no_speech_prob': 0.0006163951475173235}, {'id': 72, 'seek': 43380, 'start': 439.08, 'end': 443.24, 'text': ' focuses explicitly on what are called neural networks and how we can build neural networks that', 'tokens': [50628, 16109, 20803, 322, 437, 366, 1219, 18161, 9590, 293, 577, 321, 393, 1322, 18161, 9590, 300, 50836], 'temperature': 0.0, 'avg_logprob': -0.0599158763885498, 'compression_ratio': 1.7647058823529411, 'no_speech_prob': 0.0006163951475173235}, {'id': 73, 'seek': 43380, 'start': 443.24, 'end': 447.48, 'text': ' can extract features in the data. These are basically what you can think of as patterns that', 'tokens': [50836, 393, 8947, 4122, 294, 264, 1412, 13, 1981, 366, 1936, 437, 291, 393, 519, 295, 382, 8294, 300, 51048], 'temperature': 0.0, 'avg_logprob': -0.0599158763885498, 'compression_ratio': 1.7647058823529411, 'no_speech_prob': 0.0006163951475173235}, {'id': 74, 'seek': 43380, 'start': 447.48, 'end': 454.12, 'text': \" occur within the data so that it can learn to complete these tasks as well. Now, that's exactly what\", 'tokens': [51048, 5160, 1951, 264, 1412, 370, 300, 309, 393, 1466, 281, 3566, 613, 9608, 382, 731, 13, 823, 11, 300, 311, 2293, 437, 51380], 'temperature': 0.0, 'avg_logprob': -0.0599158763885498, 'compression_ratio': 1.7647058823529411, 'no_speech_prob': 0.0006163951475173235}, {'id': 75, 'seek': 43380, 'start': 454.12, 'end': 458.28000000000003, 'text': \" this class is really all about at its core. We're going to try and teach you and give you the\", 'tokens': [51380, 341, 1508, 307, 534, 439, 466, 412, 1080, 4965, 13, 492, 434, 516, 281, 853, 293, 2924, 291, 293, 976, 291, 264, 51588], 'temperature': 0.0, 'avg_logprob': -0.0599158763885498, 'compression_ratio': 1.7647058823529411, 'no_speech_prob': 0.0006163951475173235}, {'id': 76, 'seek': 45828, 'start': 458.28, 'end': 464.91999999999996, 'text': ' foundational understanding and how we can build and teach computers to learn tasks, many different', 'tokens': [50364, 32195, 3701, 293, 577, 321, 393, 1322, 293, 2924, 10807, 281, 1466, 9608, 11, 867, 819, 50696], 'temperature': 0.0, 'avg_logprob': -0.08930349804106213, 'compression_ratio': 1.7077464788732395, 'no_speech_prob': 0.00017124900477938354}, {'id': 77, 'seek': 45828, 'start': 464.91999999999996, 'end': 470.35999999999996, 'text': \" types of tasks directly from raw data. And that's really what this class boils down to at its\", 'tokens': [50696, 3467, 295, 9608, 3838, 490, 8936, 1412, 13, 400, 300, 311, 534, 437, 341, 1508, 35049, 760, 281, 412, 1080, 50968], 'temperature': 0.0, 'avg_logprob': -0.08930349804106213, 'compression_ratio': 1.7077464788732395, 'no_speech_prob': 0.00017124900477938354}, {'id': 78, 'seek': 45828, 'start': 470.35999999999996, 'end': 476.44, 'text': \" most simple form. And we'll provide a very solid foundation for you both on the technical side\", 'tokens': [50968, 881, 2199, 1254, 13, 400, 321, 603, 2893, 257, 588, 5100, 7030, 337, 291, 1293, 322, 264, 6191, 1252, 51272], 'temperature': 0.0, 'avg_logprob': -0.08930349804106213, 'compression_ratio': 1.7077464788732395, 'no_speech_prob': 0.00017124900477938354}, {'id': 79, 'seek': 45828, 'start': 476.44, 'end': 480.91999999999996, 'text': ' through the lectures, which will happen in two parts throughout the class, the first lecture and', 'tokens': [51272, 807, 264, 16564, 11, 597, 486, 1051, 294, 732, 3166, 3710, 264, 1508, 11, 264, 700, 7991, 293, 51496], 'temperature': 0.0, 'avg_logprob': -0.08930349804106213, 'compression_ratio': 1.7077464788732395, 'no_speech_prob': 0.00017124900477938354}, {'id': 80, 'seek': 45828, 'start': 480.91999999999996, 'end': 485.79999999999995, 'text': ' the second lecture, each one about one hour long, followed by a software lab, which will immediately', 'tokens': [51496, 264, 1150, 7991, 11, 1184, 472, 466, 472, 1773, 938, 11, 6263, 538, 257, 4722, 2715, 11, 597, 486, 4258, 51740], 'temperature': 0.0, 'avg_logprob': -0.08930349804106213, 'compression_ratio': 1.7077464788732395, 'no_speech_prob': 0.00017124900477938354}, {'id': 81, 'seek': 48580, 'start': 485.8, 'end': 491.56, 'text': ' follow the lectures, which will try to reinforce a lot of what we cover in the technical part of', 'tokens': [50364, 1524, 264, 16564, 11, 597, 486, 853, 281, 22634, 257, 688, 295, 437, 321, 2060, 294, 264, 6191, 644, 295, 50652], 'temperature': 0.0, 'avg_logprob': -0.0960556377064098, 'compression_ratio': 1.726643598615917, 'no_speech_prob': 0.00046497173025272787}, {'id': 82, 'seek': 48580, 'start': 491.56, 'end': 498.6, 'text': ' the class and give you hands-on experience implementing those ideas. So this program is split between', 'tokens': [50652, 264, 1508, 293, 976, 291, 2377, 12, 266, 1752, 18114, 729, 3487, 13, 407, 341, 1461, 307, 7472, 1296, 51004], 'temperature': 0.0, 'avg_logprob': -0.0960556377064098, 'compression_ratio': 1.726643598615917, 'no_speech_prob': 0.00046497173025272787}, {'id': 83, 'seek': 48580, 'start': 498.6, 'end': 503.16, 'text': ' these two pieces, the technical lectures and the software labs. We have several new updates this', 'tokens': [51004, 613, 732, 3755, 11, 264, 6191, 16564, 293, 264, 4722, 20339, 13, 492, 362, 2940, 777, 9205, 341, 51232], 'temperature': 0.0, 'avg_logprob': -0.0960556377064098, 'compression_ratio': 1.726643598615917, 'no_speech_prob': 0.00046497173025272787}, {'id': 84, 'seek': 48580, 'start': 503.16, 'end': 509.0, 'text': ' year in specific, especially in many of the later lectures. The first lecture will cover the foundations', 'tokens': [51232, 1064, 294, 2685, 11, 2318, 294, 867, 295, 264, 1780, 16564, 13, 440, 700, 7991, 486, 2060, 264, 22467, 51524], 'temperature': 0.0, 'avg_logprob': -0.0960556377064098, 'compression_ratio': 1.726643598615917, 'no_speech_prob': 0.00046497173025272787}, {'id': 85, 'seek': 48580, 'start': 509.0, 'end': 514.6800000000001, 'text': \" of deep learning, which is going to be right now. And finally, we'll conclude the course with some\", 'tokens': [51524, 295, 2452, 2539, 11, 597, 307, 516, 281, 312, 558, 586, 13, 400, 2721, 11, 321, 603, 16886, 264, 1164, 365, 512, 51808], 'temperature': 0.0, 'avg_logprob': -0.0960556377064098, 'compression_ratio': 1.726643598615917, 'no_speech_prob': 0.00046497173025272787}, {'id': 86, 'seek': 51468, 'start': 514.68, 'end': 520.28, 'text': ' very exciting guest lectures from both academia and industry who are really leading and driving', 'tokens': [50364, 588, 4670, 8341, 16564, 490, 1293, 28937, 293, 3518, 567, 366, 534, 5775, 293, 4840, 50644], 'temperature': 0.0, 'avg_logprob': -0.12208969221202605, 'compression_ratio': 1.6734693877551021, 'no_speech_prob': 0.0006960933678783476}, {'id': 87, 'seek': 51468, 'start': 520.28, 'end': 526.28, 'text': ' forward the state of AI and deep learning. And of course, we have many awesome prizes that go with', 'tokens': [50644, 2128, 264, 1785, 295, 7318, 293, 2452, 2539, 13, 400, 295, 1164, 11, 321, 362, 867, 3476, 27350, 300, 352, 365, 50944], 'temperature': 0.0, 'avg_logprob': -0.12208969221202605, 'compression_ratio': 1.6734693877551021, 'no_speech_prob': 0.0006960933678783476}, {'id': 88, 'seek': 51468, 'start': 527.3199999999999, 'end': 532.76, 'text': ' all of the software labs and the project competition at the end of the course. So maybe quickly to go', 'tokens': [50996, 439, 295, 264, 4722, 20339, 293, 264, 1716, 6211, 412, 264, 917, 295, 264, 1164, 13, 407, 1310, 2661, 281, 352, 51268], 'temperature': 0.0, 'avg_logprob': -0.12208969221202605, 'compression_ratio': 1.6734693877551021, 'no_speech_prob': 0.0006960933678783476}, {'id': 89, 'seek': 51468, 'start': 532.76, 'end': 537.0799999999999, 'text': \" through these each day, like I said, we'll have dedicated software labs that couple with the lectures.\", 'tokens': [51268, 807, 613, 1184, 786, 11, 411, 286, 848, 11, 321, 603, 362, 8374, 4722, 20339, 300, 1916, 365, 264, 16564, 13, 51484], 'temperature': 0.0, 'avg_logprob': -0.12208969221202605, 'compression_ratio': 1.6734693877551021, 'no_speech_prob': 0.0006960933678783476}, {'id': 90, 'seek': 51468, 'start': 538.28, 'end': 543.0799999999999, 'text': \" Starting today with Lab 1, you'll actually build a neural network, keeping with the theme of\", 'tokens': [51544, 16217, 965, 365, 10137, 502, 11, 291, 603, 767, 1322, 257, 18161, 3209, 11, 5145, 365, 264, 6314, 295, 51784], 'temperature': 0.0, 'avg_logprob': -0.12208969221202605, 'compression_ratio': 1.6734693877551021, 'no_speech_prob': 0.0006960933678783476}, {'id': 91, 'seek': 54308, 'start': 543.08, 'end': 548.12, 'text': \" generative AI, you'll build a neural network that can learn, listen to a lot of music and actually\", 'tokens': [50364, 1337, 1166, 7318, 11, 291, 603, 1322, 257, 18161, 3209, 300, 393, 1466, 11, 2140, 281, 257, 688, 295, 1318, 293, 767, 50616], 'temperature': 0.0, 'avg_logprob': -0.09207787116368611, 'compression_ratio': 1.6265560165975104, 'no_speech_prob': 0.0009971457766368985}, {'id': 92, 'seek': 54308, 'start': 548.12, 'end': 554.6800000000001, 'text': ' learn how to generate brand new songs in that genre of music. At the end, at the next level of the', 'tokens': [50616, 1466, 577, 281, 8460, 3360, 777, 5781, 294, 300, 11022, 295, 1318, 13, 1711, 264, 917, 11, 412, 264, 958, 1496, 295, 264, 50944], 'temperature': 0.0, 'avg_logprob': -0.09207787116368611, 'compression_ratio': 1.6265560165975104, 'no_speech_prob': 0.0009971457766368985}, {'id': 93, 'seek': 54308, 'start': 554.6800000000001, 'end': 560.2, 'text': \" class on Friday, we'll host a project pitch competition where either you individually or as part of\", 'tokens': [50944, 1508, 322, 6984, 11, 321, 603, 3975, 257, 1716, 7293, 6211, 689, 2139, 291, 16652, 420, 382, 644, 295, 51220], 'temperature': 0.0, 'avg_logprob': -0.09207787116368611, 'compression_ratio': 1.6265560165975104, 'no_speech_prob': 0.0009971457766368985}, {'id': 94, 'seek': 54308, 'start': 560.2, 'end': 567.72, 'text': \" a group can participate and present an idea, a novel deep learning idea to all of us. It'll be\", 'tokens': [51220, 257, 1594, 393, 8197, 293, 1974, 364, 1558, 11, 257, 7613, 2452, 2539, 1558, 281, 439, 295, 505, 13, 467, 603, 312, 51596], 'temperature': 0.0, 'avg_logprob': -0.09207787116368611, 'compression_ratio': 1.6265560165975104, 'no_speech_prob': 0.0009971457766368985}, {'id': 95, 'seek': 56772, 'start': 567.72, 'end': 574.52, 'text': ' roughly three minutes in length. And we will focus not as much because this is a one week program.', 'tokens': [50364, 9810, 1045, 2077, 294, 4641, 13, 400, 321, 486, 1879, 406, 382, 709, 570, 341, 307, 257, 472, 1243, 1461, 13, 50704], 'temperature': 0.0, 'avg_logprob': -0.11240362906241202, 'compression_ratio': 1.6296296296296295, 'no_speech_prob': 0.0009387017344124615}, {'id': 96, 'seek': 56772, 'start': 574.52, 'end': 579.08, 'text': ' We are not going to focus so much on the results of your pitch, but rather the invasion and the idea', 'tokens': [50704, 492, 366, 406, 516, 281, 1879, 370, 709, 322, 264, 3542, 295, 428, 7293, 11, 457, 2831, 264, 21575, 293, 264, 1558, 50932], 'temperature': 0.0, 'avg_logprob': -0.11240362906241202, 'compression_ratio': 1.6296296296296295, 'no_speech_prob': 0.0009387017344124615}, {'id': 97, 'seek': 56772, 'start': 579.08, 'end': 584.44, 'text': \" and the novelty of what you're trying to propose. The prizes here are quite significant already.\", 'tokens': [50932, 293, 264, 44805, 295, 437, 291, 434, 1382, 281, 17421, 13, 440, 27350, 510, 366, 1596, 4776, 1217, 13, 51200], 'temperature': 0.0, 'avg_logprob': -0.11240362906241202, 'compression_ratio': 1.6296296296296295, 'no_speech_prob': 0.0009387017344124615}, {'id': 98, 'seek': 56772, 'start': 584.44, 'end': 589.88, 'text': ' Where first prize is going to get an Nvidia GPU, which is really a key piece of hardware that is', 'tokens': [51200, 2305, 700, 12818, 307, 516, 281, 483, 364, 46284, 18407, 11, 597, 307, 534, 257, 2141, 2522, 295, 8837, 300, 307, 51472], 'temperature': 0.0, 'avg_logprob': -0.11240362906241202, 'compression_ratio': 1.6296296296296295, 'no_speech_prob': 0.0009387017344124615}, {'id': 99, 'seek': 56772, 'start': 589.88, 'end': 594.0400000000001, 'text': ' instrumental. If you want to actually build a deep learning project and train these neural', 'tokens': [51472, 17388, 13, 759, 291, 528, 281, 767, 1322, 257, 2452, 2539, 1716, 293, 3847, 613, 18161, 51680], 'temperature': 0.0, 'avg_logprob': -0.11240362906241202, 'compression_ratio': 1.6296296296296295, 'no_speech_prob': 0.0009387017344124615}, {'id': 100, 'seek': 59404, 'start': 594.04, 'end': 598.36, 'text': ' networks, which can be very large and require a lot of compute, these prizes will give you the compute', 'tokens': [50364, 9590, 11, 597, 393, 312, 588, 2416, 293, 3651, 257, 688, 295, 14722, 11, 613, 27350, 486, 976, 291, 264, 14722, 50580], 'temperature': 0.0, 'avg_logprob': -0.10590675898960658, 'compression_ratio': 1.667785234899329, 'no_speech_prob': 0.01381796132773161}, {'id': 101, 'seek': 59404, 'start': 598.36, 'end': 604.28, 'text': \" to do so. And finally, this year we'll be awarding a grand prize for labs two and three combined,\", 'tokens': [50580, 281, 360, 370, 13, 400, 2721, 11, 341, 1064, 321, 603, 312, 7130, 278, 257, 2697, 12818, 337, 20339, 732, 293, 1045, 9354, 11, 50876], 'temperature': 0.0, 'avg_logprob': -0.10590675898960658, 'compression_ratio': 1.667785234899329, 'no_speech_prob': 0.01381796132773161}, {'id': 102, 'seek': 59404, 'start': 604.28, 'end': 608.92, 'text': ' which will occur on Tuesday and Wednesday, focused on what I believe is actually solving some of the', 'tokens': [50876, 597, 486, 5160, 322, 10017, 293, 10579, 11, 5178, 322, 437, 286, 1697, 307, 767, 12606, 512, 295, 264, 51108], 'temperature': 0.0, 'avg_logprob': -0.10590675898960658, 'compression_ratio': 1.667785234899329, 'no_speech_prob': 0.01381796132773161}, {'id': 103, 'seek': 59404, 'start': 608.92, 'end': 615.0, 'text': ' most exciting problems in this field of deep learning and how specifically how we can build models', 'tokens': [51108, 881, 4670, 2740, 294, 341, 2519, 295, 2452, 2539, 293, 577, 4682, 577, 321, 393, 1322, 5245, 51412], 'temperature': 0.0, 'avg_logprob': -0.10590675898960658, 'compression_ratio': 1.667785234899329, 'no_speech_prob': 0.01381796132773161}, {'id': 104, 'seek': 59404, 'start': 615.0, 'end': 621.16, 'text': \" that can be robust, not only accurate, but robust and trustworthy and safe when they're deployed\", 'tokens': [51412, 300, 393, 312, 13956, 11, 406, 787, 8559, 11, 457, 13956, 293, 39714, 293, 3273, 562, 436, 434, 17826, 51720], 'temperature': 0.0, 'avg_logprob': -0.10590675898960658, 'compression_ratio': 1.667785234899329, 'no_speech_prob': 0.01381796132773161}, {'id': 105, 'seek': 62116, 'start': 621.24, 'end': 625.7199999999999, 'text': \" as well. And you'll actually get experience developing those types of solutions that can actually\", 'tokens': [50368, 382, 731, 13, 400, 291, 603, 767, 483, 1752, 6416, 729, 3467, 295, 6547, 300, 393, 767, 50592], 'temperature': 0.0, 'avg_logprob': -0.12320238968421673, 'compression_ratio': 1.698961937716263, 'no_speech_prob': 0.008174169808626175}, {'id': 106, 'seek': 62116, 'start': 625.7199999999999, 'end': 632.12, 'text': ' advance the state of the art and AI. Now, all of these labs that I mentioned and competitions here', 'tokens': [50592, 7295, 264, 1785, 295, 264, 1523, 293, 7318, 13, 823, 11, 439, 295, 613, 20339, 300, 286, 2835, 293, 26185, 510, 50912], 'temperature': 0.0, 'avg_logprob': -0.12320238968421673, 'compression_ratio': 1.698961937716263, 'no_speech_prob': 0.008174169808626175}, {'id': 107, 'seek': 62116, 'start': 632.12, 'end': 638.76, 'text': \" are going to be due on Thursday night at 11 p.m. right before the last day of class. And we'll be\", 'tokens': [50912, 366, 516, 281, 312, 3462, 322, 10383, 1818, 412, 2975, 280, 13, 76, 13, 558, 949, 264, 1036, 786, 295, 1508, 13, 400, 321, 603, 312, 51244], 'temperature': 0.0, 'avg_logprob': -0.12320238968421673, 'compression_ratio': 1.698961937716263, 'no_speech_prob': 0.008174169808626175}, {'id': 108, 'seek': 62116, 'start': 638.76, 'end': 644.8399999999999, 'text': ' helping you all along the way. This prize or this competition in particular has very significant', 'tokens': [51244, 4315, 291, 439, 2051, 264, 636, 13, 639, 12818, 420, 341, 6211, 294, 1729, 575, 588, 4776, 51548], 'temperature': 0.0, 'avg_logprob': -0.12320238968421673, 'compression_ratio': 1.698961937716263, 'no_speech_prob': 0.008174169808626175}, {'id': 109, 'seek': 62116, 'start': 644.8399999999999, 'end': 650.52, 'text': ' prizes. So I encourage all of you to really enter this prize and try to try to give a chance to win', 'tokens': [51548, 27350, 13, 407, 286, 5373, 439, 295, 291, 281, 534, 3242, 341, 12818, 293, 853, 281, 853, 281, 976, 257, 2931, 281, 1942, 51832], 'temperature': 0.0, 'avg_logprob': -0.12320238968421673, 'compression_ratio': 1.698961937716263, 'no_speech_prob': 0.008174169808626175}, {'id': 110, 'seek': 65052, 'start': 650.52, 'end': 655.56, 'text': \" the prize. And of course, like I said, we're going to be helping you all along the way who are\", 'tokens': [50364, 264, 12818, 13, 400, 295, 1164, 11, 411, 286, 848, 11, 321, 434, 516, 281, 312, 4315, 291, 439, 2051, 264, 636, 567, 366, 50616], 'temperature': 0.0, 'avg_logprob': -0.10341705259729604, 'compression_ratio': 1.7173144876325088, 'no_speech_prob': 0.001300527947023511}, {'id': 111, 'seek': 65052, 'start': 655.56, 'end': 661.3199999999999, 'text': ' many available resources throughout this class to help you achieve this. Please post a Piazza if', 'tokens': [50616, 867, 2435, 3593, 3710, 341, 1508, 281, 854, 291, 4584, 341, 13, 2555, 2183, 257, 430, 654, 26786, 498, 50904], 'temperature': 0.0, 'avg_logprob': -0.10341705259729604, 'compression_ratio': 1.7173144876325088, 'no_speech_prob': 0.001300527947023511}, {'id': 112, 'seek': 65052, 'start': 661.3199999999999, 'end': 666.6, 'text': ' you have any questions. And of course, this program has an incredible team that you can reach out to', 'tokens': [50904, 291, 362, 604, 1651, 13, 400, 295, 1164, 11, 341, 1461, 575, 364, 4651, 1469, 300, 291, 393, 2524, 484, 281, 51168], 'temperature': 0.0, 'avg_logprob': -0.10341705259729604, 'compression_ratio': 1.7173144876325088, 'no_speech_prob': 0.001300527947023511}, {'id': 113, 'seek': 65052, 'start': 666.6, 'end': 672.6, 'text': ' at any point in case you have any issues or questions on the materials. Myself and Ava will be your', 'tokens': [51168, 412, 604, 935, 294, 1389, 291, 362, 604, 2663, 420, 1651, 322, 264, 5319, 13, 37795, 1967, 293, 316, 2757, 486, 312, 428, 51468], 'temperature': 0.0, 'avg_logprob': -0.10341705259729604, 'compression_ratio': 1.7173144876325088, 'no_speech_prob': 0.001300527947023511}, {'id': 114, 'seek': 65052, 'start': 672.6, 'end': 677.3199999999999, 'text': \" two main lectures for the first part of the class. We'll also be hearing, like I said, in the\", 'tokens': [51468, 732, 2135, 16564, 337, 264, 700, 644, 295, 264, 1508, 13, 492, 603, 611, 312, 4763, 11, 411, 286, 848, 11, 294, 264, 51704], 'temperature': 0.0, 'avg_logprob': -0.10341705259729604, 'compression_ratio': 1.7173144876325088, 'no_speech_prob': 0.001300527947023511}, {'id': 115, 'seek': 67732, 'start': 677.32, 'end': 682.2800000000001, 'text': ' later part of the class from some guest lectures who will share some really cutting-edge state-of-the-art', 'tokens': [50364, 1780, 644, 295, 264, 1508, 490, 512, 8341, 16564, 567, 486, 2073, 512, 534, 6492, 12, 12203, 1785, 12, 2670, 12, 3322, 12, 446, 50612], 'temperature': 0.0, 'avg_logprob': -0.09914700446590301, 'compression_ratio': 1.6644518272425248, 'no_speech_prob': 0.0017688462976366282}, {'id': 116, 'seek': 67732, 'start': 682.2800000000001, 'end': 686.6800000000001, 'text': ' developments and deep learning. And of course, I want to give a huge shout out and thanks to all of', 'tokens': [50612, 20862, 293, 2452, 2539, 13, 400, 295, 1164, 11, 286, 528, 281, 976, 257, 2603, 8043, 484, 293, 3231, 281, 439, 295, 50832], 'temperature': 0.0, 'avg_logprob': -0.09914700446590301, 'compression_ratio': 1.6644518272425248, 'no_speech_prob': 0.0017688462976366282}, {'id': 117, 'seek': 67732, 'start': 686.6800000000001, 'end': 691.96, 'text': \" our sponsors who without their support, this program wouldn't have been possible for yet again\", 'tokens': [50832, 527, 22593, 567, 1553, 641, 1406, 11, 341, 1461, 2759, 380, 362, 668, 1944, 337, 1939, 797, 51096], 'temperature': 0.0, 'avg_logprob': -0.09914700446590301, 'compression_ratio': 1.6644518272425248, 'no_speech_prob': 0.0017688462976366282}, {'id': 118, 'seek': 67732, 'start': 691.96, 'end': 698.6, 'text': \" another year. So thank you all. Okay, so now with that, let's really dive into the really fun stuff\", 'tokens': [51096, 1071, 1064, 13, 407, 1309, 291, 439, 13, 1033, 11, 370, 586, 365, 300, 11, 718, 311, 534, 9192, 666, 264, 534, 1019, 1507, 51428], 'temperature': 0.0, 'avg_logprob': -0.09914700446590301, 'compression_ratio': 1.6644518272425248, 'no_speech_prob': 0.0017688462976366282}, {'id': 119, 'seek': 67732, 'start': 698.6, 'end': 704.5200000000001, 'text': \" of today's lecture, which is, you know, the technical part. And I think I want to start this part by\", 'tokens': [51428, 295, 965, 311, 7991, 11, 597, 307, 11, 291, 458, 11, 264, 6191, 644, 13, 400, 286, 519, 286, 528, 281, 722, 341, 644, 538, 51724], 'temperature': 0.0, 'avg_logprob': -0.09914700446590301, 'compression_ratio': 1.6644518272425248, 'no_speech_prob': 0.0017688462976366282}, {'id': 120, 'seek': 70452, 'start': 704.52, 'end': 710.12, 'text': ' asking all of you and having yourselves ask yourself, you know, having you ask yourselves this', 'tokens': [50364, 3365, 439, 295, 291, 293, 1419, 14791, 1029, 1803, 11, 291, 458, 11, 1419, 291, 1029, 14791, 341, 50644], 'temperature': 0.0, 'avg_logprob': -0.08213835103171212, 'compression_ratio': 2.1008771929824563, 'no_speech_prob': 0.0026249366346746683}, {'id': 121, 'seek': 70452, 'start': 710.12, 'end': 715.3199999999999, 'text': ' question of, you know, why are all of you here, first of all, why do you care about this topic', 'tokens': [50644, 1168, 295, 11, 291, 458, 11, 983, 366, 439, 295, 291, 510, 11, 700, 295, 439, 11, 983, 360, 291, 1127, 466, 341, 4829, 50904], 'temperature': 0.0, 'avg_logprob': -0.08213835103171212, 'compression_ratio': 2.1008771929824563, 'no_speech_prob': 0.0026249366346746683}, {'id': 122, 'seek': 70452, 'start': 715.3199999999999, 'end': 720.84, 'text': ' in the first place? Now, I think to answer this question, we have to take a step back and think', 'tokens': [50904, 294, 264, 700, 1081, 30, 823, 11, 286, 519, 281, 1867, 341, 1168, 11, 321, 362, 281, 747, 257, 1823, 646, 293, 519, 51180], 'temperature': 0.0, 'avg_logprob': -0.08213835103171212, 'compression_ratio': 2.1008771929824563, 'no_speech_prob': 0.0026249366346746683}, {'id': 123, 'seek': 70452, 'start': 720.84, 'end': 725.8, 'text': ' about, you know, the history of machine learning and what machine learning is and what deep learning', 'tokens': [51180, 466, 11, 291, 458, 11, 264, 2503, 295, 3479, 2539, 293, 437, 3479, 2539, 307, 293, 437, 2452, 2539, 51428], 'temperature': 0.0, 'avg_logprob': -0.08213835103171212, 'compression_ratio': 2.1008771929824563, 'no_speech_prob': 0.0026249366346746683}, {'id': 124, 'seek': 70452, 'start': 725.8, 'end': 730.84, 'text': ' brings to the table on top of machine learning. Now, traditional machine learning algorithms', 'tokens': [51428, 5607, 281, 264, 3199, 322, 1192, 295, 3479, 2539, 13, 823, 11, 5164, 3479, 2539, 14642, 51680], 'temperature': 0.0, 'avg_logprob': -0.08213835103171212, 'compression_ratio': 2.1008771929824563, 'no_speech_prob': 0.0026249366346746683}, {'id': 125, 'seek': 73084, 'start': 730.84, 'end': 735.5600000000001, 'text': ' typically define what are called these set of features in the data. You can think of these as', 'tokens': [50364, 5850, 6964, 437, 366, 1219, 613, 992, 295, 4122, 294, 264, 1412, 13, 509, 393, 519, 295, 613, 382, 50600], 'temperature': 0.0, 'avg_logprob': -0.07935115778557608, 'compression_ratio': 1.8467432950191571, 'no_speech_prob': 0.005816567223519087}, {'id': 126, 'seek': 73084, 'start': 735.5600000000001, 'end': 740.36, 'text': ' certain patterns in the data and usually these features are hand engineered. So probably a human', 'tokens': [50600, 1629, 8294, 294, 264, 1412, 293, 2673, 613, 4122, 366, 1011, 38648, 13, 407, 1391, 257, 1952, 50840], 'temperature': 0.0, 'avg_logprob': -0.07935115778557608, 'compression_ratio': 1.8467432950191571, 'no_speech_prob': 0.005816567223519087}, {'id': 127, 'seek': 73084, 'start': 740.36, 'end': 745.72, 'text': ' will come into the data set and with a lot of domain knowledge and experience can try to uncover', 'tokens': [50840, 486, 808, 666, 264, 1412, 992, 293, 365, 257, 688, 295, 9274, 3601, 293, 1752, 393, 853, 281, 21694, 51108], 'temperature': 0.0, 'avg_logprob': -0.07935115778557608, 'compression_ratio': 1.8467432950191571, 'no_speech_prob': 0.005816567223519087}, {'id': 128, 'seek': 73084, 'start': 745.72, 'end': 750.44, 'text': ' what these features might be. Now, the key idea of deep learning and this is really central to this', 'tokens': [51108, 437, 613, 4122, 1062, 312, 13, 823, 11, 264, 2141, 1558, 295, 2452, 2539, 293, 341, 307, 534, 5777, 281, 341, 51344], 'temperature': 0.0, 'avg_logprob': -0.07935115778557608, 'compression_ratio': 1.8467432950191571, 'no_speech_prob': 0.005816567223519087}, {'id': 129, 'seek': 73084, 'start': 750.44, 'end': 755.8000000000001, 'text': ' class is that instead of having a human define these features, what if we could have a machine', 'tokens': [51344, 1508, 307, 300, 2602, 295, 1419, 257, 1952, 6964, 613, 4122, 11, 437, 498, 321, 727, 362, 257, 3479, 51612], 'temperature': 0.0, 'avg_logprob': -0.07935115778557608, 'compression_ratio': 1.8467432950191571, 'no_speech_prob': 0.005816567223519087}, {'id': 130, 'seek': 75580, 'start': 756.3599999999999, 'end': 761.24, 'text': ' look at all of this data and actually try to extract and uncover what are the core patterns in', 'tokens': [50392, 574, 412, 439, 295, 341, 1412, 293, 767, 853, 281, 8947, 293, 21694, 437, 366, 264, 4965, 8294, 294, 50636], 'temperature': 0.0, 'avg_logprob': -0.08013797657830375, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.009848624467849731}, {'id': 131, 'seek': 75580, 'start': 761.24, 'end': 766.5999999999999, 'text': ' the data so that it can use those when it sees new data to make some decisions. So, for example,', 'tokens': [50636, 264, 1412, 370, 300, 309, 393, 764, 729, 562, 309, 8194, 777, 1412, 281, 652, 512, 5327, 13, 407, 11, 337, 1365, 11, 50904], 'temperature': 0.0, 'avg_logprob': -0.08013797657830375, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.009848624467849731}, {'id': 132, 'seek': 75580, 'start': 766.5999999999999, 'end': 772.28, 'text': ' if we wanted to detect faces in an image, a deep neural network algorithm might actually learn', 'tokens': [50904, 498, 321, 1415, 281, 5531, 8475, 294, 364, 3256, 11, 257, 2452, 18161, 3209, 9284, 1062, 767, 1466, 51188], 'temperature': 0.0, 'avg_logprob': -0.08013797657830375, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.009848624467849731}, {'id': 133, 'seek': 75580, 'start': 772.28, 'end': 777.3199999999999, 'text': ' that in order to detect a face, it first has to detect things like edges in the image, lines and', 'tokens': [51188, 300, 294, 1668, 281, 5531, 257, 1851, 11, 309, 700, 575, 281, 5531, 721, 411, 8819, 294, 264, 3256, 11, 3876, 293, 51440], 'temperature': 0.0, 'avg_logprob': -0.08013797657830375, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.009848624467849731}, {'id': 134, 'seek': 75580, 'start': 777.3199999999999, 'end': 782.4399999999999, 'text': ' edges. And when you combine those lines and edges, you can actually create compositions of features', 'tokens': [51440, 8819, 13, 400, 562, 291, 10432, 729, 3876, 293, 8819, 11, 291, 393, 767, 1884, 43401, 295, 4122, 51696], 'temperature': 0.0, 'avg_logprob': -0.08013797657830375, 'compression_ratio': 1.795539033457249, 'no_speech_prob': 0.009848624467849731}, {'id': 135, 'seek': 78244, 'start': 782.44, 'end': 788.2800000000001, 'text': ' like corners and curves, which when you combine those, you can create more high level features,', 'tokens': [50364, 411, 12413, 293, 19490, 11, 597, 562, 291, 10432, 729, 11, 291, 393, 1884, 544, 1090, 1496, 4122, 11, 50656], 'temperature': 0.0, 'avg_logprob': -0.11767586874305655, 'compression_ratio': 1.8014705882352942, 'no_speech_prob': 0.00022683096176479012}, {'id': 136, 'seek': 78244, 'start': 788.2800000000001, 'end': 793.8000000000001, 'text': ' for example, eyes and noses and ears. And then those are the features that allow you to ultimately', 'tokens': [50656, 337, 1365, 11, 2575, 293, 3269, 279, 293, 8798, 13, 400, 550, 729, 366, 264, 4122, 300, 2089, 291, 281, 6284, 50932], 'temperature': 0.0, 'avg_logprob': -0.11767586874305655, 'compression_ratio': 1.8014705882352942, 'no_speech_prob': 0.00022683096176479012}, {'id': 137, 'seek': 78244, 'start': 793.8000000000001, 'end': 797.72, 'text': ' detect what you care about detecting, which is the face. But all of these come from what are called', 'tokens': [50932, 5531, 437, 291, 1127, 466, 40237, 11, 597, 307, 264, 1851, 13, 583, 439, 295, 613, 808, 490, 437, 366, 1219, 51128], 'temperature': 0.0, 'avg_logprob': -0.11767586874305655, 'compression_ratio': 1.8014705882352942, 'no_speech_prob': 0.00022683096176479012}, {'id': 138, 'seek': 78244, 'start': 797.72, 'end': 802.6800000000001, 'text': ' kind of a hierarchical learning of features. And you can actually see some examples of these.', 'tokens': [51128, 733, 295, 257, 35250, 804, 2539, 295, 4122, 13, 400, 291, 393, 767, 536, 512, 5110, 295, 613, 13, 51376], 'temperature': 0.0, 'avg_logprob': -0.11767586874305655, 'compression_ratio': 1.8014705882352942, 'no_speech_prob': 0.00022683096176479012}, {'id': 139, 'seek': 78244, 'start': 802.6800000000001, 'end': 807.24, 'text': \" These are real features learned by a neural network and how they're combined defines this progression\", 'tokens': [51376, 1981, 366, 957, 4122, 3264, 538, 257, 18161, 3209, 293, 577, 436, 434, 9354, 23122, 341, 18733, 51604], 'temperature': 0.0, 'avg_logprob': -0.11767586874305655, 'compression_ratio': 1.8014705882352942, 'no_speech_prob': 0.00022683096176479012}, {'id': 140, 'seek': 80724, 'start': 807.24, 'end': 813.5600000000001, 'text': ' of information. But in fact, what I just described, this underlying and fundamental building block', 'tokens': [50364, 295, 1589, 13, 583, 294, 1186, 11, 437, 286, 445, 7619, 11, 341, 14217, 293, 8088, 2390, 3461, 50680], 'temperature': 0.0, 'avg_logprob': -0.1006308143789118, 'compression_ratio': 1.5975609756097562, 'no_speech_prob': 0.0015240820357576013}, {'id': 141, 'seek': 80724, 'start': 813.5600000000001, 'end': 819.4, 'text': ' of neural networks and deep learning have actually existed for decades. Now, why are we studying', 'tokens': [50680, 295, 18161, 9590, 293, 2452, 2539, 362, 767, 13135, 337, 7878, 13, 823, 11, 983, 366, 321, 7601, 50972], 'temperature': 0.0, 'avg_logprob': -0.1006308143789118, 'compression_ratio': 1.5975609756097562, 'no_speech_prob': 0.0015240820357576013}, {'id': 142, 'seek': 80724, 'start': 819.4, 'end': 825.08, 'text': ' all of this now and today in this class with all this great enthusiasm to learn this, right? Well,', 'tokens': [50972, 439, 295, 341, 586, 293, 965, 294, 341, 1508, 365, 439, 341, 869, 23417, 281, 1466, 341, 11, 558, 30, 1042, 11, 51256], 'temperature': 0.0, 'avg_logprob': -0.1006308143789118, 'compression_ratio': 1.5975609756097562, 'no_speech_prob': 0.0015240820357576013}, {'id': 143, 'seek': 80724, 'start': 825.08, 'end': 831.16, 'text': ' for one, there have been several key advances that have occurred in the past decade. Number one is', 'tokens': [51256, 337, 472, 11, 456, 362, 668, 2940, 2141, 25297, 300, 362, 11068, 294, 264, 1791, 10378, 13, 5118, 472, 307, 51560], 'temperature': 0.0, 'avg_logprob': -0.1006308143789118, 'compression_ratio': 1.5975609756097562, 'no_speech_prob': 0.0015240820357576013}, {'id': 144, 'seek': 83116, 'start': 831.16, 'end': 837.56, 'text': ' that data is so much more pervasive than it has ever been before in our lifetimes. These models', 'tokens': [50364, 300, 1412, 307, 370, 709, 544, 680, 39211, 813, 309, 575, 1562, 668, 949, 294, 527, 4545, 302, 1532, 13, 1981, 5245, 50684], 'temperature': 0.0, 'avg_logprob': -0.09459432642510597, 'compression_ratio': 1.646808510638298, 'no_speech_prob': 0.0032690782099962234}, {'id': 145, 'seek': 83116, 'start': 837.56, 'end': 844.52, 'text': \" are hungry for more data. And we're living in the age of big data. More data is available to these\", 'tokens': [50684, 366, 8067, 337, 544, 1412, 13, 400, 321, 434, 2647, 294, 264, 3205, 295, 955, 1412, 13, 5048, 1412, 307, 2435, 281, 613, 51032], 'temperature': 0.0, 'avg_logprob': -0.09459432642510597, 'compression_ratio': 1.646808510638298, 'no_speech_prob': 0.0032690782099962234}, {'id': 146, 'seek': 83116, 'start': 844.52, 'end': 850.68, 'text': ' models than ever before and they thrive off of that. Secondly, these algorithms are massively', 'tokens': [51032, 5245, 813, 1562, 949, 293, 436, 21233, 766, 295, 300, 13, 19483, 11, 613, 14642, 366, 29379, 51340], 'temperature': 0.0, 'avg_logprob': -0.09459432642510597, 'compression_ratio': 1.646808510638298, 'no_speech_prob': 0.0032690782099962234}, {'id': 147, 'seek': 83116, 'start': 850.68, 'end': 856.76, 'text': \" parallelizable. They require a lot of compute. And we're also at a unique time in history where we\", 'tokens': [51340, 8952, 22395, 13, 814, 3651, 257, 688, 295, 14722, 13, 400, 321, 434, 611, 412, 257, 3845, 565, 294, 2503, 689, 321, 51644], 'temperature': 0.0, 'avg_logprob': -0.09459432642510597, 'compression_ratio': 1.646808510638298, 'no_speech_prob': 0.0032690782099962234}, {'id': 148, 'seek': 85676, 'start': 856.76, 'end': 862.12, 'text': ' have the ability to train these extremely large scale algorithms and techniques that have existed', 'tokens': [50364, 362, 264, 3485, 281, 3847, 613, 4664, 2416, 4373, 14642, 293, 7512, 300, 362, 13135, 50632], 'temperature': 0.0, 'avg_logprob': -0.06812401537625294, 'compression_ratio': 1.6713286713286712, 'no_speech_prob': 0.0003352251660544425}, {'id': 149, 'seek': 85676, 'start': 862.12, 'end': 866.4399999999999, 'text': ' for a very long time. But we can now train them due to the hardware advances that have been made.', 'tokens': [50632, 337, 257, 588, 938, 565, 13, 583, 321, 393, 586, 3847, 552, 3462, 281, 264, 8837, 25297, 300, 362, 668, 1027, 13, 50848], 'temperature': 0.0, 'avg_logprob': -0.06812401537625294, 'compression_ratio': 1.6713286713286712, 'no_speech_prob': 0.0003352251660544425}, {'id': 150, 'seek': 85676, 'start': 866.4399999999999, 'end': 871.72, 'text': ' And finally, due to open source toolboxes and software platforms like TensorFlow, for example,', 'tokens': [50848, 400, 2721, 11, 3462, 281, 1269, 4009, 44593, 279, 293, 4722, 9473, 411, 37624, 11, 337, 1365, 11, 51112], 'temperature': 0.0, 'avg_logprob': -0.06812401537625294, 'compression_ratio': 1.6713286713286712, 'no_speech_prob': 0.0003352251660544425}, {'id': 151, 'seek': 85676, 'start': 871.72, 'end': 877.24, 'text': ' which all of you will get a lot of experience on in this class, training and building the code', 'tokens': [51112, 597, 439, 295, 291, 486, 483, 257, 688, 295, 1752, 322, 294, 341, 1508, 11, 3097, 293, 2390, 264, 3089, 51388], 'temperature': 0.0, 'avg_logprob': -0.06812401537625294, 'compression_ratio': 1.6713286713286712, 'no_speech_prob': 0.0003352251660544425}, {'id': 152, 'seek': 85676, 'start': 877.24, 'end': 881.48, 'text': ' for these neural networks has never been easier. So from the software point of view as well,', 'tokens': [51388, 337, 613, 18161, 9590, 575, 1128, 668, 3571, 13, 407, 490, 264, 4722, 935, 295, 1910, 382, 731, 11, 51600], 'temperature': 0.0, 'avg_logprob': -0.06812401537625294, 'compression_ratio': 1.6713286713286712, 'no_speech_prob': 0.0003352251660544425}, {'id': 153, 'seek': 88148, 'start': 881.48, 'end': 887.16, 'text': \" there have been incredible advances to open source the underlying fundamentals of what you're going\", 'tokens': [50364, 456, 362, 668, 4651, 25297, 281, 1269, 4009, 264, 14217, 29505, 295, 437, 291, 434, 516, 50648], 'temperature': 0.0, 'avg_logprob': -0.08751854029568759, 'compression_ratio': 1.8148148148148149, 'no_speech_prob': 0.001985959243029356}, {'id': 154, 'seek': 88148, 'start': 887.16, 'end': 893.72, 'text': ' to learn. So let me start now with just building up from the ground up, the fundamental building block', 'tokens': [50648, 281, 1466, 13, 407, 718, 385, 722, 586, 365, 445, 2390, 493, 490, 264, 2727, 493, 11, 264, 8088, 2390, 3461, 50976], 'temperature': 0.0, 'avg_logprob': -0.08751854029568759, 'compression_ratio': 1.8148148148148149, 'no_speech_prob': 0.001985959243029356}, {'id': 155, 'seek': 88148, 'start': 893.72, 'end': 898.12, 'text': \" of every single neural network that you're going to learn in this class. And that's going to be\", 'tokens': [50976, 295, 633, 2167, 18161, 3209, 300, 291, 434, 516, 281, 1466, 294, 341, 1508, 13, 400, 300, 311, 516, 281, 312, 51196], 'temperature': 0.0, 'avg_logprob': -0.08751854029568759, 'compression_ratio': 1.8148148148148149, 'no_speech_prob': 0.001985959243029356}, {'id': 156, 'seek': 88148, 'start': 898.12, 'end': 903.88, 'text': ' just a single neuron. And in neural network language, a single neuron is called a perceptron.', 'tokens': [51196, 445, 257, 2167, 34090, 13, 400, 294, 18161, 3209, 2856, 11, 257, 2167, 34090, 307, 1219, 257, 43276, 2044, 13, 51484], 'temperature': 0.0, 'avg_logprob': -0.08751854029568759, 'compression_ratio': 1.8148148148148149, 'no_speech_prob': 0.001985959243029356}, {'id': 157, 'seek': 90388, 'start': 904.2, 'end': 912.2, 'text': \" So what is a perceptron? A perceptron is, like I said, a single neuron. And it's actually, I'm going\", 'tokens': [50380, 407, 437, 307, 257, 43276, 2044, 30, 316, 43276, 2044, 307, 11, 411, 286, 848, 11, 257, 2167, 34090, 13, 400, 309, 311, 767, 11, 286, 478, 516, 50780], 'temperature': 0.0, 'avg_logprob': -0.12694815869601267, 'compression_ratio': 1.6962025316455696, 'no_speech_prob': 0.0016734832897782326}, {'id': 158, 'seek': 90388, 'start': 912.2, 'end': 916.36, 'text': \" to say it's very, very simple idea. So I want to make sure that everyone in the audience understands\", 'tokens': [50780, 281, 584, 309, 311, 588, 11, 588, 2199, 1558, 13, 407, 286, 528, 281, 652, 988, 300, 1518, 294, 264, 4034, 15146, 50988], 'temperature': 0.0, 'avg_logprob': -0.12694815869601267, 'compression_ratio': 1.6962025316455696, 'no_speech_prob': 0.0016734832897782326}, {'id': 159, 'seek': 90388, 'start': 916.36, 'end': 923.0, 'text': \" exactly what a perceptron is and how it works. So let's start by first defining a perceptron as taking\", 'tokens': [50988, 2293, 437, 257, 43276, 2044, 307, 293, 577, 309, 1985, 13, 407, 718, 311, 722, 538, 700, 17827, 257, 43276, 2044, 382, 1940, 51320], 'temperature': 0.0, 'avg_logprob': -0.12694815869601267, 'compression_ratio': 1.6962025316455696, 'no_speech_prob': 0.0016734832897782326}, {'id': 160, 'seek': 90388, 'start': 923.0, 'end': 929.8, 'text': ' as input a set of inputs. So on the left hand side, you can see this perceptron takes M different', 'tokens': [51320, 382, 4846, 257, 992, 295, 15743, 13, 407, 322, 264, 1411, 1011, 1252, 11, 291, 393, 536, 341, 43276, 2044, 2516, 376, 819, 51660], 'temperature': 0.0, 'avg_logprob': -0.12694815869601267, 'compression_ratio': 1.6962025316455696, 'no_speech_prob': 0.0016734832897782326}, {'id': 161, 'seek': 92980, 'start': 929.8, 'end': 935.0799999999999, 'text': \" inputs, 1 to M. These are the blue circles. We're denoting these inputs as x's.\", 'tokens': [50364, 15743, 11, 502, 281, 376, 13, 1981, 366, 264, 3344, 13040, 13, 492, 434, 1441, 17001, 613, 15743, 382, 2031, 311, 13, 50628], 'temperature': 0.0, 'avg_logprob': -0.119297260933734, 'compression_ratio': 1.6278026905829597, 'no_speech_prob': 0.0003005787148140371}, {'id': 162, 'seek': 92980, 'start': 936.5999999999999, 'end': 942.3599999999999, 'text': ' Each of these numbers, each of these inputs, is then multiplied by a corresponding weight,', 'tokens': [50704, 6947, 295, 613, 3547, 11, 1184, 295, 613, 15743, 11, 307, 550, 17207, 538, 257, 11760, 3364, 11, 50992], 'temperature': 0.0, 'avg_logprob': -0.119297260933734, 'compression_ratio': 1.6278026905829597, 'no_speech_prob': 0.0003005787148140371}, {'id': 163, 'seek': 92980, 'start': 942.3599999999999, 'end': 949.0799999999999, 'text': \" which we can call w. So x1 will be multiplied by w1. And we'll add the result of all of these\", 'tokens': [50992, 597, 321, 393, 818, 261, 13, 407, 2031, 16, 486, 312, 17207, 538, 261, 16, 13, 400, 321, 603, 909, 264, 1874, 295, 439, 295, 613, 51328], 'temperature': 0.0, 'avg_logprob': -0.119297260933734, 'compression_ratio': 1.6278026905829597, 'no_speech_prob': 0.0003005787148140371}, {'id': 164, 'seek': 92980, 'start': 949.0799999999999, 'end': 955.64, 'text': ' multiplications together. Now we take that single number after the addition and we pass it through', 'tokens': [51328, 17596, 763, 1214, 13, 823, 321, 747, 300, 2167, 1230, 934, 264, 4500, 293, 321, 1320, 309, 807, 51656], 'temperature': 0.0, 'avg_logprob': -0.119297260933734, 'compression_ratio': 1.6278026905829597, 'no_speech_prob': 0.0003005787148140371}, {'id': 165, 'seek': 95564, 'start': 955.64, 'end': 960.4399999999999, 'text': ' this nonlinear, what we call a nonlinear activation function. And that produces our final output', 'tokens': [50364, 341, 2107, 28263, 11, 437, 321, 818, 257, 2107, 28263, 24433, 2445, 13, 400, 300, 14725, 527, 2572, 5598, 50604], 'temperature': 0.0, 'avg_logprob': -0.08200388623957049, 'compression_ratio': 1.6884057971014492, 'no_speech_prob': 0.0007206487352959812}, {'id': 166, 'seek': 95564, 'start': 960.4399999999999, 'end': 967.3199999999999, 'text': ' of the perceptron, which we can call y. Now this is actually not entirely accurate of the', 'tokens': [50604, 295, 264, 43276, 2044, 11, 597, 321, 393, 818, 288, 13, 823, 341, 307, 767, 406, 7696, 8559, 295, 264, 50948], 'temperature': 0.0, 'avg_logprob': -0.08200388623957049, 'compression_ratio': 1.6884057971014492, 'no_speech_prob': 0.0007206487352959812}, {'id': 167, 'seek': 95564, 'start': 967.96, 'end': 972.68, 'text': \" picture of a perceptron. There's one step that I forgot to mention here. So in addition to\", 'tokens': [50980, 3036, 295, 257, 43276, 2044, 13, 821, 311, 472, 1823, 300, 286, 5298, 281, 2152, 510, 13, 407, 294, 4500, 281, 51216], 'temperature': 0.0, 'avg_logprob': -0.08200388623957049, 'compression_ratio': 1.6884057971014492, 'no_speech_prob': 0.0007206487352959812}, {'id': 168, 'seek': 95564, 'start': 973.4, 'end': 977.08, 'text': \" multiplying all of these inputs with their corresponding weights, we're also now going to add\", 'tokens': [51252, 30955, 439, 295, 613, 15743, 365, 641, 11760, 17443, 11, 321, 434, 611, 586, 516, 281, 909, 51436], 'temperature': 0.0, 'avg_logprob': -0.08200388623957049, 'compression_ratio': 1.6884057971014492, 'no_speech_prob': 0.0007206487352959812}, {'id': 169, 'seek': 95564, 'start': 977.08, 'end': 983.08, 'text': \" what's called a bias term. Here denoted as this w0, which is just a scalar weight. And you can\", 'tokens': [51436, 437, 311, 1219, 257, 12577, 1433, 13, 1692, 1441, 23325, 382, 341, 261, 15, 11, 597, 307, 445, 257, 39684, 3364, 13, 400, 291, 393, 51736], 'temperature': 0.0, 'avg_logprob': -0.08200388623957049, 'compression_ratio': 1.6884057971014492, 'no_speech_prob': 0.0007206487352959812}, {'id': 170, 'seek': 98308, 'start': 983.08, 'end': 989.24, 'text': \" think of it coming with an input of just 1. So that's going to allow the network to basically shift\", 'tokens': [50364, 519, 295, 309, 1348, 365, 364, 4846, 295, 445, 502, 13, 407, 300, 311, 516, 281, 2089, 264, 3209, 281, 1936, 5513, 50672], 'temperature': 0.0, 'avg_logprob': -0.10530625581741333, 'compression_ratio': 1.6221198156682028, 'no_speech_prob': 0.00021647842368111014}, {'id': 171, 'seek': 98308, 'start': 989.24, 'end': 995.8000000000001, 'text': ' its nonlinear activation function nonlinearly as it sees its inputs.', 'tokens': [50672, 1080, 2107, 28263, 24433, 2445, 2107, 28263, 356, 382, 309, 8194, 1080, 15743, 13, 51000], 'temperature': 0.0, 'avg_logprob': -0.10530625581741333, 'compression_ratio': 1.6221198156682028, 'no_speech_prob': 0.00021647842368111014}, {'id': 172, 'seek': 98308, 'start': 996.76, 'end': 1002.36, 'text': ' Now on the right hand side, you can see this diagram mathematically formulated. As a single', 'tokens': [51048, 823, 322, 264, 558, 1011, 1252, 11, 291, 393, 536, 341, 10686, 44003, 48936, 13, 1018, 257, 2167, 51328], 'temperature': 0.0, 'avg_logprob': -0.10530625581741333, 'compression_ratio': 1.6221198156682028, 'no_speech_prob': 0.00021647842368111014}, {'id': 173, 'seek': 98308, 'start': 1002.36, 'end': 1008.6800000000001, 'text': ' equation, we can now rewrite this linear this equation with linear algebra terms of vectors', 'tokens': [51328, 5367, 11, 321, 393, 586, 28132, 341, 8213, 341, 5367, 365, 8213, 21989, 2115, 295, 18875, 51644], 'temperature': 0.0, 'avg_logprob': -0.10530625581741333, 'compression_ratio': 1.6221198156682028, 'no_speech_prob': 0.00021647842368111014}, {'id': 174, 'seek': 100868, 'start': 1008.68, 'end': 1016.5999999999999, 'text': ' and dot products. So for example, we can define our entire inputs x1 to xm as large vector x.', 'tokens': [50364, 293, 5893, 3383, 13, 407, 337, 1365, 11, 321, 393, 6964, 527, 2302, 15743, 2031, 16, 281, 2031, 76, 382, 2416, 8062, 2031, 13, 50760], 'temperature': 0.0, 'avg_logprob': -0.18037852612170543, 'compression_ratio': 1.7170731707317073, 'no_speech_prob': 0.002148179104551673}, {'id': 175, 'seek': 100868, 'start': 1017.56, 'end': 1023.56, 'text': ' That large vector x can be multiplied by or take you a dot, excuse me, matrix multiplied', 'tokens': [50808, 663, 2416, 8062, 2031, 393, 312, 17207, 538, 420, 747, 291, 257, 5893, 11, 8960, 385, 11, 8141, 17207, 51108], 'temperature': 0.0, 'avg_logprob': -0.18037852612170543, 'compression_ratio': 1.7170731707317073, 'no_speech_prob': 0.002148179104551673}, {'id': 176, 'seek': 100868, 'start': 1023.56, 'end': 1029.56, 'text': ' with our weights w. This again, another vector of our weights w1 to wm.', 'tokens': [51108, 365, 527, 17443, 261, 13, 639, 797, 11, 1071, 8062, 295, 527, 17443, 261, 16, 281, 261, 76, 13, 51408], 'temperature': 0.0, 'avg_logprob': -0.18037852612170543, 'compression_ratio': 1.7170731707317073, 'no_speech_prob': 0.002148179104551673}, {'id': 177, 'seek': 100868, 'start': 1030.36, 'end': 1036.12, 'text': ' Taking their dot product not only multiplies them, but it also adds the resulting terms together.', 'tokens': [51448, 17837, 641, 5893, 1674, 406, 787, 12788, 530, 552, 11, 457, 309, 611, 10860, 264, 16505, 2115, 1214, 13, 51736], 'temperature': 0.0, 'avg_logprob': -0.18037852612170543, 'compression_ratio': 1.7170731707317073, 'no_speech_prob': 0.002148179104551673}, {'id': 178, 'seek': 103612, 'start': 1036.6, 'end': 1040.36, 'text': ' Adding a bias, like we said before, and applying this nonlinearity.', 'tokens': [50388, 31204, 257, 12577, 11, 411, 321, 848, 949, 11, 293, 9275, 341, 2107, 1889, 17409, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.1346704743125222, 'compression_ratio': 1.6289592760180995, 'no_speech_prob': 0.0011876709759235382}, {'id': 179, 'seek': 103612, 'start': 1042.6799999999998, 'end': 1047.3999999999999, 'text': \" Now you might be wondering what is this nonlinear function? I've mentioned it a few times already.\", 'tokens': [50692, 823, 291, 1062, 312, 6359, 437, 307, 341, 2107, 28263, 2445, 30, 286, 600, 2835, 309, 257, 1326, 1413, 1217, 13, 50928], 'temperature': 0.0, 'avg_logprob': -0.1346704743125222, 'compression_ratio': 1.6289592760180995, 'no_speech_prob': 0.0011876709759235382}, {'id': 180, 'seek': 103612, 'start': 1047.3999999999999, 'end': 1054.12, 'text': ' Well, I said it is a function that we pass the outputs of the neural network through before we', 'tokens': [50928, 1042, 11, 286, 848, 309, 307, 257, 2445, 300, 321, 1320, 264, 23930, 295, 264, 18161, 3209, 807, 949, 321, 51264], 'temperature': 0.0, 'avg_logprob': -0.1346704743125222, 'compression_ratio': 1.6289592760180995, 'no_speech_prob': 0.0011876709759235382}, {'id': 181, 'seek': 103612, 'start': 1054.12, 'end': 1060.76, 'text': \" return it to the next neuron in the pipeline. So one common example of a nonlinear function that's\", 'tokens': [51264, 2736, 309, 281, 264, 958, 34090, 294, 264, 15517, 13, 407, 472, 2689, 1365, 295, 257, 2107, 28263, 2445, 300, 311, 51596], 'temperature': 0.0, 'avg_logprob': -0.1346704743125222, 'compression_ratio': 1.6289592760180995, 'no_speech_prob': 0.0011876709759235382}, {'id': 182, 'seek': 106076, 'start': 1060.76, 'end': 1065.24, 'text': ' very popular in deep neural networks is called the sigmoid function. You can think of this as kind', 'tokens': [50364, 588, 3743, 294, 2452, 18161, 9590, 307, 1219, 264, 4556, 3280, 327, 2445, 13, 509, 393, 519, 295, 341, 382, 733, 50588], 'temperature': 0.0, 'avg_logprob': -0.12640114834434107, 'compression_ratio': 1.770909090909091, 'no_speech_prob': 0.001283917110413313}, {'id': 183, 'seek': 106076, 'start': 1065.24, 'end': 1072.04, 'text': ' of a continuous version of a threshold function. It goes from 0 to 1 and it can take us input any', 'tokens': [50588, 295, 257, 10957, 3037, 295, 257, 14678, 2445, 13, 467, 1709, 490, 1958, 281, 502, 293, 309, 393, 747, 505, 4846, 604, 50928], 'temperature': 0.0, 'avg_logprob': -0.12640114834434107, 'compression_ratio': 1.770909090909091, 'no_speech_prob': 0.001283917110413313}, {'id': 184, 'seek': 106076, 'start': 1072.04, 'end': 1077.08, 'text': ' real number on the real number line. And you can see an example of it illustrated on the bottom', 'tokens': [50928, 957, 1230, 322, 264, 957, 1230, 1622, 13, 400, 291, 393, 536, 364, 1365, 295, 309, 33875, 322, 264, 2767, 51180], 'temperature': 0.0, 'avg_logprob': -0.12640114834434107, 'compression_ratio': 1.770909090909091, 'no_speech_prob': 0.001283917110413313}, {'id': 185, 'seek': 106076, 'start': 1077.08, 'end': 1082.6, 'text': ' right hand. Now, in fact, there are many types of nonlinear activation functions that are popular', 'tokens': [51180, 558, 1011, 13, 823, 11, 294, 1186, 11, 456, 366, 867, 3467, 295, 2107, 28263, 24433, 6828, 300, 366, 3743, 51456], 'temperature': 0.0, 'avg_logprob': -0.12640114834434107, 'compression_ratio': 1.770909090909091, 'no_speech_prob': 0.001283917110413313}, {'id': 186, 'seek': 106076, 'start': 1082.6, 'end': 1086.52, 'text': \" in deep neural networks. And here are some common ones. And throughout this presentation, you'll\", 'tokens': [51456, 294, 2452, 18161, 9590, 13, 400, 510, 366, 512, 2689, 2306, 13, 400, 3710, 341, 5860, 11, 291, 603, 51652], 'temperature': 0.0, 'avg_logprob': -0.12640114834434107, 'compression_ratio': 1.770909090909091, 'no_speech_prob': 0.001283917110413313}, {'id': 187, 'seek': 108652, 'start': 1086.52, 'end': 1091.08, 'text': \" actually see some examples of these code snippets on the bottom of the slides where we'll try and\", 'tokens': [50364, 767, 536, 512, 5110, 295, 613, 3089, 35623, 1385, 322, 264, 2767, 295, 264, 9788, 689, 321, 603, 853, 293, 50592], 'temperature': 0.0, 'avg_logprob': -0.08031508326530457, 'compression_ratio': 1.7777777777777777, 'no_speech_prob': 0.0002377682103542611}, {'id': 188, 'seek': 108652, 'start': 1091.08, 'end': 1096.04, 'text': \" actually tie in some of what you're learning in the lectures to actual software and how you can\", 'tokens': [50592, 767, 7582, 294, 512, 295, 437, 291, 434, 2539, 294, 264, 16564, 281, 3539, 4722, 293, 577, 291, 393, 50840], 'temperature': 0.0, 'avg_logprob': -0.08031508326530457, 'compression_ratio': 1.7777777777777777, 'no_speech_prob': 0.0002377682103542611}, {'id': 189, 'seek': 108652, 'start': 1096.04, 'end': 1101.0, 'text': ' implement these pieces, which will help you a lot for your software labs explicitly. So the sigmoid', 'tokens': [50840, 4445, 613, 3755, 11, 597, 486, 854, 291, 257, 688, 337, 428, 4722, 20339, 20803, 13, 407, 264, 4556, 3280, 327, 51088], 'temperature': 0.0, 'avg_logprob': -0.08031508326530457, 'compression_ratio': 1.7777777777777777, 'no_speech_prob': 0.0002377682103542611}, {'id': 190, 'seek': 108652, 'start': 1101.0, 'end': 1106.04, 'text': \" activation on the left is very popular since it's a function that outputs between 0 and 1. So\", 'tokens': [51088, 24433, 322, 264, 1411, 307, 588, 3743, 1670, 309, 311, 257, 2445, 300, 23930, 1296, 1958, 293, 502, 13, 407, 51340], 'temperature': 0.0, 'avg_logprob': -0.08031508326530457, 'compression_ratio': 1.7777777777777777, 'no_speech_prob': 0.0002377682103542611}, {'id': 191, 'seek': 108652, 'start': 1106.04, 'end': 1110.44, 'text': ' especially when you want to deal with probability distributions, for example, this is very important', 'tokens': [51340, 2318, 562, 291, 528, 281, 2028, 365, 8482, 37870, 11, 337, 1365, 11, 341, 307, 588, 1021, 51560], 'temperature': 0.0, 'avg_logprob': -0.08031508326530457, 'compression_ratio': 1.7777777777777777, 'no_speech_prob': 0.0002377682103542611}, {'id': 192, 'seek': 108652, 'start': 1110.44, 'end': 1115.4, 'text': ' because probabilities live between 0 and 1. In modern deep neural networks, though, the', 'tokens': [51560, 570, 33783, 1621, 1296, 1958, 293, 502, 13, 682, 4363, 2452, 18161, 9590, 11, 1673, 11, 264, 51808], 'temperature': 0.0, 'avg_logprob': -0.08031508326530457, 'compression_ratio': 1.7777777777777777, 'no_speech_prob': 0.0002377682103542611}, {'id': 193, 'seek': 111540, 'start': 1115.4, 'end': 1119.64, 'text': ' relu function, which you can see on the far right hand, is a very popular activation function', 'tokens': [50364, 1039, 84, 2445, 11, 597, 291, 393, 536, 322, 264, 1400, 558, 1011, 11, 307, 257, 588, 3743, 24433, 2445, 50576], 'temperature': 0.0, 'avg_logprob': -0.12633131467379055, 'compression_ratio': 1.734375, 'no_speech_prob': 0.0007550930604338646}, {'id': 194, 'seek': 111540, 'start': 1119.64, 'end': 1123.3200000000002, 'text': \" because it's piecewise linear. It's extremely efficient to compute, especially when\", 'tokens': [50576, 570, 309, 311, 2522, 3711, 8213, 13, 467, 311, 4664, 7148, 281, 14722, 11, 2318, 562, 50760], 'temperature': 0.0, 'avg_logprob': -0.12633131467379055, 'compression_ratio': 1.734375, 'no_speech_prob': 0.0007550930604338646}, {'id': 195, 'seek': 111540, 'start': 1123.3200000000002, 'end': 1129.96, 'text': \" computing it's derivatives, right? It's derivatives are constants, except for nonlinear, yet 0.\", 'tokens': [50760, 15866, 309, 311, 33733, 11, 558, 30, 467, 311, 33733, 366, 35870, 11, 3993, 337, 2107, 28263, 11, 1939, 1958, 13, 51092], 'temperature': 0.0, 'avg_logprob': -0.12633131467379055, 'compression_ratio': 1.734375, 'no_speech_prob': 0.0007550930604338646}, {'id': 196, 'seek': 111540, 'start': 1131.64, 'end': 1135.88, 'text': ' Now, I hope actually all of you are probably asking this question to yourself of why do we even', 'tokens': [51176, 823, 11, 286, 1454, 767, 439, 295, 291, 366, 1391, 3365, 341, 1168, 281, 1803, 295, 983, 360, 321, 754, 51388], 'temperature': 0.0, 'avg_logprob': -0.12633131467379055, 'compression_ratio': 1.734375, 'no_speech_prob': 0.0007550930604338646}, {'id': 197, 'seek': 111540, 'start': 1135.88, 'end': 1139.4, 'text': ' need this nonlinear activation function? It seems like it kind of just complicates this whole', 'tokens': [51388, 643, 341, 2107, 28263, 24433, 2445, 30, 467, 2544, 411, 309, 733, 295, 445, 16060, 1024, 341, 1379, 51564], 'temperature': 0.0, 'avg_logprob': -0.12633131467379055, 'compression_ratio': 1.734375, 'no_speech_prob': 0.0007550930604338646}, {'id': 198, 'seek': 111540, 'start': 1139.4, 'end': 1144.1200000000001, 'text': \" picture when we didn't really need it in the first place. And I want to just spend a moment\", 'tokens': [51564, 3036, 562, 321, 994, 380, 534, 643, 309, 294, 264, 700, 1081, 13, 400, 286, 528, 281, 445, 3496, 257, 1623, 51800], 'temperature': 0.0, 'avg_logprob': -0.12633131467379055, 'compression_ratio': 1.734375, 'no_speech_prob': 0.0007550930604338646}, {'id': 199, 'seek': 114412, 'start': 1144.12, 'end': 1149.4799999999998, 'text': ' on answering this because the point of a nonlinear activation function is, of course, number one is', 'tokens': [50364, 322, 13430, 341, 570, 264, 935, 295, 257, 2107, 28263, 24433, 2445, 307, 11, 295, 1164, 11, 1230, 472, 307, 50632], 'temperature': 0.0, 'avg_logprob': -0.0811201757635952, 'compression_ratio': 1.775, 'no_speech_prob': 0.0020184454042464495}, {'id': 200, 'seek': 114412, 'start': 1149.4799999999998, 'end': 1155.9599999999998, 'text': ' to introduce nonlinearities to our data, right? If we think about our data, almost all data that we', 'tokens': [50632, 281, 5366, 2107, 28263, 1088, 281, 527, 1412, 11, 558, 30, 759, 321, 519, 466, 527, 1412, 11, 1920, 439, 1412, 300, 321, 50956], 'temperature': 0.0, 'avg_logprob': -0.0811201757635952, 'compression_ratio': 1.775, 'no_speech_prob': 0.0020184454042464495}, {'id': 201, 'seek': 114412, 'start': 1155.9599999999998, 'end': 1161.56, 'text': ' care about, all real world data is highly nonlinear. Now, this is important because if we want to be', 'tokens': [50956, 1127, 466, 11, 439, 957, 1002, 1412, 307, 5405, 2107, 28263, 13, 823, 11, 341, 307, 1021, 570, 498, 321, 528, 281, 312, 51236], 'temperature': 0.0, 'avg_logprob': -0.0811201757635952, 'compression_ratio': 1.775, 'no_speech_prob': 0.0020184454042464495}, {'id': 202, 'seek': 114412, 'start': 1161.56, 'end': 1165.7199999999998, 'text': ' able to deal with those types of data sets, we need models that are also nonlinear so they can capture', 'tokens': [51236, 1075, 281, 2028, 365, 729, 3467, 295, 1412, 6352, 11, 321, 643, 5245, 300, 366, 611, 2107, 28263, 370, 436, 393, 7983, 51444], 'temperature': 0.0, 'avg_logprob': -0.0811201757635952, 'compression_ratio': 1.775, 'no_speech_prob': 0.0020184454042464495}, {'id': 203, 'seek': 114412, 'start': 1165.7199999999998, 'end': 1169.6399999999999, 'text': ' those same types of patterns. So imagine I told you to separate, for example, I gave you this', 'tokens': [51444, 729, 912, 3467, 295, 8294, 13, 407, 3811, 286, 1907, 291, 281, 4994, 11, 337, 1365, 11, 286, 2729, 291, 341, 51640], 'temperature': 0.0, 'avg_logprob': -0.0811201757635952, 'compression_ratio': 1.775, 'no_speech_prob': 0.0020184454042464495}, {'id': 204, 'seek': 116964, 'start': 1169.64, 'end': 1173.88, 'text': ' data set, red points from green points and I asked you to try and separate those two types of', 'tokens': [50364, 1412, 992, 11, 2182, 2793, 490, 3092, 2793, 293, 286, 2351, 291, 281, 853, 293, 4994, 729, 732, 3467, 295, 50576], 'temperature': 0.0, 'avg_logprob': -0.09396726497705432, 'compression_ratio': 1.8491803278688526, 'no_speech_prob': 0.001304221455939114}, {'id': 205, 'seek': 116964, 'start': 1173.88, 'end': 1178.68, 'text': ' data points. Now, you might think that this is easy, but what if I could only, if I told you,', 'tokens': [50576, 1412, 2793, 13, 823, 11, 291, 1062, 519, 300, 341, 307, 1858, 11, 457, 437, 498, 286, 727, 787, 11, 498, 286, 1907, 291, 11, 50816], 'temperature': 0.0, 'avg_logprob': -0.09396726497705432, 'compression_ratio': 1.8491803278688526, 'no_speech_prob': 0.001304221455939114}, {'id': 206, 'seek': 116964, 'start': 1178.68, 'end': 1183.16, 'text': ' that you could only use a single line to do so? Well, now it becomes a very complicated problem.', 'tokens': [50816, 300, 291, 727, 787, 764, 257, 2167, 1622, 281, 360, 370, 30, 1042, 11, 586, 309, 3643, 257, 588, 6179, 1154, 13, 51040], 'temperature': 0.0, 'avg_logprob': -0.09396726497705432, 'compression_ratio': 1.8491803278688526, 'no_speech_prob': 0.001304221455939114}, {'id': 207, 'seek': 116964, 'start': 1183.16, 'end': 1189.4, 'text': \" In fact, you can't really solve it effectively with a single line. And in fact, if you introduce\", 'tokens': [51040, 682, 1186, 11, 291, 393, 380, 534, 5039, 309, 8659, 365, 257, 2167, 1622, 13, 400, 294, 1186, 11, 498, 291, 5366, 51352], 'temperature': 0.0, 'avg_logprob': -0.09396726497705432, 'compression_ratio': 1.8491803278688526, 'no_speech_prob': 0.001304221455939114}, {'id': 208, 'seek': 116964, 'start': 1189.4, 'end': 1194.76, 'text': \" nonlinear activation functions to your solution, that's exactly what allows you to, you know,\", 'tokens': [51352, 2107, 28263, 24433, 6828, 281, 428, 3827, 11, 300, 311, 2293, 437, 4045, 291, 281, 11, 291, 458, 11, 51620], 'temperature': 0.0, 'avg_logprob': -0.09396726497705432, 'compression_ratio': 1.8491803278688526, 'no_speech_prob': 0.001304221455939114}, {'id': 209, 'seek': 116964, 'start': 1194.76, 'end': 1199.16, 'text': ' deal with these types of problems. Nonlinear activation functions allow you to deal with', 'tokens': [51620, 2028, 365, 613, 3467, 295, 2740, 13, 8774, 28263, 24433, 6828, 2089, 291, 281, 2028, 365, 51840], 'temperature': 0.0, 'avg_logprob': -0.09396726497705432, 'compression_ratio': 1.8491803278688526, 'no_speech_prob': 0.001304221455939114}, {'id': 210, 'seek': 119916, 'start': 1199.16, 'end': 1205.72, 'text': \" nonlinear types of data. Now, and that's what exactly makes neural networks so powerful at their core.\", 'tokens': [50364, 2107, 28263, 3467, 295, 1412, 13, 823, 11, 293, 300, 311, 437, 2293, 1669, 18161, 9590, 370, 4005, 412, 641, 4965, 13, 50692], 'temperature': 0.0, 'avg_logprob': -0.11388023628676233, 'compression_ratio': 1.6923076923076923, 'no_speech_prob': 0.0004877775500062853}, {'id': 211, 'seek': 119916, 'start': 1206.6000000000001, 'end': 1210.52, 'text': \" So let's understand this maybe with a very simple example, walking through this diagram of a\", 'tokens': [50736, 407, 718, 311, 1223, 341, 1310, 365, 257, 588, 2199, 1365, 11, 4494, 807, 341, 10686, 295, 257, 50932], 'temperature': 0.0, 'avg_logprob': -0.11388023628676233, 'compression_ratio': 1.6923076923076923, 'no_speech_prob': 0.0004877775500062853}, {'id': 212, 'seek': 119916, 'start': 1210.52, 'end': 1215.96, 'text': ' perceptron one more time. Imagine I give you this trained neural network with weights now, not', 'tokens': [50932, 43276, 2044, 472, 544, 565, 13, 11739, 286, 976, 291, 341, 8895, 18161, 3209, 365, 17443, 586, 11, 406, 51204], 'temperature': 0.0, 'avg_logprob': -0.11388023628676233, 'compression_ratio': 1.6923076923076923, 'no_speech_prob': 0.0004877775500062853}, {'id': 213, 'seek': 119916, 'start': 1215.96, 'end': 1221.24, 'text': \" W1, W2. I'm going to actually give you numbers at these locations, right? So the trained weights,\", 'tokens': [51204, 343, 16, 11, 343, 17, 13, 286, 478, 516, 281, 767, 976, 291, 3547, 412, 613, 9253, 11, 558, 30, 407, 264, 8895, 17443, 11, 51468], 'temperature': 0.0, 'avg_logprob': -0.11388023628676233, 'compression_ratio': 1.6923076923076923, 'no_speech_prob': 0.0004877775500062853}, {'id': 214, 'seek': 119916, 'start': 1221.24, 'end': 1229.0800000000002, 'text': ' W0 will be 1 and W will be a vector of 3 and negative 2. So this neural network has two inputs,', 'tokens': [51468, 343, 15, 486, 312, 502, 293, 343, 486, 312, 257, 8062, 295, 805, 293, 3671, 568, 13, 407, 341, 18161, 3209, 575, 732, 15743, 11, 51860], 'temperature': 0.0, 'avg_logprob': -0.11388023628676233, 'compression_ratio': 1.6923076923076923, 'no_speech_prob': 0.0004877775500062853}, {'id': 215, 'seek': 122908, 'start': 1229.3999999999999, 'end': 1234.1999999999998, 'text': ' like we said before, it has input x1 and has input x2. If we want to get the output of it,', 'tokens': [50380, 411, 321, 848, 949, 11, 309, 575, 4846, 2031, 16, 293, 575, 4846, 2031, 17, 13, 759, 321, 528, 281, 483, 264, 5598, 295, 309, 11, 50620], 'temperature': 0.0, 'avg_logprob': -0.12547870340018435, 'compression_ratio': 1.7559055118110236, 'no_speech_prob': 0.0013877855380997062}, {'id': 216, 'seek': 122908, 'start': 1234.76, 'end': 1238.76, 'text': ' this is also the main thing I want all of you to take away from this lecture today is that', 'tokens': [50648, 341, 307, 611, 264, 2135, 551, 286, 528, 439, 295, 291, 281, 747, 1314, 490, 341, 7991, 965, 307, 300, 50848], 'temperature': 0.0, 'avg_logprob': -0.12547870340018435, 'compression_ratio': 1.7559055118110236, 'no_speech_prob': 0.0013877855380997062}, {'id': 217, 'seek': 122908, 'start': 1238.76, 'end': 1243.1599999999999, 'text': ' to get the output of a perceptron, there are three steps we need to take, right? From this stage,', 'tokens': [50848, 281, 483, 264, 5598, 295, 257, 43276, 2044, 11, 456, 366, 1045, 4439, 321, 643, 281, 747, 11, 558, 30, 3358, 341, 3233, 11, 51068], 'temperature': 0.0, 'avg_logprob': -0.12547870340018435, 'compression_ratio': 1.7559055118110236, 'no_speech_prob': 0.0013877855380997062}, {'id': 218, 'seek': 122908, 'start': 1243.1599999999999, 'end': 1246.6, 'text': ' we first compute the multiplication of our inputs with our weights.', 'tokens': [51068, 321, 700, 14722, 264, 27290, 295, 527, 15743, 365, 527, 17443, 13, 51240], 'temperature': 0.0, 'avg_logprob': -0.12547870340018435, 'compression_ratio': 1.7559055118110236, 'no_speech_prob': 0.0013877855380997062}, {'id': 219, 'seek': 122908, 'start': 1248.6, 'end': 1255.0, 'text': \" Sorry, yeah, multiply them together, add their result and compute a nonlinearity. It's these three\", 'tokens': [51340, 4919, 11, 1338, 11, 12972, 552, 1214, 11, 909, 641, 1874, 293, 14722, 257, 2107, 1889, 17409, 13, 467, 311, 613, 1045, 51660], 'temperature': 0.0, 'avg_logprob': -0.12547870340018435, 'compression_ratio': 1.7559055118110236, 'no_speech_prob': 0.0013877855380997062}, {'id': 220, 'seek': 125500, 'start': 1255.08, 'end': 1260.04, 'text': ' steps that define the forward propagation of information through a perceptron.', 'tokens': [50368, 4439, 300, 6964, 264, 2128, 38377, 295, 1589, 807, 257, 43276, 2044, 13, 50616], 'temperature': 0.0, 'avg_logprob': -0.13584467080923227, 'compression_ratio': 1.5895196506550218, 'no_speech_prob': 0.0011156236287206411}, {'id': 221, 'seek': 125500, 'start': 1261.0, 'end': 1266.68, 'text': \" So let's take a look at how that exactly works, right? So if we plug in these numbers to those\", 'tokens': [50664, 407, 718, 311, 747, 257, 574, 412, 577, 300, 2293, 1985, 11, 558, 30, 407, 498, 321, 5452, 294, 613, 3547, 281, 729, 50948], 'temperature': 0.0, 'avg_logprob': -0.13584467080923227, 'compression_ratio': 1.5895196506550218, 'no_speech_prob': 0.0011156236287206411}, {'id': 222, 'seek': 125500, 'start': 1266.68, 'end': 1272.84, 'text': ' equations, we can see that everything inside of our nonlinearity, here the nonlinearity is G,', 'tokens': [50948, 11787, 11, 321, 393, 536, 300, 1203, 1854, 295, 527, 2107, 1889, 17409, 11, 510, 264, 2107, 1889, 17409, 307, 460, 11, 51256], 'temperature': 0.0, 'avg_logprob': -0.13584467080923227, 'compression_ratio': 1.5895196506550218, 'no_speech_prob': 0.0011156236287206411}, {'id': 223, 'seek': 125500, 'start': 1272.84, 'end': 1279.88, 'text': ' right? That function G, which could be a sigmoid, we saw a previous slide. That component inside', 'tokens': [51256, 558, 30, 663, 2445, 460, 11, 597, 727, 312, 257, 4556, 3280, 327, 11, 321, 1866, 257, 3894, 4137, 13, 663, 6542, 1854, 51608], 'temperature': 0.0, 'avg_logprob': -0.13584467080923227, 'compression_ratio': 1.5895196506550218, 'no_speech_prob': 0.0011156236287206411}, {'id': 224, 'seek': 127988, 'start': 1279.96, 'end': 1285.64, 'text': ' our nonlinearity is in fact just a two-dimensional line. It has two inputs, and if we consider the', 'tokens': [50368, 527, 2107, 1889, 17409, 307, 294, 1186, 445, 257, 732, 12, 18759, 1622, 13, 467, 575, 732, 15743, 11, 293, 498, 321, 1949, 264, 50652], 'temperature': 0.0, 'avg_logprob': -0.06707105554383376, 'compression_ratio': 1.8098859315589353, 'no_speech_prob': 0.000766830809880048}, {'id': 225, 'seek': 127988, 'start': 1285.64, 'end': 1291.64, 'text': ' space of all of the possible inputs that this neural network could see, we can actually plot this', 'tokens': [50652, 1901, 295, 439, 295, 264, 1944, 15743, 300, 341, 18161, 3209, 727, 536, 11, 321, 393, 767, 7542, 341, 50952], 'temperature': 0.0, 'avg_logprob': -0.06707105554383376, 'compression_ratio': 1.8098859315589353, 'no_speech_prob': 0.000766830809880048}, {'id': 226, 'seek': 127988, 'start': 1291.64, 'end': 1298.6000000000001, 'text': ' on a decision boundary, right? We can plot this two-dimensional line as a decision boundary,', 'tokens': [50952, 322, 257, 3537, 12866, 11, 558, 30, 492, 393, 7542, 341, 732, 12, 18759, 1622, 382, 257, 3537, 12866, 11, 51300], 'temperature': 0.0, 'avg_logprob': -0.06707105554383376, 'compression_ratio': 1.8098859315589353, 'no_speech_prob': 0.000766830809880048}, {'id': 227, 'seek': 127988, 'start': 1298.6000000000001, 'end': 1305.0, 'text': ' as a plane separating these two components of our space. In fact, not only is it a single plane,', 'tokens': [51300, 382, 257, 5720, 29279, 613, 732, 6677, 295, 527, 1901, 13, 682, 1186, 11, 406, 787, 307, 309, 257, 2167, 5720, 11, 51620], 'temperature': 0.0, 'avg_logprob': -0.06707105554383376, 'compression_ratio': 1.8098859315589353, 'no_speech_prob': 0.000766830809880048}, {'id': 228, 'seek': 127988, 'start': 1305.0, 'end': 1309.16, 'text': \" there's a directionality component, depending on which side of the plane that we live on.\", 'tokens': [51620, 456, 311, 257, 3513, 1860, 6542, 11, 5413, 322, 597, 1252, 295, 264, 5720, 300, 321, 1621, 322, 13, 51828], 'temperature': 0.0, 'avg_logprob': -0.06707105554383376, 'compression_ratio': 1.8098859315589353, 'no_speech_prob': 0.000766830809880048}, {'id': 229, 'seek': 130916, 'start': 1309.16, 'end': 1315.0, 'text': ' If we see an input, for example, here, negative one, two, we actually know that it lives on one', 'tokens': [50364, 759, 321, 536, 364, 4846, 11, 337, 1365, 11, 510, 11, 3671, 472, 11, 732, 11, 321, 767, 458, 300, 309, 2909, 322, 472, 50656], 'temperature': 0.0, 'avg_logprob': -0.07795185771414904, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.0011875671334564686}, {'id': 230, 'seek': 130916, 'start': 1315.0, 'end': 1320.0400000000002, 'text': ' side of the plane, and it will have a certain type of output. In this case, that output is going to be', 'tokens': [50656, 1252, 295, 264, 5720, 11, 293, 309, 486, 362, 257, 1629, 2010, 295, 5598, 13, 682, 341, 1389, 11, 300, 5598, 307, 516, 281, 312, 50908], 'temperature': 0.0, 'avg_logprob': -0.07795185771414904, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.0011875671334564686}, {'id': 231, 'seek': 130916, 'start': 1320.76, 'end': 1325.72, 'text': ' positive, right? Because in this case, when we plug those components into our equation,', 'tokens': [50944, 3353, 11, 558, 30, 1436, 294, 341, 1389, 11, 562, 321, 5452, 729, 6677, 666, 527, 5367, 11, 51192], 'temperature': 0.0, 'avg_logprob': -0.07795185771414904, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.0011875671334564686}, {'id': 232, 'seek': 130916, 'start': 1325.72, 'end': 1331.4, 'text': \" we'll get a positive number that passes through the nonlinearity component, and that gets propagated\", 'tokens': [51192, 321, 603, 483, 257, 3353, 1230, 300, 11335, 807, 264, 2107, 1889, 17409, 6542, 11, 293, 300, 2170, 12425, 770, 51476], 'temperature': 0.0, 'avg_logprob': -0.07795185771414904, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.0011875671334564686}, {'id': 233, 'seek': 130916, 'start': 1331.4, 'end': 1336.2, 'text': \" through as well. Of course, if you're on the other side of the space, you're going to have the\", 'tokens': [51476, 807, 382, 731, 13, 2720, 1164, 11, 498, 291, 434, 322, 264, 661, 1252, 295, 264, 1901, 11, 291, 434, 516, 281, 362, 264, 51716], 'temperature': 0.0, 'avg_logprob': -0.07795185771414904, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.0011875671334564686}, {'id': 234, 'seek': 133620, 'start': 1336.2, 'end': 1341.56, 'text': ' opposite result, right? That thresholding function is going to essentially live at this decision', 'tokens': [50364, 6182, 1874, 11, 558, 30, 663, 14678, 278, 2445, 307, 516, 281, 4476, 1621, 412, 341, 3537, 50632], 'temperature': 0.0, 'avg_logprob': -0.10868958814428487, 'compression_ratio': 1.7320754716981133, 'no_speech_prob': 0.0002065837470581755}, {'id': 235, 'seek': 133620, 'start': 1341.56, 'end': 1345.96, 'text': ' boundary. Depending on which side of the space you live on, that thresholding function, that', 'tokens': [50632, 12866, 13, 22539, 322, 597, 1252, 295, 264, 1901, 291, 1621, 322, 11, 300, 14678, 278, 2445, 11, 300, 50852], 'temperature': 0.0, 'avg_logprob': -0.10868958814428487, 'compression_ratio': 1.7320754716981133, 'no_speech_prob': 0.0002065837470581755}, {'id': 236, 'seek': 133620, 'start': 1345.96, 'end': 1350.76, 'text': ' sigmoid function, is going to then control how you move to one side or the other.', 'tokens': [50852, 4556, 3280, 327, 2445, 11, 307, 516, 281, 550, 1969, 577, 291, 1286, 281, 472, 1252, 420, 264, 661, 13, 51092], 'temperature': 0.0, 'avg_logprob': -0.10868958814428487, 'compression_ratio': 1.7320754716981133, 'no_speech_prob': 0.0002065837470581755}, {'id': 237, 'seek': 133620, 'start': 1352.52, 'end': 1357.48, 'text': ' Now, in this particular example, this is very convenient, because we can actually visualize,', 'tokens': [51180, 823, 11, 294, 341, 1729, 1365, 11, 341, 307, 588, 10851, 11, 570, 321, 393, 767, 23273, 11, 51428], 'temperature': 0.0, 'avg_logprob': -0.10868958814428487, 'compression_ratio': 1.7320754716981133, 'no_speech_prob': 0.0002065837470581755}, {'id': 238, 'seek': 133620, 'start': 1357.48, 'end': 1362.3600000000001, 'text': \" and I can draw this exact full space for you on this slide. It's only a two-dimensional space,\", 'tokens': [51428, 293, 286, 393, 2642, 341, 1900, 1577, 1901, 337, 291, 322, 341, 4137, 13, 467, 311, 787, 257, 732, 12, 18759, 1901, 11, 51672], 'temperature': 0.0, 'avg_logprob': -0.10868958814428487, 'compression_ratio': 1.7320754716981133, 'no_speech_prob': 0.0002065837470581755}, {'id': 239, 'seek': 136236, 'start': 1362.36, 'end': 1367.9599999999998, 'text': \" so it's very easy for us to visualize. But, of course, for almost all problems that we care about,\", 'tokens': [50364, 370, 309, 311, 588, 1858, 337, 505, 281, 23273, 13, 583, 11, 295, 1164, 11, 337, 1920, 439, 2740, 300, 321, 1127, 466, 11, 50644], 'temperature': 0.0, 'avg_logprob': -0.10711025172828609, 'compression_ratio': 1.7633587786259541, 'no_speech_prob': 0.0028870070818811655}, {'id': 240, 'seek': 136236, 'start': 1367.9599999999998, 'end': 1371.8799999999999, 'text': ' our data points are not going to be two-dimensional. If you think about an image,', 'tokens': [50644, 527, 1412, 2793, 366, 406, 516, 281, 312, 732, 12, 18759, 13, 759, 291, 519, 466, 364, 3256, 11, 50840], 'temperature': 0.0, 'avg_logprob': -0.10711025172828609, 'compression_ratio': 1.7633587786259541, 'no_speech_prob': 0.0028870070818811655}, {'id': 241, 'seek': 136236, 'start': 1372.4399999999998, 'end': 1376.6, 'text': ' the dimensionality of an image is going to be the number of pixels that you have in the image.', 'tokens': [50868, 264, 10139, 1860, 295, 364, 3256, 307, 516, 281, 312, 264, 1230, 295, 18668, 300, 291, 362, 294, 264, 3256, 13, 51076], 'temperature': 0.0, 'avg_logprob': -0.10711025172828609, 'compression_ratio': 1.7633587786259541, 'no_speech_prob': 0.0028870070818811655}, {'id': 242, 'seek': 136236, 'start': 1376.6, 'end': 1381.0, 'text': ' So these are going to be thousands of dimensions, millions of dimensions, or even more.', 'tokens': [51076, 407, 613, 366, 516, 281, 312, 5383, 295, 12819, 11, 6803, 295, 12819, 11, 420, 754, 544, 13, 51296], 'temperature': 0.0, 'avg_logprob': -0.10711025172828609, 'compression_ratio': 1.7633587786259541, 'no_speech_prob': 0.0028870070818811655}, {'id': 243, 'seek': 136236, 'start': 1381.6399999999999, 'end': 1387.24, 'text': \" And then, drawing these types of plots, like you see here, is simply not feasible. We can't always\", 'tokens': [51328, 400, 550, 11, 6316, 613, 3467, 295, 28609, 11, 411, 291, 536, 510, 11, 307, 2935, 406, 26648, 13, 492, 393, 380, 1009, 51608], 'temperature': 0.0, 'avg_logprob': -0.10711025172828609, 'compression_ratio': 1.7633587786259541, 'no_speech_prob': 0.0028870070818811655}, {'id': 244, 'seek': 138724, 'start': 1387.24, 'end': 1392.6, 'text': ' do this, but hopefully this gives you some intuition to understand, kind of, as we build up into', 'tokens': [50364, 360, 341, 11, 457, 4696, 341, 2709, 291, 512, 24002, 281, 1223, 11, 733, 295, 11, 382, 321, 1322, 493, 666, 50632], 'temperature': 0.0, 'avg_logprob': -0.09594910084700384, 'compression_ratio': 1.7745454545454546, 'no_speech_prob': 0.005814924370497465}, {'id': 245, 'seek': 138724, 'start': 1392.6, 'end': 1398.28, 'text': \" more complex models. So now that we have an idea of the perceptron, let's see how we can actually\", 'tokens': [50632, 544, 3997, 5245, 13, 407, 586, 300, 321, 362, 364, 1558, 295, 264, 43276, 2044, 11, 718, 311, 536, 577, 321, 393, 767, 50916], 'temperature': 0.0, 'avg_logprob': -0.09594910084700384, 'compression_ratio': 1.7745454545454546, 'no_speech_prob': 0.005814924370497465}, {'id': 246, 'seek': 138724, 'start': 1398.28, 'end': 1402.52, 'text': ' take this single neuron and start to build it up into something more complicated, a full neural', 'tokens': [50916, 747, 341, 2167, 34090, 293, 722, 281, 1322, 309, 493, 666, 746, 544, 6179, 11, 257, 1577, 18161, 51128], 'temperature': 0.0, 'avg_logprob': -0.09594910084700384, 'compression_ratio': 1.7745454545454546, 'no_speech_prob': 0.005814924370497465}, {'id': 247, 'seek': 138724, 'start': 1402.52, 'end': 1408.36, 'text': \" network, and build a model from that. So let's revisit, again, this previous diagram of the perceptron.\", 'tokens': [51128, 3209, 11, 293, 1322, 257, 2316, 490, 300, 13, 407, 718, 311, 32676, 11, 797, 11, 341, 3894, 10686, 295, 264, 43276, 2044, 13, 51420], 'temperature': 0.0, 'avg_logprob': -0.09594910084700384, 'compression_ratio': 1.7745454545454546, 'no_speech_prob': 0.005814924370497465}, {'id': 248, 'seek': 138724, 'start': 1408.92, 'end': 1413.72, 'text': ' If, again, just to reiterate one more time, this core piece of information that I want all of', 'tokens': [51448, 759, 11, 797, 11, 445, 281, 33528, 472, 544, 565, 11, 341, 4965, 2522, 295, 1589, 300, 286, 528, 439, 295, 51688], 'temperature': 0.0, 'avg_logprob': -0.09594910084700384, 'compression_ratio': 1.7745454545454546, 'no_speech_prob': 0.005814924370497465}, {'id': 249, 'seek': 141372, 'start': 1413.72, 'end': 1419.88, 'text': ' you to take away from this class is how a perceptron works and how it propagates information to', 'tokens': [50364, 291, 281, 747, 1314, 490, 341, 1508, 307, 577, 257, 43276, 2044, 1985, 293, 577, 309, 12425, 1024, 1589, 281, 50672], 'temperature': 0.0, 'avg_logprob': -0.11581386145898852, 'compression_ratio': 1.7, 'no_speech_prob': 0.0017532159108668566}, {'id': 250, 'seek': 141372, 'start': 1419.88, 'end': 1425.4, 'text': ' its decision. There are three steps. First is the dot product, second is the bias, and third is', 'tokens': [50672, 1080, 3537, 13, 821, 366, 1045, 4439, 13, 2386, 307, 264, 5893, 1674, 11, 1150, 307, 264, 12577, 11, 293, 2636, 307, 50948], 'temperature': 0.0, 'avg_logprob': -0.11581386145898852, 'compression_ratio': 1.7, 'no_speech_prob': 0.0017532159108668566}, {'id': 251, 'seek': 141372, 'start': 1425.4, 'end': 1429.96, 'text': ' the non-miniarity. And you keep repeating this process for every single perceptron in your neural', 'tokens': [50948, 264, 2107, 12, 2367, 72, 17409, 13, 400, 291, 1066, 18617, 341, 1399, 337, 633, 2167, 43276, 2044, 294, 428, 18161, 51176], 'temperature': 0.0, 'avg_logprob': -0.11581386145898852, 'compression_ratio': 1.7, 'no_speech_prob': 0.0017532159108668566}, {'id': 252, 'seek': 141372, 'start': 1429.96, 'end': 1436.52, 'text': \" network. Let's simplify the diagram a little bit. I'll get rid of the weights. And you can assume that\", 'tokens': [51176, 3209, 13, 961, 311, 20460, 264, 10686, 257, 707, 857, 13, 286, 603, 483, 3973, 295, 264, 17443, 13, 400, 291, 393, 6552, 300, 51504], 'temperature': 0.0, 'avg_logprob': -0.11581386145898852, 'compression_ratio': 1.7, 'no_speech_prob': 0.0017532159108668566}, {'id': 253, 'seek': 141372, 'start': 1436.52, 'end': 1441.8, 'text': \" every line here, now basically has an associated weight scalar that's associated with it. Every line\", 'tokens': [51504, 633, 1622, 510, 11, 586, 1936, 575, 364, 6615, 3364, 39684, 300, 311, 6615, 365, 309, 13, 2048, 1622, 51768], 'temperature': 0.0, 'avg_logprob': -0.11581386145898852, 'compression_ratio': 1.7, 'no_speech_prob': 0.0017532159108668566}, {'id': 254, 'seek': 144180, 'start': 1441.8799999999999, 'end': 1446.84, 'text': \" also has, it corresponds to the input that's coming in. It has a weight that's coming in also at\", 'tokens': [50368, 611, 575, 11, 309, 23249, 281, 264, 4846, 300, 311, 1348, 294, 13, 467, 575, 257, 3364, 300, 311, 1348, 294, 611, 412, 50616], 'temperature': 0.0, 'avg_logprob': -0.12234494292620317, 'compression_ratio': 1.6680851063829787, 'no_speech_prob': 0.0014320992631837726}, {'id': 255, 'seek': 144180, 'start': 1448.28, 'end': 1453.48, 'text': \" on the line itself. And I've also removed the bias just for sake of simplicity, but it's still there.\", 'tokens': [50688, 322, 264, 1622, 2564, 13, 400, 286, 600, 611, 7261, 264, 12577, 445, 337, 9717, 295, 25632, 11, 457, 309, 311, 920, 456, 13, 50948], 'temperature': 0.0, 'avg_logprob': -0.12234494292620317, 'compression_ratio': 1.6680851063829787, 'no_speech_prob': 0.0014320992631837726}, {'id': 256, 'seek': 144180, 'start': 1454.28, 'end': 1460.9199999999998, 'text': \" So now the result is that Z, which let's call that the result of our dot product plus the bias,\", 'tokens': [50988, 407, 586, 264, 1874, 307, 300, 1176, 11, 597, 718, 311, 818, 300, 264, 1874, 295, 527, 5893, 1674, 1804, 264, 12577, 11, 51320], 'temperature': 0.0, 'avg_logprob': -0.12234494292620317, 'compression_ratio': 1.6680851063829787, 'no_speech_prob': 0.0014320992631837726}, {'id': 257, 'seek': 144180, 'start': 1461.48, 'end': 1467.56, 'text': \" is going, and that's what we pass into our non-linear function, that piece is going to be applied\", 'tokens': [51348, 307, 516, 11, 293, 300, 311, 437, 321, 1320, 666, 527, 2107, 12, 28263, 2445, 11, 300, 2522, 307, 516, 281, 312, 6456, 51652], 'temperature': 0.0, 'avg_logprob': -0.12234494292620317, 'compression_ratio': 1.6680851063829787, 'no_speech_prob': 0.0014320992631837726}, {'id': 258, 'seek': 146756, 'start': 1467.56, 'end': 1474.12, 'text': ' to that activation function. Now the final output here is simply going to be G, which is our', 'tokens': [50364, 281, 300, 24433, 2445, 13, 823, 264, 2572, 5598, 510, 307, 2935, 516, 281, 312, 460, 11, 597, 307, 527, 50692], 'temperature': 0.0, 'avg_logprob': -0.07738526392791231, 'compression_ratio': 1.7252747252747254, 'no_speech_prob': 0.0010159402154386044}, {'id': 259, 'seek': 146756, 'start': 1474.12, 'end': 1479.56, 'text': ' activation function of Z, right? Z is going to be basically what you can think of the state of', 'tokens': [50692, 24433, 2445, 295, 1176, 11, 558, 30, 1176, 307, 516, 281, 312, 1936, 437, 291, 393, 519, 295, 264, 1785, 295, 50964], 'temperature': 0.0, 'avg_logprob': -0.07738526392791231, 'compression_ratio': 1.7252747252747254, 'no_speech_prob': 0.0010159402154386044}, {'id': 260, 'seek': 146756, 'start': 1479.56, 'end': 1486.36, 'text': \" this neuron. It's the result of that dot product plus bias. Now if we want to define and build up\", 'tokens': [50964, 341, 34090, 13, 467, 311, 264, 1874, 295, 300, 5893, 1674, 1804, 12577, 13, 823, 498, 321, 528, 281, 6964, 293, 1322, 493, 51304], 'temperature': 0.0, 'avg_logprob': -0.07738526392791231, 'compression_ratio': 1.7252747252747254, 'no_speech_prob': 0.0010159402154386044}, {'id': 261, 'seek': 146756, 'start': 1486.36, 'end': 1491.1599999999999, 'text': ' a multi-layered output neural network, if we want two outputs to this function, for example,', 'tokens': [51304, 257, 4825, 12, 8376, 4073, 5598, 18161, 3209, 11, 498, 321, 528, 732, 23930, 281, 341, 2445, 11, 337, 1365, 11, 51544], 'temperature': 0.0, 'avg_logprob': -0.07738526392791231, 'compression_ratio': 1.7252747252747254, 'no_speech_prob': 0.0010159402154386044}, {'id': 262, 'seek': 146756, 'start': 1491.1599999999999, 'end': 1495.8799999999999, 'text': \" it's a very simple procedure. We just have now two neurons, two perceptrons. Each perceptron\", 'tokens': [51544, 309, 311, 257, 588, 2199, 10747, 13, 492, 445, 362, 586, 732, 22027, 11, 732, 43276, 13270, 13, 6947, 43276, 2044, 51780], 'temperature': 0.0, 'avg_logprob': -0.07738526392791231, 'compression_ratio': 1.7252747252747254, 'no_speech_prob': 0.0010159402154386044}, {'id': 263, 'seek': 149588, 'start': 1495.88, 'end': 1501.96, 'text': ' will control the output for its associated piece, right? So now we have two outputs. Each one is a', 'tokens': [50364, 486, 1969, 264, 5598, 337, 1080, 6615, 2522, 11, 558, 30, 407, 586, 321, 362, 732, 23930, 13, 6947, 472, 307, 257, 50668], 'temperature': 0.0, 'avg_logprob': -0.08449513571602958, 'compression_ratio': 1.7007042253521127, 'no_speech_prob': 0.0006873965030536056}, {'id': 264, 'seek': 149588, 'start': 1501.96, 'end': 1507.0800000000002, 'text': ' normal perceptron. It takes all of the inputs, so they both take the same inputs, but amazingly,', 'tokens': [50668, 2710, 43276, 2044, 13, 467, 2516, 439, 295, 264, 15743, 11, 370, 436, 1293, 747, 264, 912, 15743, 11, 457, 31762, 11, 50924], 'temperature': 0.0, 'avg_logprob': -0.08449513571602958, 'compression_ratio': 1.7007042253521127, 'no_speech_prob': 0.0006873965030536056}, {'id': 265, 'seek': 149588, 'start': 1507.0800000000002, 'end': 1512.6000000000001, 'text': ' now with this mathematical understanding, we can start to build our first neural network entirely', 'tokens': [50924, 586, 365, 341, 18894, 3701, 11, 321, 393, 722, 281, 1322, 527, 700, 18161, 3209, 7696, 51200], 'temperature': 0.0, 'avg_logprob': -0.08449513571602958, 'compression_ratio': 1.7007042253521127, 'no_speech_prob': 0.0006873965030536056}, {'id': 266, 'seek': 149588, 'start': 1512.6000000000001, 'end': 1518.1200000000001, 'text': ' from scratch. So what does that look like? So we can start by firstly initializing these two', 'tokens': [51200, 490, 8459, 13, 407, 437, 775, 300, 574, 411, 30, 407, 321, 393, 722, 538, 27376, 5883, 3319, 613, 732, 51476], 'temperature': 0.0, 'avg_logprob': -0.08449513571602958, 'compression_ratio': 1.7007042253521127, 'no_speech_prob': 0.0006873965030536056}, {'id': 267, 'seek': 149588, 'start': 1518.1200000000001, 'end': 1523.0, 'text': ' components. The first component that we saw was the weight matrix, excuse me, the weight vector.', 'tokens': [51476, 6677, 13, 440, 700, 6542, 300, 321, 1866, 390, 264, 3364, 8141, 11, 8960, 385, 11, 264, 3364, 8062, 13, 51720], 'temperature': 0.0, 'avg_logprob': -0.08449513571602958, 'compression_ratio': 1.7007042253521127, 'no_speech_prob': 0.0006873965030536056}, {'id': 268, 'seek': 152300, 'start': 1523.0, 'end': 1530.2, 'text': \" It's a vector of weights, in this case. And the second component is the bias vector that we're\", 'tokens': [50364, 467, 311, 257, 8062, 295, 17443, 11, 294, 341, 1389, 13, 400, 264, 1150, 6542, 307, 264, 12577, 8062, 300, 321, 434, 50724], 'temperature': 0.0, 'avg_logprob': -0.09302898001881828, 'compression_ratio': 1.75, 'no_speech_prob': 0.001896020257845521}, {'id': 269, 'seek': 152300, 'start': 1530.2, 'end': 1536.44, 'text': ' going to multiply with the dot product of all of our inputs by our weights, right? So the only', 'tokens': [50724, 516, 281, 12972, 365, 264, 5893, 1674, 295, 439, 295, 527, 15743, 538, 527, 17443, 11, 558, 30, 407, 264, 787, 51036], 'temperature': 0.0, 'avg_logprob': -0.09302898001881828, 'compression_ratio': 1.75, 'no_speech_prob': 0.001896020257845521}, {'id': 270, 'seek': 152300, 'start': 1536.44, 'end': 1542.68, 'text': \" remaining step now after we've defined these parameters of our layer is to now define, you know,\", 'tokens': [51036, 8877, 1823, 586, 934, 321, 600, 7642, 613, 9834, 295, 527, 4583, 307, 281, 586, 6964, 11, 291, 458, 11, 51348], 'temperature': 0.0, 'avg_logprob': -0.09302898001881828, 'compression_ratio': 1.75, 'no_speech_prob': 0.001896020257845521}, {'id': 271, 'seek': 152300, 'start': 1542.68, 'end': 1547.16, 'text': \" how this forward propagation of information works. And that's exactly those three main components\", 'tokens': [51348, 577, 341, 2128, 38377, 295, 1589, 1985, 13, 400, 300, 311, 2293, 729, 1045, 2135, 6677, 51572], 'temperature': 0.0, 'avg_logprob': -0.09302898001881828, 'compression_ratio': 1.75, 'no_speech_prob': 0.001896020257845521}, {'id': 272, 'seek': 152300, 'start': 1547.16, 'end': 1552.68, 'text': \" that I've been stressing to you. So we can create this call function to do exactly that, to define\", 'tokens': [51572, 300, 286, 600, 668, 48233, 281, 291, 13, 407, 321, 393, 1884, 341, 818, 2445, 281, 360, 2293, 300, 11, 281, 6964, 51848], 'temperature': 0.0, 'avg_logprob': -0.09302898001881828, 'compression_ratio': 1.75, 'no_speech_prob': 0.001896020257845521}, {'id': 273, 'seek': 155268, 'start': 1552.76, 'end': 1557.4, 'text': \" this forward propagation of information. And the story here is exactly the same as we've been\", 'tokens': [50368, 341, 2128, 38377, 295, 1589, 13, 400, 264, 1657, 510, 307, 2293, 264, 912, 382, 321, 600, 668, 50600], 'temperature': 0.0, 'avg_logprob': -0.12805334996368925, 'compression_ratio': 1.7, 'no_speech_prob': 0.0009542771149426699}, {'id': 274, 'seek': 155268, 'start': 1557.4, 'end': 1565.64, 'text': ' seeing it, right? Matrix multiply our inputs with our weights, right? Add a bias and then apply a', 'tokens': [50600, 2577, 309, 11, 558, 30, 36274, 12972, 527, 15743, 365, 527, 17443, 11, 558, 30, 5349, 257, 12577, 293, 550, 3079, 257, 51012], 'temperature': 0.0, 'avg_logprob': -0.12805334996368925, 'compression_ratio': 1.7, 'no_speech_prob': 0.0009542771149426699}, {'id': 275, 'seek': 155268, 'start': 1565.64, 'end': 1571.0800000000002, 'text': ' non-linearity and return the result, right? And that literally, this code will run. This will define', 'tokens': [51012, 2107, 12, 1889, 17409, 293, 2736, 264, 1874, 11, 558, 30, 400, 300, 3736, 11, 341, 3089, 486, 1190, 13, 639, 486, 6964, 51284], 'temperature': 0.0, 'avg_logprob': -0.12805334996368925, 'compression_ratio': 1.7, 'no_speech_prob': 0.0009542771149426699}, {'id': 276, 'seek': 155268, 'start': 1571.0800000000002, 'end': 1578.1200000000001, 'text': ' a full neural network layer that you can then take like this. And of course, actually,', 'tokens': [51284, 257, 1577, 18161, 3209, 4583, 300, 291, 393, 550, 747, 411, 341, 13, 400, 295, 1164, 11, 767, 11, 51636], 'temperature': 0.0, 'avg_logprob': -0.12805334996368925, 'compression_ratio': 1.7, 'no_speech_prob': 0.0009542771149426699}, {'id': 277, 'seek': 155268, 'start': 1578.1200000000001, 'end': 1582.6000000000001, 'text': \" luckily for all of you, all of that code, which wasn't much code, that's been abstracted away by\", 'tokens': [51636, 22880, 337, 439, 295, 291, 11, 439, 295, 300, 3089, 11, 597, 2067, 380, 709, 3089, 11, 300, 311, 668, 12649, 292, 1314, 538, 51860], 'temperature': 0.0, 'avg_logprob': -0.12805334996368925, 'compression_ratio': 1.7, 'no_speech_prob': 0.0009542771149426699}, {'id': 278, 'seek': 158260, 'start': 1582.6799999999998, 'end': 1587.3999999999999, 'text': ' these libraries like TensorFlow, you can simply call functions like this, which will actually,', 'tokens': [50368, 613, 15148, 411, 37624, 11, 291, 393, 2935, 818, 6828, 411, 341, 11, 597, 486, 767, 11, 50604], 'temperature': 0.0, 'avg_logprob': -0.0937581966663229, 'compression_ratio': 1.710801393728223, 'no_speech_prob': 0.00043042932520620525}, {'id': 279, 'seek': 158260, 'start': 1587.3999999999999, 'end': 1592.6, 'text': \" you know, replicate exactly that piece of code. So you don't need to necessarily copy all of that\", 'tokens': [50604, 291, 458, 11, 25356, 2293, 300, 2522, 295, 3089, 13, 407, 291, 500, 380, 643, 281, 4725, 5055, 439, 295, 300, 50864], 'temperature': 0.0, 'avg_logprob': -0.0937581966663229, 'compression_ratio': 1.710801393728223, 'no_speech_prob': 0.00043042932520620525}, {'id': 280, 'seek': 158260, 'start': 1592.6, 'end': 1599.6399999999999, 'text': ' code down. You just, you can just call it. And with that understanding, you know, we just saw', 'tokens': [50864, 3089, 760, 13, 509, 445, 11, 291, 393, 445, 818, 309, 13, 400, 365, 300, 3701, 11, 291, 458, 11, 321, 445, 1866, 51216], 'temperature': 0.0, 'avg_logprob': -0.0937581966663229, 'compression_ratio': 1.710801393728223, 'no_speech_prob': 0.00043042932520620525}, {'id': 281, 'seek': 158260, 'start': 1599.6399999999999, 'end': 1604.36, 'text': ' how you could build a single layer. But of course, now you can actually start to think about how you', 'tokens': [51216, 577, 291, 727, 1322, 257, 2167, 4583, 13, 583, 295, 1164, 11, 586, 291, 393, 767, 722, 281, 519, 466, 577, 291, 51452], 'temperature': 0.0, 'avg_logprob': -0.0937581966663229, 'compression_ratio': 1.710801393728223, 'no_speech_prob': 0.00043042932520620525}, {'id': 282, 'seek': 158260, 'start': 1604.36, 'end': 1611.8799999999999, 'text': ' can stack these layers as well. So since we now have this transformation, essentially, from our inputs,', 'tokens': [51452, 393, 8630, 613, 7914, 382, 731, 13, 407, 1670, 321, 586, 362, 341, 9887, 11, 4476, 11, 490, 527, 15743, 11, 51828], 'temperature': 0.0, 'avg_logprob': -0.0937581966663229, 'compression_ratio': 1.710801393728223, 'no_speech_prob': 0.00043042932520620525}, {'id': 283, 'seek': 161188, 'start': 1611.88, 'end': 1619.72, 'text': ' to a hidden output, you can think of this as basically how we can define some way of transforming', 'tokens': [50364, 281, 257, 7633, 5598, 11, 291, 393, 519, 295, 341, 382, 1936, 577, 321, 393, 6964, 512, 636, 295, 27210, 50756], 'temperature': 0.0, 'avg_logprob': -0.0608330860472562, 'compression_ratio': 1.7526501766784452, 'no_speech_prob': 0.0016221243422478437}, {'id': 284, 'seek': 161188, 'start': 1619.72, 'end': 1626.2800000000002, 'text': ' those inputs, right, into some new dimensional space, right? Perhaps closer to the value that we', 'tokens': [50756, 729, 15743, 11, 558, 11, 666, 512, 777, 18795, 1901, 11, 558, 30, 10517, 4966, 281, 264, 2158, 300, 321, 51084], 'temperature': 0.0, 'avg_logprob': -0.0608330860472562, 'compression_ratio': 1.7526501766784452, 'no_speech_prob': 0.0016221243422478437}, {'id': 285, 'seek': 161188, 'start': 1626.2800000000002, 'end': 1631.5600000000002, 'text': ' want to predict. And that transformation is going to be eventually learned to know how to transform', 'tokens': [51084, 528, 281, 6069, 13, 400, 300, 9887, 307, 516, 281, 312, 4728, 3264, 281, 458, 577, 281, 4088, 51348], 'temperature': 0.0, 'avg_logprob': -0.0608330860472562, 'compression_ratio': 1.7526501766784452, 'no_speech_prob': 0.0016221243422478437}, {'id': 286, 'seek': 161188, 'start': 1631.5600000000002, 'end': 1636.5200000000002, 'text': \" those inputs into our desired outputs. And we'll get to that later. But for now, the piece that I want\", 'tokens': [51348, 729, 15743, 666, 527, 14721, 23930, 13, 400, 321, 603, 483, 281, 300, 1780, 13, 583, 337, 586, 11, 264, 2522, 300, 286, 528, 51596], 'temperature': 0.0, 'avg_logprob': -0.0608330860472562, 'compression_ratio': 1.7526501766784452, 'no_speech_prob': 0.0016221243422478437}, {'id': 287, 'seek': 161188, 'start': 1636.5200000000002, 'end': 1641.48, 'text': ' to really focus on is if we have these more complex neural networks, I want to really distill down', 'tokens': [51596, 281, 534, 1879, 322, 307, 498, 321, 362, 613, 544, 3997, 18161, 9590, 11, 286, 528, 281, 534, 42923, 760, 51844], 'temperature': 0.0, 'avg_logprob': -0.0608330860472562, 'compression_ratio': 1.7526501766784452, 'no_speech_prob': 0.0016221243422478437}, {'id': 288, 'seek': 164148, 'start': 1641.48, 'end': 1646.52, 'text': \" that this is nothing more complex than what we've already seen. If we focus on just one neuron in\", 'tokens': [50364, 300, 341, 307, 1825, 544, 3997, 813, 437, 321, 600, 1217, 1612, 13, 759, 321, 1879, 322, 445, 472, 34090, 294, 50616], 'temperature': 0.0, 'avg_logprob': -0.11233979145079169, 'compression_ratio': 1.6952054794520548, 'no_speech_prob': 0.0004044391098432243}, {'id': 289, 'seek': 164148, 'start': 1646.52, 'end': 1653.08, 'text': \" this diagram, take a, here, for example, Z2, right? Z2 is this neuron that's highlighted in the\", 'tokens': [50616, 341, 10686, 11, 747, 257, 11, 510, 11, 337, 1365, 11, 1176, 17, 11, 558, 30, 1176, 17, 307, 341, 34090, 300, 311, 17173, 294, 264, 50944], 'temperature': 0.0, 'avg_logprob': -0.11233979145079169, 'compression_ratio': 1.6952054794520548, 'no_speech_prob': 0.0004044391098432243}, {'id': 290, 'seek': 164148, 'start': 1653.08, 'end': 1659.0, 'text': \" middle layer. It's just the same perceptron that we've been seeing so far in this class. It was\", 'tokens': [50944, 2808, 4583, 13, 467, 311, 445, 264, 912, 43276, 2044, 300, 321, 600, 668, 2577, 370, 1400, 294, 341, 1508, 13, 467, 390, 51240], 'temperature': 0.0, 'avg_logprob': -0.11233979145079169, 'compression_ratio': 1.6952054794520548, 'no_speech_prob': 0.0004044391098432243}, {'id': 291, 'seek': 164148, 'start': 1659.0, 'end': 1664.1200000000001, 'text': \" it's output is obtained by taking a dot product, adding a bias, and then applying that non-linearity\", 'tokens': [51240, 309, 311, 5598, 307, 14879, 538, 1940, 257, 5893, 1674, 11, 5127, 257, 12577, 11, 293, 550, 9275, 300, 2107, 12, 1889, 17409, 51496], 'temperature': 0.0, 'avg_logprob': -0.11233979145079169, 'compression_ratio': 1.6952054794520548, 'no_speech_prob': 0.0004044391098432243}, {'id': 292, 'seek': 164148, 'start': 1664.1200000000001, 'end': 1669.48, 'text': ' between all of its inputs. If we look at a different node, for example, Z3, which is the one right below', 'tokens': [51496, 1296, 439, 295, 1080, 15743, 13, 759, 321, 574, 412, 257, 819, 9984, 11, 337, 1365, 11, 1176, 18, 11, 597, 307, 264, 472, 558, 2507, 51764], 'temperature': 0.0, 'avg_logprob': -0.11233979145079169, 'compression_ratio': 1.6952054794520548, 'no_speech_prob': 0.0004044391098432243}, {'id': 293, 'seek': 166948, 'start': 1669.96, 'end': 1673.8, 'text': \" it, it's the exact same story again. It sees all the same inputs, but it has a different set of\", 'tokens': [50388, 309, 11, 309, 311, 264, 1900, 912, 1657, 797, 13, 467, 8194, 439, 264, 912, 15743, 11, 457, 309, 575, 257, 819, 992, 295, 50580], 'temperature': 0.0, 'avg_logprob': -0.08803776840665448, 'compression_ratio': 1.878594249201278, 'no_speech_prob': 0.0020183383021503687}, {'id': 294, 'seek': 166948, 'start': 1673.8, 'end': 1678.76, 'text': \" weight matrix that it's going to apply to those inputs. So we'll have a different output. But the\", 'tokens': [50580, 3364, 8141, 300, 309, 311, 516, 281, 3079, 281, 729, 15743, 13, 407, 321, 603, 362, 257, 819, 5598, 13, 583, 264, 50828], 'temperature': 0.0, 'avg_logprob': -0.08803776840665448, 'compression_ratio': 1.878594249201278, 'no_speech_prob': 0.0020183383021503687}, {'id': 295, 'seek': 166948, 'start': 1678.76, 'end': 1683.56, 'text': \" mathematically equations are exactly the same. So from now on, I'm just going to kind of simplify\", 'tokens': [50828, 44003, 11787, 366, 2293, 264, 912, 13, 407, 490, 586, 322, 11, 286, 478, 445, 516, 281, 733, 295, 20460, 51068], 'temperature': 0.0, 'avg_logprob': -0.08803776840665448, 'compression_ratio': 1.878594249201278, 'no_speech_prob': 0.0020183383021503687}, {'id': 296, 'seek': 166948, 'start': 1683.56, 'end': 1688.76, 'text': ' all of these lines and diagrams just to show these icons in the middle just to demonstrate that', 'tokens': [51068, 439, 295, 613, 3876, 293, 36709, 445, 281, 855, 613, 23308, 294, 264, 2808, 445, 281, 11698, 300, 51328], 'temperature': 0.0, 'avg_logprob': -0.08803776840665448, 'compression_ratio': 1.878594249201278, 'no_speech_prob': 0.0020183383021503687}, {'id': 297, 'seek': 166948, 'start': 1688.76, 'end': 1693.08, 'text': ' these means everything is going to fully connect it to everything and defined by those mathematical', 'tokens': [51328, 613, 1355, 1203, 307, 516, 281, 4498, 1745, 309, 281, 1203, 293, 7642, 538, 729, 18894, 51544], 'temperature': 0.0, 'avg_logprob': -0.08803776840665448, 'compression_ratio': 1.878594249201278, 'no_speech_prob': 0.0020183383021503687}, {'id': 298, 'seek': 166948, 'start': 1693.08, 'end': 1698.2, 'text': \" equations that we've been covering. But there's no extra complexity in these models from what you've\", 'tokens': [51544, 11787, 300, 321, 600, 668, 10322, 13, 583, 456, 311, 572, 2857, 14024, 294, 613, 5245, 490, 437, 291, 600, 51800], 'temperature': 0.0, 'avg_logprob': -0.08803776840665448, 'compression_ratio': 1.878594249201278, 'no_speech_prob': 0.0020183383021503687}, {'id': 299, 'seek': 169820, 'start': 1698.2, 'end': 1704.2, 'text': ' already seen. Now, if you want to stack these types of solutions on top of each other, these', 'tokens': [50364, 1217, 1612, 13, 823, 11, 498, 291, 528, 281, 8630, 613, 3467, 295, 6547, 322, 1192, 295, 1184, 661, 11, 613, 50664], 'temperature': 0.0, 'avg_logprob': -0.0605198100761131, 'compression_ratio': 1.908366533864542, 'no_speech_prob': 0.00043727882439270616}, {'id': 300, 'seek': 169820, 'start': 1704.2, 'end': 1708.8400000000001, 'text': ' layers on top of each other, you can not only define one layer very easily, but you can actually', 'tokens': [50664, 7914, 322, 1192, 295, 1184, 661, 11, 291, 393, 406, 787, 6964, 472, 4583, 588, 3612, 11, 457, 291, 393, 767, 50896], 'temperature': 0.0, 'avg_logprob': -0.0605198100761131, 'compression_ratio': 1.908366533864542, 'no_speech_prob': 0.00043727882439270616}, {'id': 301, 'seek': 169820, 'start': 1708.8400000000001, 'end': 1713.48, 'text': ' create what are called sequential models. These sequential models, you can define one layer after', 'tokens': [50896, 1884, 437, 366, 1219, 42881, 5245, 13, 1981, 42881, 5245, 11, 291, 393, 6964, 472, 4583, 934, 51128], 'temperature': 0.0, 'avg_logprob': -0.0605198100761131, 'compression_ratio': 1.908366533864542, 'no_speech_prob': 0.00043727882439270616}, {'id': 302, 'seek': 169820, 'start': 1713.48, 'end': 1718.92, 'text': ' another, and they define basically the forward propagation of information, not just from the neuron', 'tokens': [51128, 1071, 11, 293, 436, 6964, 1936, 264, 2128, 38377, 295, 1589, 11, 406, 445, 490, 264, 34090, 51400], 'temperature': 0.0, 'avg_logprob': -0.0605198100761131, 'compression_ratio': 1.908366533864542, 'no_speech_prob': 0.00043727882439270616}, {'id': 303, 'seek': 169820, 'start': 1718.92, 'end': 1723.88, 'text': ' level, but now from the layer level. Every layer will be fully connected to the next layer,', 'tokens': [51400, 1496, 11, 457, 586, 490, 264, 4583, 1496, 13, 2048, 4583, 486, 312, 4498, 4582, 281, 264, 958, 4583, 11, 51648], 'temperature': 0.0, 'avg_logprob': -0.0605198100761131, 'compression_ratio': 1.908366533864542, 'no_speech_prob': 0.00043727882439270616}, {'id': 304, 'seek': 172388, 'start': 1723.88, 'end': 1728.3600000000001, 'text': ' and the inputs of the secondary layer will be all of the outputs of the prior layer.', 'tokens': [50364, 293, 264, 15743, 295, 264, 11396, 4583, 486, 312, 439, 295, 264, 23930, 295, 264, 4059, 4583, 13, 50588], 'temperature': 0.0, 'avg_logprob': -0.09981254909349524, 'compression_ratio': 1.7840909090909092, 'no_speech_prob': 0.0018962319009006023}, {'id': 305, 'seek': 172388, 'start': 1730.0400000000002, 'end': 1734.2800000000002, 'text': ' Now, of course, if you want to create a very deep neural network, all the deep neural network is,', 'tokens': [50672, 823, 11, 295, 1164, 11, 498, 291, 528, 281, 1884, 257, 588, 2452, 18161, 3209, 11, 439, 264, 2452, 18161, 3209, 307, 11, 50884], 'temperature': 0.0, 'avg_logprob': -0.09981254909349524, 'compression_ratio': 1.7840909090909092, 'no_speech_prob': 0.0018962319009006023}, {'id': 306, 'seek': 172388, 'start': 1734.2800000000002, 'end': 1738.44, 'text': \" is we just keep stacking these layers on top of each other. There's nothing else to this story.\", 'tokens': [50884, 307, 321, 445, 1066, 41376, 613, 7914, 322, 1192, 295, 1184, 661, 13, 821, 311, 1825, 1646, 281, 341, 1657, 13, 51092], 'temperature': 0.0, 'avg_logprob': -0.09981254909349524, 'compression_ratio': 1.7840909090909092, 'no_speech_prob': 0.0018962319009006023}, {'id': 307, 'seek': 172388, 'start': 1738.44, 'end': 1744.1200000000001, 'text': \" That's really as simple as it is. Once, so these layers are basically all they are, it's just layers\", 'tokens': [51092, 663, 311, 534, 382, 2199, 382, 309, 307, 13, 3443, 11, 370, 613, 7914, 366, 1936, 439, 436, 366, 11, 309, 311, 445, 7914, 51376], 'temperature': 0.0, 'avg_logprob': -0.09981254909349524, 'compression_ratio': 1.7840909090909092, 'no_speech_prob': 0.0018962319009006023}, {'id': 308, 'seek': 172388, 'start': 1744.1200000000001, 'end': 1749.96, 'text': ' where the final output is computed, right, by going deeper and deeper into this progression', 'tokens': [51376, 689, 264, 2572, 5598, 307, 40610, 11, 558, 11, 538, 516, 7731, 293, 7731, 666, 341, 18733, 51668], 'temperature': 0.0, 'avg_logprob': -0.09981254909349524, 'compression_ratio': 1.7840909090909092, 'no_speech_prob': 0.0018962319009006023}, {'id': 309, 'seek': 174996, 'start': 1750.04, 'end': 1754.04, 'text': ' of different layers, right, and you just keep stacking them until you get to the last layer,', 'tokens': [50368, 295, 819, 7914, 11, 558, 11, 293, 291, 445, 1066, 41376, 552, 1826, 291, 483, 281, 264, 1036, 4583, 11, 50568], 'temperature': 0.0, 'avg_logprob': -0.09673422755617084, 'compression_ratio': 1.7620578778135048, 'no_speech_prob': 0.002630263101309538}, {'id': 310, 'seek': 174996, 'start': 1754.04, 'end': 1757.0, 'text': \" which is your output layer. It's your final prediction that you want to output.\", 'tokens': [50568, 597, 307, 428, 5598, 4583, 13, 467, 311, 428, 2572, 17630, 300, 291, 528, 281, 5598, 13, 50716], 'temperature': 0.0, 'avg_logprob': -0.09673422755617084, 'compression_ratio': 1.7620578778135048, 'no_speech_prob': 0.002630263101309538}, {'id': 311, 'seek': 174996, 'start': 1758.44, 'end': 1762.3600000000001, 'text': ' Right, we can create a deep neural network to do all of this by stacking these layers and', 'tokens': [50788, 1779, 11, 321, 393, 1884, 257, 2452, 18161, 3209, 281, 360, 439, 295, 341, 538, 41376, 613, 7914, 293, 50984], 'temperature': 0.0, 'avg_logprob': -0.09673422755617084, 'compression_ratio': 1.7620578778135048, 'no_speech_prob': 0.002630263101309538}, {'id': 312, 'seek': 174996, 'start': 1762.3600000000001, 'end': 1767.64, 'text': \" creating these more hierarchical models, like we saw very early in the beginning of today's lecture.\", 'tokens': [50984, 4084, 613, 544, 35250, 804, 5245, 11, 411, 321, 1866, 588, 2440, 294, 264, 2863, 295, 965, 311, 7991, 13, 51248], 'temperature': 0.0, 'avg_logprob': -0.09673422755617084, 'compression_ratio': 1.7620578778135048, 'no_speech_prob': 0.002630263101309538}, {'id': 313, 'seek': 174996, 'start': 1767.64, 'end': 1772.68, 'text': ' One where the final output is really computed by, you know, just going deeper and deeper into this', 'tokens': [51248, 1485, 689, 264, 2572, 5598, 307, 534, 40610, 538, 11, 291, 458, 11, 445, 516, 7731, 293, 7731, 666, 341, 51500], 'temperature': 0.0, 'avg_logprob': -0.09673422755617084, 'compression_ratio': 1.7620578778135048, 'no_speech_prob': 0.002630263101309538}, {'id': 314, 'seek': 174996, 'start': 1772.68, 'end': 1779.88, 'text': \" system. Okay, so that's awesome. So we've now seen how we can go from a single neuron\", 'tokens': [51500, 1185, 13, 1033, 11, 370, 300, 311, 3476, 13, 407, 321, 600, 586, 1612, 577, 321, 393, 352, 490, 257, 2167, 34090, 51860], 'temperature': 0.0, 'avg_logprob': -0.09673422755617084, 'compression_ratio': 1.7620578778135048, 'no_speech_prob': 0.002630263101309538}, {'id': 315, 'seek': 177988, 'start': 1780.0400000000002, 'end': 1785.0, 'text': ' to a layer to all the way to a deep neural network, right, building off of these foundational', 'tokens': [50372, 281, 257, 4583, 281, 439, 264, 636, 281, 257, 2452, 18161, 3209, 11, 558, 11, 2390, 766, 295, 613, 32195, 50620], 'temperature': 0.0, 'avg_logprob': -0.09389500032391465, 'compression_ratio': 1.7279411764705883, 'no_speech_prob': 0.0003352342755533755}, {'id': 316, 'seek': 177988, 'start': 1785.0, 'end': 1792.44, 'text': \" principles. Let's take a look at how exactly we can use these, you know, principles that we've\", 'tokens': [50620, 9156, 13, 961, 311, 747, 257, 574, 412, 577, 2293, 321, 393, 764, 613, 11, 291, 458, 11, 9156, 300, 321, 600, 50992], 'temperature': 0.0, 'avg_logprob': -0.09389500032391465, 'compression_ratio': 1.7279411764705883, 'no_speech_prob': 0.0003352342755533755}, {'id': 317, 'seek': 177988, 'start': 1792.44, 'end': 1797.5600000000002, 'text': ' just discussed to solve a very real problem that I think all of you are probably very concerned about', 'tokens': [50992, 445, 7152, 281, 5039, 257, 588, 957, 1154, 300, 286, 519, 439, 295, 291, 366, 1391, 588, 5922, 466, 51248], 'temperature': 0.0, 'avg_logprob': -0.09389500032391465, 'compression_ratio': 1.7279411764705883, 'no_speech_prob': 0.0003352342755533755}, {'id': 318, 'seek': 177988, 'start': 1798.68, 'end': 1804.5200000000002, 'text': ' this morning when you when you woke up. So that problem is how we can build a neural network', 'tokens': [51304, 341, 2446, 562, 291, 562, 291, 12852, 493, 13, 407, 300, 1154, 307, 577, 321, 393, 1322, 257, 18161, 3209, 51596], 'temperature': 0.0, 'avg_logprob': -0.09389500032391465, 'compression_ratio': 1.7279411764705883, 'no_speech_prob': 0.0003352342755533755}, {'id': 319, 'seek': 177988, 'start': 1804.5200000000002, 'end': 1809.24, 'text': ' to answer this question, which is, will I pass this class and if I will or will I not?', 'tokens': [51596, 281, 1867, 341, 1168, 11, 597, 307, 11, 486, 286, 1320, 341, 1508, 293, 498, 286, 486, 420, 486, 286, 406, 30, 51832], 'temperature': 0.0, 'avg_logprob': -0.09389500032391465, 'compression_ratio': 1.7279411764705883, 'no_speech_prob': 0.0003352342755533755}, {'id': 320, 'seek': 180988, 'start': 1810.44, 'end': 1815.24, 'text': \" So to answer this question, let's see if we can train a neural network to solve this problem.\", 'tokens': [50392, 407, 281, 1867, 341, 1168, 11, 718, 311, 536, 498, 321, 393, 3847, 257, 18161, 3209, 281, 5039, 341, 1154, 13, 50632], 'temperature': 0.0, 'avg_logprob': -0.07829432409317767, 'compression_ratio': 1.8, 'no_speech_prob': 0.0008293313439935446}, {'id': 321, 'seek': 180988, 'start': 1815.24, 'end': 1820.92, 'text': \" Okay, so to do this, let's start with a very simple neural network, right, we'll train this model\", 'tokens': [50632, 1033, 11, 370, 281, 360, 341, 11, 718, 311, 722, 365, 257, 588, 2199, 18161, 3209, 11, 558, 11, 321, 603, 3847, 341, 2316, 50916], 'temperature': 0.0, 'avg_logprob': -0.07829432409317767, 'compression_ratio': 1.8, 'no_speech_prob': 0.0008293313439935446}, {'id': 322, 'seek': 180988, 'start': 1820.92, 'end': 1825.64, 'text': ' with two inputs, just two inputs. One input is going to be the number of lectures that you attend', 'tokens': [50916, 365, 732, 15743, 11, 445, 732, 15743, 13, 1485, 4846, 307, 516, 281, 312, 264, 1230, 295, 16564, 300, 291, 6888, 51152], 'temperature': 0.0, 'avg_logprob': -0.07829432409317767, 'compression_ratio': 1.8, 'no_speech_prob': 0.0008293313439935446}, {'id': 323, 'seek': 180988, 'start': 1825.64, 'end': 1830.92, 'text': ' over the course of this one week. And the second input is going to be how many hours that you spend', 'tokens': [51152, 670, 264, 1164, 295, 341, 472, 1243, 13, 400, 264, 1150, 4846, 307, 516, 281, 312, 577, 867, 2496, 300, 291, 3496, 51416], 'temperature': 0.0, 'avg_logprob': -0.07829432409317767, 'compression_ratio': 1.8, 'no_speech_prob': 0.0008293313439935446}, {'id': 324, 'seek': 180988, 'start': 1830.92, 'end': 1837.3200000000002, 'text': \" on your final project or your competition. Okay, so what we're going to do is firstly go out and\", 'tokens': [51416, 322, 428, 2572, 1716, 420, 428, 6211, 13, 1033, 11, 370, 437, 321, 434, 516, 281, 360, 307, 27376, 352, 484, 293, 51736], 'temperature': 0.0, 'avg_logprob': -0.07829432409317767, 'compression_ratio': 1.8, 'no_speech_prob': 0.0008293313439935446}, {'id': 325, 'seek': 183732, 'start': 1837.32, 'end': 1841.48, 'text': \" collect a lot of data from all of the past years that we've taught this course. And we can plot all\", 'tokens': [50364, 2500, 257, 688, 295, 1412, 490, 439, 295, 264, 1791, 924, 300, 321, 600, 5928, 341, 1164, 13, 400, 321, 393, 7542, 439, 50572], 'temperature': 0.0, 'avg_logprob': -0.12700286075986664, 'compression_ratio': 1.9052287581699345, 'no_speech_prob': 0.001098139677196741}, {'id': 326, 'seek': 183732, 'start': 1841.48, 'end': 1845.56, 'text': \" of this data because it's only two input space, we can plot this data on a two-dimensional\", 'tokens': [50572, 295, 341, 1412, 570, 309, 311, 787, 732, 4846, 1901, 11, 321, 393, 7542, 341, 1412, 322, 257, 732, 12, 18759, 50776], 'temperature': 0.0, 'avg_logprob': -0.12700286075986664, 'compression_ratio': 1.9052287581699345, 'no_speech_prob': 0.001098139677196741}, {'id': 327, 'seek': 183732, 'start': 1845.56, 'end': 1850.84, 'text': ' feature space, right. We can actually look at all of the students before you that have passed the', 'tokens': [50776, 4111, 1901, 11, 558, 13, 492, 393, 767, 574, 412, 439, 295, 264, 1731, 949, 291, 300, 362, 4678, 264, 51040], 'temperature': 0.0, 'avg_logprob': -0.12700286075986664, 'compression_ratio': 1.9052287581699345, 'no_speech_prob': 0.001098139677196741}, {'id': 328, 'seek': 183732, 'start': 1850.84, 'end': 1855.8, 'text': ' class and failed the class and see where they lived in this space for the amount of hours that they', 'tokens': [51040, 1508, 293, 7612, 264, 1508, 293, 536, 689, 436, 5152, 294, 341, 1901, 337, 264, 2372, 295, 2496, 300, 436, 51288], 'temperature': 0.0, 'avg_logprob': -0.12700286075986664, 'compression_ratio': 1.9052287581699345, 'no_speech_prob': 0.001098139677196741}, {'id': 329, 'seek': 183732, 'start': 1855.8, 'end': 1859.8799999999999, 'text': \" spent, the number of lectures that they've attended and so on. Green points are the people who have\", 'tokens': [51288, 4418, 11, 264, 1230, 295, 16564, 300, 436, 600, 15990, 293, 370, 322, 13, 6969, 2793, 366, 264, 561, 567, 362, 51492], 'temperature': 0.0, 'avg_logprob': -0.12700286075986664, 'compression_ratio': 1.9052287581699345, 'no_speech_prob': 0.001098139677196741}, {'id': 330, 'seek': 183732, 'start': 1859.8799999999999, 'end': 1865.8, 'text': \" passed, read, or those who have failed. Now, and here's you, right, you're right here. Four or\", 'tokens': [51492, 4678, 11, 1401, 11, 420, 729, 567, 362, 7612, 13, 823, 11, 293, 510, 311, 291, 11, 558, 11, 291, 434, 558, 510, 13, 7451, 420, 51788], 'temperature': 0.0, 'avg_logprob': -0.12700286075986664, 'compression_ratio': 1.9052287581699345, 'no_speech_prob': 0.001098139677196741}, {'id': 331, 'seek': 186580, 'start': 1865.8, 'end': 1870.9199999999998, 'text': \" five is your coordinate space. You fall right there and you've attended four lectures. You've spent\", 'tokens': [50364, 1732, 307, 428, 15670, 1901, 13, 509, 2100, 558, 456, 293, 291, 600, 15990, 1451, 16564, 13, 509, 600, 4418, 50620], 'temperature': 0.0, 'avg_logprob': -0.10555257087896679, 'compression_ratio': 1.7228070175438597, 'no_speech_prob': 0.0008688080124557018}, {'id': 332, 'seek': 186580, 'start': 1870.9199999999998, 'end': 1875.48, 'text': ' five hours on your final project. We want to build a neural network to answer the question of,', 'tokens': [50620, 1732, 2496, 322, 428, 2572, 1716, 13, 492, 528, 281, 1322, 257, 18161, 3209, 281, 1867, 264, 1168, 295, 11, 50848], 'temperature': 0.0, 'avg_logprob': -0.10555257087896679, 'compression_ratio': 1.7228070175438597, 'no_speech_prob': 0.0008688080124557018}, {'id': 333, 'seek': 186580, 'start': 1875.48, 'end': 1881.24, 'text': \" will you pass the class or will you fail the class? So let's do it. We have two inputs. One is four,\", 'tokens': [50848, 486, 291, 1320, 264, 1508, 420, 486, 291, 3061, 264, 1508, 30, 407, 718, 311, 360, 309, 13, 492, 362, 732, 15743, 13, 1485, 307, 1451, 11, 51136], 'temperature': 0.0, 'avg_logprob': -0.10555257087896679, 'compression_ratio': 1.7228070175438597, 'no_speech_prob': 0.0008688080124557018}, {'id': 334, 'seek': 186580, 'start': 1881.24, 'end': 1885.6399999999999, 'text': \" one is five. These are two numbers. We can feed them through a neural network that we've just seen\", 'tokens': [51136, 472, 307, 1732, 13, 1981, 366, 732, 3547, 13, 492, 393, 3154, 552, 807, 257, 18161, 3209, 300, 321, 600, 445, 1612, 51356], 'temperature': 0.0, 'avg_logprob': -0.10555257087896679, 'compression_ratio': 1.7228070175438597, 'no_speech_prob': 0.0008688080124557018}, {'id': 335, 'seek': 186580, 'start': 1885.6399999999999, 'end': 1891.32, 'text': ' how we can build that. And we feed that into a single layered neural network. Three hidden units', 'tokens': [51356, 577, 321, 393, 1322, 300, 13, 400, 321, 3154, 300, 666, 257, 2167, 34666, 18161, 3209, 13, 6244, 7633, 6815, 51640], 'temperature': 0.0, 'avg_logprob': -0.10555257087896679, 'compression_ratio': 1.7228070175438597, 'no_speech_prob': 0.0008688080124557018}, {'id': 336, 'seek': 189132, 'start': 1891.32, 'end': 1895.24, 'text': ' in this example, but we could make it larger if we want it to be more expressive and more powerful.', 'tokens': [50364, 294, 341, 1365, 11, 457, 321, 727, 652, 309, 4833, 498, 321, 528, 309, 281, 312, 544, 40189, 293, 544, 4005, 13, 50560], 'temperature': 0.0, 'avg_logprob': -0.14193738196506972, 'compression_ratio': 1.645484949832776, 'no_speech_prob': 0.00021990336244925857}, {'id': 337, 'seek': 189132, 'start': 1896.2, 'end': 1900.6, 'text': \" And we see here that the probability of you passing those classes point one. It's pretty\", 'tokens': [50608, 400, 321, 536, 510, 300, 264, 8482, 295, 291, 8437, 729, 5359, 935, 472, 13, 467, 311, 1238, 50828], 'temperature': 0.0, 'avg_logprob': -0.14193738196506972, 'compression_ratio': 1.645484949832776, 'no_speech_prob': 0.00021990336244925857}, {'id': 338, 'seek': 189132, 'start': 1900.6, 'end': 1905.96, 'text': \" physical. So why would this be the case, right? What did we do wrong? Because I don't think it's\", 'tokens': [50828, 4001, 13, 407, 983, 576, 341, 312, 264, 1389, 11, 558, 30, 708, 630, 321, 360, 2085, 30, 1436, 286, 500, 380, 519, 309, 311, 51096], 'temperature': 0.0, 'avg_logprob': -0.14193738196506972, 'compression_ratio': 1.645484949832776, 'no_speech_prob': 0.00021990336244925857}, {'id': 339, 'seek': 189132, 'start': 1905.96, 'end': 1910.84, 'text': ' correct, right? When we looked at this space, it looked like actually you were a good candidate to', 'tokens': [51096, 3006, 11, 558, 30, 1133, 321, 2956, 412, 341, 1901, 11, 309, 2956, 411, 767, 291, 645, 257, 665, 11532, 281, 51340], 'temperature': 0.0, 'avg_logprob': -0.14193738196506972, 'compression_ratio': 1.645484949832776, 'no_speech_prob': 0.00021990336244925857}, {'id': 340, 'seek': 189132, 'start': 1910.84, 'end': 1916.36, 'text': \" pass the class. But why is the neural network saying that there's only 10% likelihood that you should pass?\", 'tokens': [51340, 1320, 264, 1508, 13, 583, 983, 307, 264, 18161, 3209, 1566, 300, 456, 311, 787, 1266, 4, 22119, 300, 291, 820, 1320, 30, 51616], 'temperature': 0.0, 'avg_logprob': -0.14193738196506972, 'compression_ratio': 1.645484949832776, 'no_speech_prob': 0.00021990336244925857}, {'id': 341, 'seek': 191636, 'start': 1916.36, 'end': 1928.84, 'text': ' Does anyone have any ideas? Exactly. So this neural network is just like it was just born,', 'tokens': [50364, 4402, 2878, 362, 604, 3487, 30, 7587, 13, 407, 341, 18161, 3209, 307, 445, 411, 309, 390, 445, 4232, 11, 50988], 'temperature': 0.0, 'avg_logprob': -0.16165242876325334, 'compression_ratio': 1.5683060109289617, 'no_speech_prob': 0.002357634250074625}, {'id': 342, 'seek': 191636, 'start': 1928.84, 'end': 1934.04, 'text': \" right? It has no information about the world or this class. It doesn't know what four and five\", 'tokens': [50988, 558, 30, 467, 575, 572, 1589, 466, 264, 1002, 420, 341, 1508, 13, 467, 1177, 380, 458, 437, 1451, 293, 1732, 51248], 'temperature': 0.0, 'avg_logprob': -0.16165242876325334, 'compression_ratio': 1.5683060109289617, 'no_speech_prob': 0.002357634250074625}, {'id': 343, 'seek': 191636, 'start': 1934.04, 'end': 1941.08, 'text': ' mean or what the notion of passing or failing means, right? So exactly right. This neural network has', 'tokens': [51248, 914, 420, 437, 264, 10710, 295, 8437, 420, 18223, 1355, 11, 558, 30, 407, 2293, 558, 13, 639, 18161, 3209, 575, 51600], 'temperature': 0.0, 'avg_logprob': -0.16165242876325334, 'compression_ratio': 1.5683060109289617, 'no_speech_prob': 0.002357634250074625}, {'id': 344, 'seek': 194108, 'start': 1941.08, 'end': 1946.04, 'text': \" not been trained. You can think of it kind of as a baby. It hasn't learned anything yet. So our\", 'tokens': [50364, 406, 668, 8895, 13, 509, 393, 519, 295, 309, 733, 295, 382, 257, 3186, 13, 467, 6132, 380, 3264, 1340, 1939, 13, 407, 527, 50612], 'temperature': 0.0, 'avg_logprob': -0.08324108426533049, 'compression_ratio': 1.7118055555555556, 'no_speech_prob': 0.0022157172206789255}, {'id': 345, 'seek': 194108, 'start': 1946.04, 'end': 1951.1599999999999, 'text': ' job firstly is to train it. And part of that understanding is we first need to tell the neural', 'tokens': [50612, 1691, 27376, 307, 281, 3847, 309, 13, 400, 644, 295, 300, 3701, 307, 321, 700, 643, 281, 980, 264, 18161, 50868], 'temperature': 0.0, 'avg_logprob': -0.08324108426533049, 'compression_ratio': 1.7118055555555556, 'no_speech_prob': 0.0022157172206789255}, {'id': 346, 'seek': 194108, 'start': 1951.1599999999999, 'end': 1956.52, 'text': ' network when it makes mistakes, right? So mathematically, we should now think about how we can answer this', 'tokens': [50868, 3209, 562, 309, 1669, 8038, 11, 558, 30, 407, 44003, 11, 321, 820, 586, 519, 466, 577, 321, 393, 1867, 341, 51136], 'temperature': 0.0, 'avg_logprob': -0.08324108426533049, 'compression_ratio': 1.7118055555555556, 'no_speech_prob': 0.0022157172206789255}, {'id': 347, 'seek': 194108, 'start': 1956.52, 'end': 1962.1999999999998, 'text': ' question, which is, did my neural network make a mistake? And if it made a mistake, how can I tell', 'tokens': [51136, 1168, 11, 597, 307, 11, 630, 452, 18161, 3209, 652, 257, 6146, 30, 400, 498, 309, 1027, 257, 6146, 11, 577, 393, 286, 980, 51420], 'temperature': 0.0, 'avg_logprob': -0.08324108426533049, 'compression_ratio': 1.7118055555555556, 'no_speech_prob': 0.0022157172206789255}, {'id': 348, 'seek': 194108, 'start': 1962.1999999999998, 'end': 1967.48, 'text': ' it? How big of a mistake it was so that the next time it sees this data point, can it do better?', 'tokens': [51420, 309, 30, 1012, 955, 295, 257, 6146, 309, 390, 370, 300, 264, 958, 565, 309, 8194, 341, 1412, 935, 11, 393, 309, 360, 1101, 30, 51684], 'temperature': 0.0, 'avg_logprob': -0.08324108426533049, 'compression_ratio': 1.7118055555555556, 'no_speech_prob': 0.0022157172206789255}, {'id': 349, 'seek': 196748, 'start': 1967.56, 'end': 1973.64, 'text': ' Minimize that mistake. So in neural network language, those mistakes are called losses,', 'tokens': [50368, 2829, 43890, 300, 6146, 13, 407, 294, 18161, 3209, 2856, 11, 729, 8038, 366, 1219, 15352, 11, 50672], 'temperature': 0.0, 'avg_logprob': -0.1147997326321072, 'compression_ratio': 1.7252252252252251, 'no_speech_prob': 0.0034797717817127705}, {'id': 350, 'seek': 196748, 'start': 1974.44, 'end': 1979.0, 'text': \" and specifically you want to define what's called a loss function, which is going to take as input\", 'tokens': [50712, 293, 4682, 291, 528, 281, 6964, 437, 311, 1219, 257, 4470, 2445, 11, 597, 307, 516, 281, 747, 382, 4846, 50940], 'temperature': 0.0, 'avg_logprob': -0.1147997326321072, 'compression_ratio': 1.7252252252252251, 'no_speech_prob': 0.0034797717817127705}, {'id': 351, 'seek': 196748, 'start': 1979.64, 'end': 1985.32, 'text': ' your prediction and the true prediction, right? And how far away your prediction is from the', 'tokens': [50972, 428, 17630, 293, 264, 2074, 17630, 11, 558, 30, 400, 577, 1400, 1314, 428, 17630, 307, 490, 264, 51256], 'temperature': 0.0, 'avg_logprob': -0.1147997326321072, 'compression_ratio': 1.7252252252252251, 'no_speech_prob': 0.0034797717817127705}, {'id': 352, 'seek': 196748, 'start': 1985.32, 'end': 1992.68, 'text': \" true prediction tells you how big of a loss there is, right? So for example, let's say we want to build\", 'tokens': [51256, 2074, 17630, 5112, 291, 577, 955, 295, 257, 4470, 456, 307, 11, 558, 30, 407, 337, 1365, 11, 718, 311, 584, 321, 528, 281, 1322, 51624], 'temperature': 0.0, 'avg_logprob': -0.1147997326321072, 'compression_ratio': 1.7252252252252251, 'no_speech_prob': 0.0034797717817127705}, {'id': 353, 'seek': 199268, 'start': 1993.24, 'end': 1999.64, 'text': ' a neural network to do classification of, or sorry, actually even before that, I want to maybe give', 'tokens': [50392, 257, 18161, 3209, 281, 360, 21538, 295, 11, 420, 2597, 11, 767, 754, 949, 300, 11, 286, 528, 281, 1310, 976, 50712], 'temperature': 0.0, 'avg_logprob': -0.09832550798143659, 'compression_ratio': 1.7703180212014133, 'no_speech_prob': 0.007681469898670912}, {'id': 354, 'seek': 199268, 'start': 1999.64, 'end': 2005.48, 'text': ' you some terminology. So there are multiple different ways of saying the same thing in neural networks', 'tokens': [50712, 291, 512, 27575, 13, 407, 456, 366, 3866, 819, 2098, 295, 1566, 264, 912, 551, 294, 18161, 9590, 51004], 'temperature': 0.0, 'avg_logprob': -0.09832550798143659, 'compression_ratio': 1.7703180212014133, 'no_speech_prob': 0.007681469898670912}, {'id': 355, 'seek': 199268, 'start': 2005.48, 'end': 2010.28, 'text': ' and deep learning. So what I just described as a loss function is also commonly referred to as an', 'tokens': [51004, 293, 2452, 2539, 13, 407, 437, 286, 445, 7619, 382, 257, 4470, 2445, 307, 611, 12719, 10839, 281, 382, 364, 51244], 'temperature': 0.0, 'avg_logprob': -0.09832550798143659, 'compression_ratio': 1.7703180212014133, 'no_speech_prob': 0.007681469898670912}, {'id': 356, 'seek': 199268, 'start': 2010.28, 'end': 2015.72, 'text': \" objective function in empirical risk, a cost function. These are all exactly the same thing. They're\", 'tokens': [51244, 10024, 2445, 294, 31886, 3148, 11, 257, 2063, 2445, 13, 1981, 366, 439, 2293, 264, 912, 551, 13, 814, 434, 51516], 'temperature': 0.0, 'avg_logprob': -0.09832550798143659, 'compression_ratio': 1.7703180212014133, 'no_speech_prob': 0.007681469898670912}, {'id': 357, 'seek': 199268, 'start': 2015.72, 'end': 2020.28, 'text': ' all the way for us to train the neural network, to teach the neural network when it makes mistakes.', 'tokens': [51516, 439, 264, 636, 337, 505, 281, 3847, 264, 18161, 3209, 11, 281, 2924, 264, 18161, 3209, 562, 309, 1669, 8038, 13, 51744], 'temperature': 0.0, 'avg_logprob': -0.09832550798143659, 'compression_ratio': 1.7703180212014133, 'no_speech_prob': 0.007681469898670912}, {'id': 358, 'seek': 202028, 'start': 2021.0, 'end': 2026.2, 'text': ' And what we really ultimately want to do is over the course of an entire data set,', 'tokens': [50400, 400, 437, 321, 534, 6284, 528, 281, 360, 307, 670, 264, 1164, 295, 364, 2302, 1412, 992, 11, 50660], 'temperature': 0.0, 'avg_logprob': -0.08717152831751272, 'compression_ratio': 1.736, 'no_speech_prob': 0.0016480492195114493}, {'id': 359, 'seek': 202028, 'start': 2026.2, 'end': 2029.8799999999999, 'text': ' not just one data point of mistakes, we want to say over the entire data set,', 'tokens': [50660, 406, 445, 472, 1412, 935, 295, 8038, 11, 321, 528, 281, 584, 670, 264, 2302, 1412, 992, 11, 50844], 'temperature': 0.0, 'avg_logprob': -0.08717152831751272, 'compression_ratio': 1.736, 'no_speech_prob': 0.0016480492195114493}, {'id': 360, 'seek': 202028, 'start': 2030.44, 'end': 2035.08, 'text': ' we want to minimize all of the mistakes on average that this neural network makes.', 'tokens': [50872, 321, 528, 281, 17522, 439, 295, 264, 8038, 322, 4274, 300, 341, 18161, 3209, 1669, 13, 51104], 'temperature': 0.0, 'avg_logprob': -0.08717152831751272, 'compression_ratio': 1.736, 'no_speech_prob': 0.0016480492195114493}, {'id': 361, 'seek': 202028, 'start': 2036.68, 'end': 2041.24, 'text': ' So if we look at the problem, like I said, of binary classification, will I pass this class or', 'tokens': [51184, 407, 498, 321, 574, 412, 264, 1154, 11, 411, 286, 848, 11, 295, 17434, 21538, 11, 486, 286, 1320, 341, 1508, 420, 51412], 'temperature': 0.0, 'avg_logprob': -0.08717152831751272, 'compression_ratio': 1.736, 'no_speech_prob': 0.0016480492195114493}, {'id': 362, 'seek': 202028, 'start': 2041.24, 'end': 2047.08, 'text': \" will I not? There's a yes or no answer, that means binary classification. Now we can use what's\", 'tokens': [51412, 486, 286, 406, 30, 821, 311, 257, 2086, 420, 572, 1867, 11, 300, 1355, 17434, 21538, 13, 823, 321, 393, 764, 437, 311, 51704], 'temperature': 0.0, 'avg_logprob': -0.08717152831751272, 'compression_ratio': 1.736, 'no_speech_prob': 0.0016480492195114493}, {'id': 363, 'seek': 204708, 'start': 2047.1599999999999, 'end': 2052.44, 'text': \" called a loss function of the softmax cross entropy loss. And for those of you who aren't familiar,\", 'tokens': [50368, 1219, 257, 4470, 2445, 295, 264, 2787, 41167, 3278, 30867, 4470, 13, 400, 337, 729, 295, 291, 567, 3212, 380, 4963, 11, 50632], 'temperature': 0.0, 'avg_logprob': -0.17444560503718828, 'compression_ratio': 1.65, 'no_speech_prob': 0.0009839441627264023}, {'id': 364, 'seek': 204708, 'start': 2052.44, 'end': 2059.96, 'text': ' this notion of cross entropy is actually developed here at MIT by Shod Klanit. Shod, excuse me, yes.', 'tokens': [50632, 341, 10710, 295, 3278, 30867, 307, 767, 4743, 510, 412, 13100, 538, 1160, 378, 591, 8658, 270, 13, 1160, 378, 11, 8960, 385, 11, 2086, 13, 51008], 'temperature': 0.0, 'avg_logprob': -0.17444560503718828, 'compression_ratio': 1.65, 'no_speech_prob': 0.0009839441627264023}, {'id': 365, 'seek': 204708, 'start': 2061.4, 'end': 2068.6, 'text': ' Claude Shannon, who is a visionary, he did his masters here over 50 years ago, he introduced this', 'tokens': [51080, 12947, 2303, 28974, 11, 567, 307, 257, 49442, 11, 415, 630, 702, 19294, 510, 670, 2625, 924, 2057, 11, 415, 7268, 341, 51440], 'temperature': 0.0, 'avg_logprob': -0.17444560503718828, 'compression_ratio': 1.65, 'no_speech_prob': 0.0009839441627264023}, {'id': 366, 'seek': 204708, 'start': 2068.6, 'end': 2075.7999999999997, 'text': ' notion of cross entropy and that was pivotal in the ability for us to train these types of neural', 'tokens': [51440, 10710, 295, 3278, 30867, 293, 300, 390, 39078, 294, 264, 3485, 337, 505, 281, 3847, 613, 3467, 295, 18161, 51800], 'temperature': 0.0, 'avg_logprob': -0.17444560503718828, 'compression_ratio': 1.65, 'no_speech_prob': 0.0009839441627264023}, {'id': 367, 'seek': 207580, 'start': 2075.8, 'end': 2083.7200000000003, 'text': \" networks even now into the future. So let's start by, instead of predicting a binary cross entropy\", 'tokens': [50364, 9590, 754, 586, 666, 264, 2027, 13, 407, 718, 311, 722, 538, 11, 2602, 295, 32884, 257, 17434, 3278, 30867, 50760], 'temperature': 0.0, 'avg_logprob': -0.12272931695953618, 'compression_ratio': 1.7152777777777777, 'no_speech_prob': 0.0006258534267544746}, {'id': 368, 'seek': 207580, 'start': 2083.7200000000003, 'end': 2089.48, 'text': \" output, what if we wanted to predict a final grade of your class score? For example, that's no\", 'tokens': [50760, 5598, 11, 437, 498, 321, 1415, 281, 6069, 257, 2572, 7204, 295, 428, 1508, 6175, 30, 1171, 1365, 11, 300, 311, 572, 51048], 'temperature': 0.0, 'avg_logprob': -0.12272931695953618, 'compression_ratio': 1.7152777777777777, 'no_speech_prob': 0.0006258534267544746}, {'id': 369, 'seek': 207580, 'start': 2089.48, 'end': 2094.52, 'text': \" longer binary output, yes or no, it's actually a continuous variable, it's the grade, let's say out\", 'tokens': [51048, 2854, 17434, 5598, 11, 2086, 420, 572, 11, 309, 311, 767, 257, 10957, 7006, 11, 309, 311, 264, 7204, 11, 718, 311, 584, 484, 51300], 'temperature': 0.0, 'avg_logprob': -0.12272931695953618, 'compression_ratio': 1.7152777777777777, 'no_speech_prob': 0.0006258534267544746}, {'id': 370, 'seek': 207580, 'start': 2094.52, 'end': 2101.1600000000003, 'text': ' of 100 points, what is the value of your score in the class project? For this type of loss, we can', 'tokens': [51300, 295, 2319, 2793, 11, 437, 307, 264, 2158, 295, 428, 6175, 294, 264, 1508, 1716, 30, 1171, 341, 2010, 295, 4470, 11, 321, 393, 51632], 'temperature': 0.0, 'avg_logprob': -0.12272931695953618, 'compression_ratio': 1.7152777777777777, 'no_speech_prob': 0.0006258534267544746}, {'id': 371, 'seek': 207580, 'start': 2101.1600000000003, 'end': 2105.48, 'text': \" use what's called a mean squared error loss. You can think of this literally as just subtracting your\", 'tokens': [51632, 764, 437, 311, 1219, 257, 914, 8889, 6713, 4470, 13, 509, 393, 519, 295, 341, 3736, 382, 445, 16390, 278, 428, 51848], 'temperature': 0.0, 'avg_logprob': -0.12272931695953618, 'compression_ratio': 1.7152777777777777, 'no_speech_prob': 0.0006258534267544746}, {'id': 372, 'seek': 210548, 'start': 2105.48, 'end': 2114.28, 'text': \" predicted grade from the true grade and minimizing that distance apart. So I think now we're ready to\", 'tokens': [50364, 19147, 7204, 490, 264, 2074, 7204, 293, 46608, 300, 4560, 4936, 13, 407, 286, 519, 586, 321, 434, 1919, 281, 50804], 'temperature': 0.0, 'avg_logprob': -0.08014583587646484, 'compression_ratio': 1.645021645021645, 'no_speech_prob': 0.0001794815470930189}, {'id': 373, 'seek': 210548, 'start': 2114.28, 'end': 2120.2, 'text': ' really put all of this information together and tackle this problem of training a neural network,', 'tokens': [50804, 534, 829, 439, 295, 341, 1589, 1214, 293, 14896, 341, 1154, 295, 3097, 257, 18161, 3209, 11, 51100], 'temperature': 0.0, 'avg_logprob': -0.08014583587646484, 'compression_ratio': 1.645021645021645, 'no_speech_prob': 0.0001794815470930189}, {'id': 374, 'seek': 210548, 'start': 2120.2, 'end': 2127.96, 'text': ' right, to not just identify how erroneous it is, how large its loss is, but more importantly,', 'tokens': [51100, 558, 11, 281, 406, 445, 5876, 577, 1189, 26446, 563, 309, 307, 11, 577, 2416, 1080, 4470, 307, 11, 457, 544, 8906, 11, 51488], 'temperature': 0.0, 'avg_logprob': -0.08014583587646484, 'compression_ratio': 1.645021645021645, 'no_speech_prob': 0.0001794815470930189}, {'id': 375, 'seek': 210548, 'start': 2127.96, 'end': 2132.12, 'text': ' minimize that loss as a function of seeing all of this training data that is observed.', 'tokens': [51488, 17522, 300, 4470, 382, 257, 2445, 295, 2577, 439, 295, 341, 3097, 1412, 300, 307, 13095, 13, 51696], 'temperature': 0.0, 'avg_logprob': -0.08014583587646484, 'compression_ratio': 1.645021645021645, 'no_speech_prob': 0.0001794815470930189}, {'id': 376, 'seek': 213212, 'start': 2132.68, 'end': 2138.52, 'text': ' So we know that we want to find this neural network, like we mentioned before, that minimizes', 'tokens': [50392, 407, 321, 458, 300, 321, 528, 281, 915, 341, 18161, 3209, 11, 411, 321, 2835, 949, 11, 300, 4464, 5660, 50684], 'temperature': 0.0, 'avg_logprob': -0.13896922475283907, 'compression_ratio': 1.9253731343283582, 'no_speech_prob': 0.015178469941020012}, {'id': 377, 'seek': 213212, 'start': 2138.52, 'end': 2145.08, 'text': ' this empirical risk or this empirical loss averaged across our entire data set. Now this means that', 'tokens': [50684, 341, 31886, 3148, 420, 341, 31886, 4470, 18247, 2980, 2108, 527, 2302, 1412, 992, 13, 823, 341, 1355, 300, 51012], 'temperature': 0.0, 'avg_logprob': -0.13896922475283907, 'compression_ratio': 1.9253731343283582, 'no_speech_prob': 0.015178469941020012}, {'id': 378, 'seek': 213212, 'start': 2145.08, 'end': 2153.08, 'text': \" we want to find mathematically these W's, right, that minimize J of W. J of W is our loss function,\", 'tokens': [51012, 321, 528, 281, 915, 44003, 613, 343, 311, 11, 558, 11, 300, 17522, 508, 295, 343, 13, 508, 295, 343, 307, 527, 4470, 2445, 11, 51412], 'temperature': 0.0, 'avg_logprob': -0.13896922475283907, 'compression_ratio': 1.9253731343283582, 'no_speech_prob': 0.015178469941020012}, {'id': 379, 'seek': 213212, 'start': 2153.08, 'end': 2157.7999999999997, 'text': ' averaged over our entire data set, and W is our weight. So we want to find the set of weights', 'tokens': [51412, 18247, 2980, 670, 527, 2302, 1412, 992, 11, 293, 343, 307, 527, 3364, 13, 407, 321, 528, 281, 915, 264, 992, 295, 17443, 51648], 'temperature': 0.0, 'avg_logprob': -0.13896922475283907, 'compression_ratio': 1.9253731343283582, 'no_speech_prob': 0.015178469941020012}, {'id': 380, 'seek': 215780, 'start': 2158.52, 'end': 2167.32, 'text': ' that on average is going to give us the smallest loss as possible. Now remember that W here is just', 'tokens': [50400, 300, 322, 4274, 307, 516, 281, 976, 505, 264, 16998, 4470, 382, 1944, 13, 823, 1604, 300, 343, 510, 307, 445, 50840], 'temperature': 0.0, 'avg_logprob': -0.1457835119597766, 'compression_ratio': 1.7787610619469028, 'no_speech_prob': 0.0024338390212506056}, {'id': 381, 'seek': 215780, 'start': 2167.96, 'end': 2173.0800000000004, 'text': \" a list. Basically it's just a group of all of the weights in our neural network. You may have hundreds\", 'tokens': [50872, 257, 1329, 13, 8537, 309, 311, 445, 257, 1594, 295, 439, 295, 264, 17443, 294, 527, 18161, 3209, 13, 509, 815, 362, 6779, 51128], 'temperature': 0.0, 'avg_logprob': -0.1457835119597766, 'compression_ratio': 1.7787610619469028, 'no_speech_prob': 0.0024338390212506056}, {'id': 382, 'seek': 215780, 'start': 2173.0800000000004, 'end': 2177.2400000000002, 'text': \" of the weights and a very, very small neural network, or in today's neural networks, you may have\", 'tokens': [51128, 295, 264, 17443, 293, 257, 588, 11, 588, 1359, 18161, 3209, 11, 420, 294, 965, 311, 18161, 9590, 11, 291, 815, 362, 51336], 'temperature': 0.0, 'avg_logprob': -0.1457835119597766, 'compression_ratio': 1.7787610619469028, 'no_speech_prob': 0.0024338390212506056}, {'id': 383, 'seek': 215780, 'start': 2177.2400000000002, 'end': 2182.1200000000003, 'text': ' billions or trillions of weights, and you want to find what is the value of every single one of these', 'tokens': [51336, 17375, 420, 504, 46279, 295, 17443, 11, 293, 291, 528, 281, 915, 437, 307, 264, 2158, 295, 633, 2167, 472, 295, 613, 51580], 'temperature': 0.0, 'avg_logprob': -0.1457835119597766, 'compression_ratio': 1.7787610619469028, 'no_speech_prob': 0.0024338390212506056}, {'id': 384, 'seek': 218212, 'start': 2182.12, 'end': 2187.96, 'text': ' weights that is going to result in the smallest loss as possible. Now how can you do this?', 'tokens': [50364, 17443, 300, 307, 516, 281, 1874, 294, 264, 16998, 4470, 382, 1944, 13, 823, 577, 393, 291, 360, 341, 30, 50656], 'temperature': 0.0, 'avg_logprob': -0.09179330865542094, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 0.00022689737670589238}, {'id': 385, 'seek': 218212, 'start': 2187.96, 'end': 2194.12, 'text': ' Remember that our loss function, J of W, is just a function of our weights, right? So for any', 'tokens': [50656, 5459, 300, 527, 4470, 2445, 11, 508, 295, 343, 11, 307, 445, 257, 2445, 295, 527, 17443, 11, 558, 30, 407, 337, 604, 50964], 'temperature': 0.0, 'avg_logprob': -0.09179330865542094, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 0.00022689737670589238}, {'id': 386, 'seek': 218212, 'start': 2194.12, 'end': 2201.64, 'text': ' instantiation of our weights, we can compute a scalar value of how erroneous would our neural', 'tokens': [50964, 9836, 6642, 295, 527, 17443, 11, 321, 393, 14722, 257, 39684, 2158, 295, 577, 1189, 26446, 563, 576, 527, 18161, 51340], 'temperature': 0.0, 'avg_logprob': -0.09179330865542094, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 0.00022689737670589238}, {'id': 387, 'seek': 218212, 'start': 2201.64, 'end': 2207.7999999999997, 'text': \" network be for this instantiation of our weights. So let's try and visualize, for example, in a very\", 'tokens': [51340, 3209, 312, 337, 341, 9836, 6642, 295, 527, 17443, 13, 407, 718, 311, 853, 293, 23273, 11, 337, 1365, 11, 294, 257, 588, 51648], 'temperature': 0.0, 'avg_logprob': -0.09179330865542094, 'compression_ratio': 1.6919642857142858, 'no_speech_prob': 0.00022689737670589238}, {'id': 388, 'seek': 220780, 'start': 2207.8, 'end': 2214.1200000000003, 'text': ' simple example of a two-dimensional space where we have only two weights, extremely simple neural', 'tokens': [50364, 2199, 1365, 295, 257, 732, 12, 18759, 1901, 689, 321, 362, 787, 732, 17443, 11, 4664, 2199, 18161, 50680], 'temperature': 0.0, 'avg_logprob': -0.09197835507600204, 'compression_ratio': 1.8435114503816794, 'no_speech_prob': 0.0007206393638625741}, {'id': 389, 'seek': 220780, 'start': 2214.1200000000003, 'end': 2219.96, 'text': ' network here, very small, two-weight neural network, and we want to find what are the optimal weights', 'tokens': [50680, 3209, 510, 11, 588, 1359, 11, 732, 12, 12329, 18161, 3209, 11, 293, 321, 528, 281, 915, 437, 366, 264, 16252, 17443, 50972], 'temperature': 0.0, 'avg_logprob': -0.09197835507600204, 'compression_ratio': 1.8435114503816794, 'no_speech_prob': 0.0007206393638625741}, {'id': 390, 'seek': 220780, 'start': 2219.96, 'end': 2226.36, 'text': ' that would train this neural network. We can plot basically the loss, how erroneous the neural', 'tokens': [50972, 300, 576, 3847, 341, 18161, 3209, 13, 492, 393, 7542, 1936, 264, 4470, 11, 577, 1189, 26446, 563, 264, 18161, 51292], 'temperature': 0.0, 'avg_logprob': -0.09197835507600204, 'compression_ratio': 1.8435114503816794, 'no_speech_prob': 0.0007206393638625741}, {'id': 391, 'seek': 220780, 'start': 2226.36, 'end': 2232.1200000000003, 'text': ' network is for every single instantiation of these two weights, right? This is a huge space,', 'tokens': [51292, 3209, 307, 337, 633, 2167, 9836, 6642, 295, 613, 732, 17443, 11, 558, 30, 639, 307, 257, 2603, 1901, 11, 51580], 'temperature': 0.0, 'avg_logprob': -0.09197835507600204, 'compression_ratio': 1.8435114503816794, 'no_speech_prob': 0.0007206393638625741}, {'id': 392, 'seek': 220780, 'start': 2232.1200000000003, 'end': 2237.7200000000003, 'text': \" it's an infinite space, but still we can try to, we can have a function that evaluates at every\", 'tokens': [51580, 309, 311, 364, 13785, 1901, 11, 457, 920, 321, 393, 853, 281, 11, 321, 393, 362, 257, 2445, 300, 6133, 1024, 412, 633, 51860], 'temperature': 0.0, 'avg_logprob': -0.09197835507600204, 'compression_ratio': 1.8435114503816794, 'no_speech_prob': 0.0007206393638625741}, {'id': 393, 'seek': 223772, 'start': 2237.72, 'end': 2245.24, 'text': ' point in this space. Now what we ultimately want to do is, again, we want to find which set of', 'tokens': [50364, 935, 294, 341, 1901, 13, 823, 437, 321, 6284, 528, 281, 360, 307, 11, 797, 11, 321, 528, 281, 915, 597, 992, 295, 50740], 'temperature': 0.0, 'avg_logprob': -0.07930404725282089, 'compression_ratio': 1.6167400881057268, 'no_speech_prob': 0.0006985588697716594}, {'id': 394, 'seek': 223772, 'start': 2245.24, 'end': 2251.7999999999997, 'text': \" W's will give us the smallest loss possible. That means basically the lowest point on this\", 'tokens': [50740, 343, 311, 486, 976, 505, 264, 16998, 4470, 1944, 13, 663, 1355, 1936, 264, 12437, 935, 322, 341, 51068], 'temperature': 0.0, 'avg_logprob': -0.07930404725282089, 'compression_ratio': 1.6167400881057268, 'no_speech_prob': 0.0006985588697716594}, {'id': 395, 'seek': 223772, 'start': 2251.7999999999997, 'end': 2257.0, 'text': \" landscape that you can see here, where is the W's that bring us to that lowest point?\", 'tokens': [51068, 9661, 300, 291, 393, 536, 510, 11, 689, 307, 264, 343, 311, 300, 1565, 505, 281, 300, 12437, 935, 30, 51328], 'temperature': 0.0, 'avg_logprob': -0.07930404725282089, 'compression_ratio': 1.6167400881057268, 'no_speech_prob': 0.0006985588697716594}, {'id': 396, 'seek': 223772, 'start': 2258.9199999999996, 'end': 2264.04, 'text': ' The way that we do this is actually just by firstly starting at a random place, we have no idea', 'tokens': [51424, 440, 636, 300, 321, 360, 341, 307, 767, 445, 538, 27376, 2891, 412, 257, 4974, 1081, 11, 321, 362, 572, 1558, 51680], 'temperature': 0.0, 'avg_logprob': -0.07930404725282089, 'compression_ratio': 1.6167400881057268, 'no_speech_prob': 0.0006985588697716594}, {'id': 397, 'seek': 226404, 'start': 2264.04, 'end': 2269.0, 'text': \" where to start, so pick a random place to start in this space, and let's start there. At this\", 'tokens': [50364, 689, 281, 722, 11, 370, 1888, 257, 4974, 1081, 281, 722, 294, 341, 1901, 11, 293, 718, 311, 722, 456, 13, 1711, 341, 50612], 'temperature': 0.0, 'avg_logprob': -0.10265882165582331, 'compression_ratio': 1.9877049180327868, 'no_speech_prob': 0.0017537983367219567}, {'id': 398, 'seek': 226404, 'start': 2269.0, 'end': 2274.36, 'text': \" location, let's evaluate our neural network. We can compute the loss at this specific location,\", 'tokens': [50612, 4914, 11, 718, 311, 13059, 527, 18161, 3209, 13, 492, 393, 14722, 264, 4470, 412, 341, 2685, 4914, 11, 50880], 'temperature': 0.0, 'avg_logprob': -0.10265882165582331, 'compression_ratio': 1.9877049180327868, 'no_speech_prob': 0.0017537983367219567}, {'id': 399, 'seek': 226404, 'start': 2274.92, 'end': 2280.44, 'text': ' and on top of that we can actually compute how the loss is changing. We can compute the gradient', 'tokens': [50908, 293, 322, 1192, 295, 300, 321, 393, 767, 14722, 577, 264, 4470, 307, 4473, 13, 492, 393, 14722, 264, 16235, 51184], 'temperature': 0.0, 'avg_logprob': -0.10265882165582331, 'compression_ratio': 1.9877049180327868, 'no_speech_prob': 0.0017537983367219567}, {'id': 400, 'seek': 226404, 'start': 2280.44, 'end': 2285.8, 'text': ' of the loss because our loss function is a continuous function, right? So we can actually compute', 'tokens': [51184, 295, 264, 4470, 570, 527, 4470, 2445, 307, 257, 10957, 2445, 11, 558, 30, 407, 321, 393, 767, 14722, 51452], 'temperature': 0.0, 'avg_logprob': -0.10265882165582331, 'compression_ratio': 1.9877049180327868, 'no_speech_prob': 0.0017537983367219567}, {'id': 401, 'seek': 226404, 'start': 2285.8, 'end': 2292.36, 'text': ' derivatives of our function across the space of our weights, and the gradient tells us the direction', 'tokens': [51452, 33733, 295, 527, 2445, 2108, 264, 1901, 295, 527, 17443, 11, 293, 264, 16235, 5112, 505, 264, 3513, 51780], 'temperature': 0.0, 'avg_logprob': -0.10265882165582331, 'compression_ratio': 1.9877049180327868, 'no_speech_prob': 0.0017537983367219567}, {'id': 402, 'seek': 229236, 'start': 2292.36, 'end': 2297.48, 'text': ' of the highest point, right? So from where we stand, the gradient tells us where we should go', 'tokens': [50364, 295, 264, 6343, 935, 11, 558, 30, 407, 490, 689, 321, 1463, 11, 264, 16235, 5112, 505, 689, 321, 820, 352, 50620], 'temperature': 0.0, 'avg_logprob': -0.07470246323016512, 'compression_ratio': 1.8389513108614233, 'no_speech_prob': 0.0010002335766330361}, {'id': 403, 'seek': 229236, 'start': 2298.04, 'end': 2303.1600000000003, 'text': \" to increase our loss. Now of course we don't want to increase our loss, we want to decrease our loss,\", 'tokens': [50648, 281, 3488, 527, 4470, 13, 823, 295, 1164, 321, 500, 380, 528, 281, 3488, 527, 4470, 11, 321, 528, 281, 11514, 527, 4470, 11, 50904], 'temperature': 0.0, 'avg_logprob': -0.07470246323016512, 'compression_ratio': 1.8389513108614233, 'no_speech_prob': 0.0010002335766330361}, {'id': 404, 'seek': 229236, 'start': 2303.1600000000003, 'end': 2308.6, 'text': ' so we negate our gradient, and we take a step in the opposite direction of the gradient. That brings', 'tokens': [50904, 370, 321, 2485, 473, 527, 16235, 11, 293, 321, 747, 257, 1823, 294, 264, 6182, 3513, 295, 264, 16235, 13, 663, 5607, 51176], 'temperature': 0.0, 'avg_logprob': -0.07470246323016512, 'compression_ratio': 1.8389513108614233, 'no_speech_prob': 0.0010002335766330361}, {'id': 405, 'seek': 229236, 'start': 2308.6, 'end': 2314.84, 'text': ' us one step closer to the bottom of the landscape, and we just keep repeating this process, right?', 'tokens': [51176, 505, 472, 1823, 4966, 281, 264, 2767, 295, 264, 9661, 11, 293, 321, 445, 1066, 18617, 341, 1399, 11, 558, 30, 51488], 'temperature': 0.0, 'avg_logprob': -0.07470246323016512, 'compression_ratio': 1.8389513108614233, 'no_speech_prob': 0.0010002335766330361}, {'id': 406, 'seek': 229236, 'start': 2314.84, 'end': 2319.6400000000003, 'text': ' Over and over again, we evaluate the neural network at this new location, compute its gradient,', 'tokens': [51488, 4886, 293, 670, 797, 11, 321, 13059, 264, 18161, 3209, 412, 341, 777, 4914, 11, 14722, 1080, 16235, 11, 51728], 'temperature': 0.0, 'avg_logprob': -0.07470246323016512, 'compression_ratio': 1.8389513108614233, 'no_speech_prob': 0.0010002335766330361}, {'id': 407, 'seek': 231964, 'start': 2319.64, 'end': 2325.24, 'text': ' and step in that new direction. We keep traversing this landscape until we converge to the minimum.', 'tokens': [50364, 293, 1823, 294, 300, 777, 3513, 13, 492, 1066, 23149, 278, 341, 9661, 1826, 321, 41881, 281, 264, 7285, 13, 50644], 'temperature': 0.0, 'avg_logprob': -0.09250284830729166, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.0004653476644307375}, {'id': 408, 'seek': 231964, 'start': 2327.24, 'end': 2332.52, 'text': ' We can really summarize this algorithm, which is known formally as gradient descent, right? So', 'tokens': [50744, 492, 393, 534, 20858, 341, 9284, 11, 597, 307, 2570, 25983, 382, 16235, 23475, 11, 558, 30, 407, 51008], 'temperature': 0.0, 'avg_logprob': -0.09250284830729166, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.0004653476644307375}, {'id': 409, 'seek': 231964, 'start': 2332.52, 'end': 2337.8799999999997, 'text': ' gradient descent simply can be written like this. We initialize all of our weights, right? This can', 'tokens': [51008, 16235, 23475, 2935, 393, 312, 3720, 411, 341, 13, 492, 5883, 1125, 439, 295, 527, 17443, 11, 558, 30, 639, 393, 51276], 'temperature': 0.0, 'avg_logprob': -0.09250284830729166, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.0004653476644307375}, {'id': 410, 'seek': 231964, 'start': 2337.8799999999997, 'end': 2342.8399999999997, 'text': ' be two weights, like you saw in the previous example, it can be billions of weights, like in', 'tokens': [51276, 312, 732, 17443, 11, 411, 291, 1866, 294, 264, 3894, 1365, 11, 309, 393, 312, 17375, 295, 17443, 11, 411, 294, 51524], 'temperature': 0.0, 'avg_logprob': -0.09250284830729166, 'compression_ratio': 1.6899563318777293, 'no_speech_prob': 0.0004653476644307375}, {'id': 411, 'seek': 234284, 'start': 2342.84, 'end': 2350.28, 'text': ' real neural networks. We compute this gradient of the partial derivative of our loss with respect', 'tokens': [50364, 957, 18161, 9590, 13, 492, 14722, 341, 16235, 295, 264, 14641, 13760, 295, 527, 4470, 365, 3104, 50736], 'temperature': 0.0, 'avg_logprob': -0.07801720370417056, 'compression_ratio': 1.6738197424892705, 'no_speech_prob': 0.0014319627080112696}, {'id': 412, 'seek': 234284, 'start': 2350.28, 'end': 2354.6800000000003, 'text': ' to the weights, and then we can update our weights in the opposite direction of this gradient.', 'tokens': [50736, 281, 264, 17443, 11, 293, 550, 321, 393, 5623, 527, 17443, 294, 264, 6182, 3513, 295, 341, 16235, 13, 50956], 'temperature': 0.0, 'avg_logprob': -0.07801720370417056, 'compression_ratio': 1.6738197424892705, 'no_speech_prob': 0.0014319627080112696}, {'id': 413, 'seek': 234284, 'start': 2355.8, 'end': 2361.7200000000003, 'text': ' So essentially we just take this small amount, small step, you can think of it, which here is denoted', 'tokens': [51012, 407, 4476, 321, 445, 747, 341, 1359, 2372, 11, 1359, 1823, 11, 291, 393, 519, 295, 309, 11, 597, 510, 307, 1441, 23325, 51308], 'temperature': 0.0, 'avg_logprob': -0.07801720370417056, 'compression_ratio': 1.6738197424892705, 'no_speech_prob': 0.0014319627080112696}, {'id': 414, 'seek': 234284, 'start': 2361.7200000000003, 'end': 2369.0, 'text': \" as eta, and we refer to this small step, right? This is commonly referred to as what's known as\", 'tokens': [51308, 382, 32415, 11, 293, 321, 2864, 281, 341, 1359, 1823, 11, 558, 30, 639, 307, 12719, 10839, 281, 382, 437, 311, 2570, 382, 51672], 'temperature': 0.0, 'avg_logprob': -0.07801720370417056, 'compression_ratio': 1.6738197424892705, 'no_speech_prob': 0.0014319627080112696}, {'id': 415, 'seek': 236900, 'start': 2369.0, 'end': 2373.8, 'text': \" the learning rate. It's like how much we want to trust that gradient and step in the direction of\", 'tokens': [50364, 264, 2539, 3314, 13, 467, 311, 411, 577, 709, 321, 528, 281, 3361, 300, 16235, 293, 1823, 294, 264, 3513, 295, 50604], 'temperature': 0.0, 'avg_logprob': -0.08436430954351658, 'compression_ratio': 1.7241379310344827, 'no_speech_prob': 0.00271362392231822}, {'id': 416, 'seek': 236900, 'start': 2373.8, 'end': 2380.36, 'text': \" that gradient. We'll talk more about this later, but just to give you some sense of code, this algorithm\", 'tokens': [50604, 300, 16235, 13, 492, 603, 751, 544, 466, 341, 1780, 11, 457, 445, 281, 976, 291, 512, 2020, 295, 3089, 11, 341, 9284, 50932], 'temperature': 0.0, 'avg_logprob': -0.08436430954351658, 'compression_ratio': 1.7241379310344827, 'no_speech_prob': 0.00271362392231822}, {'id': 417, 'seek': 236900, 'start': 2380.36, 'end': 2384.92, 'text': ' is very well translatable to real code as well. For every line on the pseudo code you can see on the', 'tokens': [50932, 307, 588, 731, 5105, 31415, 281, 957, 3089, 382, 731, 13, 1171, 633, 1622, 322, 264, 35899, 3089, 291, 393, 536, 322, 264, 51160], 'temperature': 0.0, 'avg_logprob': -0.08436430954351658, 'compression_ratio': 1.7241379310344827, 'no_speech_prob': 0.00271362392231822}, {'id': 418, 'seek': 236900, 'start': 2384.92, 'end': 2389.72, 'text': ' left, you can see corresponding real code on the right that is runnable and directly implementable', 'tokens': [51160, 1411, 11, 291, 393, 536, 11760, 957, 3089, 322, 264, 558, 300, 307, 1190, 77, 712, 293, 3838, 4445, 712, 51400], 'temperature': 0.0, 'avg_logprob': -0.08436430954351658, 'compression_ratio': 1.7241379310344827, 'no_speech_prob': 0.00271362392231822}, {'id': 419, 'seek': 236900, 'start': 2389.72, 'end': 2395.0, 'text': \" by all of you in your labs. But now let's take a look specifically at this term here. This is the\", 'tokens': [51400, 538, 439, 295, 291, 294, 428, 20339, 13, 583, 586, 718, 311, 747, 257, 574, 4682, 412, 341, 1433, 510, 13, 639, 307, 264, 51664], 'temperature': 0.0, 'avg_logprob': -0.08436430954351658, 'compression_ratio': 1.7241379310344827, 'no_speech_prob': 0.00271362392231822}, {'id': 420, 'seek': 239500, 'start': 2395.0, 'end': 2400.04, 'text': ' gradient. We touched very briefly on this in the visual example. This explains, like I said,', 'tokens': [50364, 16235, 13, 492, 9828, 588, 10515, 322, 341, 294, 264, 5056, 1365, 13, 639, 13948, 11, 411, 286, 848, 11, 50616], 'temperature': 0.0, 'avg_logprob': -0.09894801215302172, 'compression_ratio': 1.794392523364486, 'no_speech_prob': 0.0009392191423103213}, {'id': 421, 'seek': 239500, 'start': 2400.04, 'end': 2405.56, 'text': ' how the loss is changing as a function of the weights, right? So as the weights move around,', 'tokens': [50616, 577, 264, 4470, 307, 4473, 382, 257, 2445, 295, 264, 17443, 11, 558, 30, 407, 382, 264, 17443, 1286, 926, 11, 50892], 'temperature': 0.0, 'avg_logprob': -0.09894801215302172, 'compression_ratio': 1.794392523364486, 'no_speech_prob': 0.0009392191423103213}, {'id': 422, 'seek': 239500, 'start': 2405.56, 'end': 2410.04, 'text': ' will my loss increase or decrease, and that will tell the neural network if it needs to move the', 'tokens': [50892, 486, 452, 4470, 3488, 420, 11514, 11, 293, 300, 486, 980, 264, 18161, 3209, 498, 309, 2203, 281, 1286, 264, 51116], 'temperature': 0.0, 'avg_logprob': -0.09894801215302172, 'compression_ratio': 1.794392523364486, 'no_speech_prob': 0.0009392191423103213}, {'id': 423, 'seek': 239500, 'start': 2410.04, 'end': 2415.4, 'text': ' weights in a certain direction or not. But I never actually told you how to compute this, right?', 'tokens': [51116, 17443, 294, 257, 1629, 3513, 420, 406, 13, 583, 286, 1128, 767, 1907, 291, 577, 281, 14722, 341, 11, 558, 30, 51384], 'temperature': 0.0, 'avg_logprob': -0.09894801215302172, 'compression_ratio': 1.794392523364486, 'no_speech_prob': 0.0009392191423103213}, {'id': 424, 'seek': 239500, 'start': 2415.4, 'end': 2418.92, 'text': \" And I think that's an extremely important part because if you don't know that, then you can't\", 'tokens': [51384, 400, 286, 519, 300, 311, 364, 4664, 1021, 644, 570, 498, 291, 500, 380, 458, 300, 11, 550, 291, 393, 380, 51560], 'temperature': 0.0, 'avg_logprob': -0.09894801215302172, 'compression_ratio': 1.794392523364486, 'no_speech_prob': 0.0009392191423103213}, {'id': 425, 'seek': 239500, 'start': 2419.88, 'end': 2424.6, 'text': \" well, you can't train your neural network, right? This is a critical part of training neural networks,\", 'tokens': [51608, 731, 11, 291, 393, 380, 3847, 428, 18161, 3209, 11, 558, 30, 639, 307, 257, 4924, 644, 295, 3097, 18161, 9590, 11, 51844], 'temperature': 0.0, 'avg_logprob': -0.09894801215302172, 'compression_ratio': 1.794392523364486, 'no_speech_prob': 0.0009392191423103213}, {'id': 426, 'seek': 242460, 'start': 2424.6, 'end': 2430.52, 'text': \" and that process of computing this line, this gradient line, is known as back propagation. So let's\", 'tokens': [50364, 293, 300, 1399, 295, 15866, 341, 1622, 11, 341, 16235, 1622, 11, 307, 2570, 382, 646, 38377, 13, 407, 718, 311, 50660], 'temperature': 0.0, 'avg_logprob': -0.07365890766711945, 'compression_ratio': 1.7105263157894737, 'no_speech_prob': 0.0002453206107020378}, {'id': 427, 'seek': 242460, 'start': 2430.52, 'end': 2437.64, 'text': \" do a very quick intro to back propagation and how it works. So again, let's start with the\", 'tokens': [50660, 360, 257, 588, 1702, 12897, 281, 646, 38377, 293, 577, 309, 1985, 13, 407, 797, 11, 718, 311, 722, 365, 264, 51016], 'temperature': 0.0, 'avg_logprob': -0.07365890766711945, 'compression_ratio': 1.7105263157894737, 'no_speech_prob': 0.0002453206107020378}, {'id': 428, 'seek': 242460, 'start': 2437.64, 'end': 2442.7599999999998, 'text': ' simplest neural network in existence. This neural network has one input, one output, and only one', 'tokens': [51016, 22811, 18161, 3209, 294, 9123, 13, 639, 18161, 3209, 575, 472, 4846, 11, 472, 5598, 11, 293, 787, 472, 51272], 'temperature': 0.0, 'avg_logprob': -0.07365890766711945, 'compression_ratio': 1.7105263157894737, 'no_speech_prob': 0.0002453206107020378}, {'id': 429, 'seek': 242460, 'start': 2442.7599999999998, 'end': 2448.92, 'text': ' neuron, right? This is as simple as it gets. We want to compute the gradient of our loss with respect', 'tokens': [51272, 34090, 11, 558, 30, 639, 307, 382, 2199, 382, 309, 2170, 13, 492, 528, 281, 14722, 264, 16235, 295, 527, 4470, 365, 3104, 51580], 'temperature': 0.0, 'avg_logprob': -0.07365890766711945, 'compression_ratio': 1.7105263157894737, 'no_speech_prob': 0.0002453206107020378}, {'id': 430, 'seek': 244892, 'start': 2448.92, 'end': 2453.16, 'text': \" to our weight. In this case, let's compute it with respect to W2, the second weight.\", 'tokens': [50364, 281, 527, 3364, 13, 682, 341, 1389, 11, 718, 311, 14722, 309, 365, 3104, 281, 343, 17, 11, 264, 1150, 3364, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.10268602118027949, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.005908672697842121}, {'id': 431, 'seek': 244892, 'start': 2454.12, 'end': 2461.16, 'text': ' So this derivative is going to tell us how much a small change in this weight will affect our loss.', 'tokens': [50624, 407, 341, 13760, 307, 516, 281, 980, 505, 577, 709, 257, 1359, 1319, 294, 341, 3364, 486, 3345, 527, 4470, 13, 50976], 'temperature': 0.0, 'avg_logprob': -0.10268602118027949, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.005908672697842121}, {'id': 432, 'seek': 244892, 'start': 2461.16, 'end': 2465.48, 'text': ' If a small change, if we change our weight a little bit in one direction, will it increase our loss', 'tokens': [50976, 759, 257, 1359, 1319, 11, 498, 321, 1319, 527, 3364, 257, 707, 857, 294, 472, 3513, 11, 486, 309, 3488, 527, 4470, 51192], 'temperature': 0.0, 'avg_logprob': -0.10268602118027949, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.005908672697842121}, {'id': 433, 'seek': 244892, 'start': 2465.48, 'end': 2471.16, 'text': ' or decrease our loss? So to compute that, we can write out this derivative. We can start with', 'tokens': [51192, 420, 11514, 527, 4470, 30, 407, 281, 14722, 300, 11, 321, 393, 2464, 484, 341, 13760, 13, 492, 393, 722, 365, 51476], 'temperature': 0.0, 'avg_logprob': -0.10268602118027949, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.005908672697842121}, {'id': 434, 'seek': 244892, 'start': 2471.16, 'end': 2477.2400000000002, 'text': ' applying the chain rule backwards from the loss function through the output. Specifically,', 'tokens': [51476, 9275, 264, 5021, 4978, 12204, 490, 264, 4470, 2445, 807, 264, 5598, 13, 26058, 11, 51780], 'temperature': 0.0, 'avg_logprob': -0.10268602118027949, 'compression_ratio': 1.8392156862745097, 'no_speech_prob': 0.005908672697842121}, {'id': 435, 'seek': 247724, 'start': 2477.24, 'end': 2482.6, 'text': ' what we can do is we can actually just decompose this derivative into two components. The first', 'tokens': [50364, 437, 321, 393, 360, 307, 321, 393, 767, 445, 22867, 541, 341, 13760, 666, 732, 6677, 13, 440, 700, 50632], 'temperature': 0.0, 'avg_logprob': -0.07867211438296887, 'compression_ratio': 1.7567567567567568, 'no_speech_prob': 0.0010811806423589587}, {'id': 436, 'seek': 247724, 'start': 2482.6, 'end': 2487.8799999999997, 'text': ' component is the derivative of our loss with respect to our output multiplied by the derivative of our', 'tokens': [50632, 6542, 307, 264, 13760, 295, 527, 4470, 365, 3104, 281, 527, 5598, 17207, 538, 264, 13760, 295, 527, 50896], 'temperature': 0.0, 'avg_logprob': -0.07867211438296887, 'compression_ratio': 1.7567567567567568, 'no_speech_prob': 0.0010811806423589587}, {'id': 437, 'seek': 247724, 'start': 2487.8799999999997, 'end': 2495.9599999999996, 'text': ' output with respect to W2, right? This is just a standard instantiation of the chain rule with', 'tokens': [50896, 5598, 365, 3104, 281, 343, 17, 11, 558, 30, 639, 307, 445, 257, 3832, 9836, 6642, 295, 264, 5021, 4978, 365, 51300], 'temperature': 0.0, 'avg_logprob': -0.07867211438296887, 'compression_ratio': 1.7567567567567568, 'no_speech_prob': 0.0010811806423589587}, {'id': 438, 'seek': 247724, 'start': 2495.9599999999996, 'end': 2501.08, 'text': \" this original derivative that we had on the left-hand side. Let's suppose we want to compute the\", 'tokens': [51300, 341, 3380, 13760, 300, 321, 632, 322, 264, 1411, 12, 5543, 1252, 13, 961, 311, 7297, 321, 528, 281, 14722, 264, 51556], 'temperature': 0.0, 'avg_logprob': -0.07867211438296887, 'compression_ratio': 1.7567567567567568, 'no_speech_prob': 0.0010811806423589587}, {'id': 439, 'seek': 250108, 'start': 2501.08, 'end': 2507.24, 'text': ' gradients of the weight before that, which in this case are not W1, but W, excuse me, not W2, but', 'tokens': [50364, 2771, 2448, 295, 264, 3364, 949, 300, 11, 597, 294, 341, 1389, 366, 406, 343, 16, 11, 457, 343, 11, 8960, 385, 11, 406, 343, 17, 11, 457, 50672], 'temperature': 0.0, 'avg_logprob': -0.10318036939277024, 'compression_ratio': 1.7406015037593985, 'no_speech_prob': 0.0014774681767448783}, {'id': 440, 'seek': 250108, 'start': 2507.24, 'end': 2513.96, 'text': ' W1. Well, all we do is replace W2 with W1 and that chain rule still holds, right? That same', 'tokens': [50672, 343, 16, 13, 1042, 11, 439, 321, 360, 307, 7406, 343, 17, 365, 343, 16, 293, 300, 5021, 4978, 920, 9190, 11, 558, 30, 663, 912, 51008], 'temperature': 0.0, 'avg_logprob': -0.10318036939277024, 'compression_ratio': 1.7406015037593985, 'no_speech_prob': 0.0014774681767448783}, {'id': 441, 'seek': 250108, 'start': 2513.96, 'end': 2519.3199999999997, 'text': ' equation holds, but now you can see on the red component, that last component of the chain rule,', 'tokens': [51008, 5367, 9190, 11, 457, 586, 291, 393, 536, 322, 264, 2182, 6542, 11, 300, 1036, 6542, 295, 264, 5021, 4978, 11, 51276], 'temperature': 0.0, 'avg_logprob': -0.10318036939277024, 'compression_ratio': 1.7406015037593985, 'no_speech_prob': 0.0014774681767448783}, {'id': 442, 'seek': 250108, 'start': 2519.3199999999997, 'end': 2524.44, 'text': \" we have to, once again, recursively apply one more chain rule because that's again another\", 'tokens': [51276, 321, 362, 281, 11, 1564, 797, 11, 20560, 3413, 3079, 472, 544, 5021, 4978, 570, 300, 311, 797, 1071, 51532], 'temperature': 0.0, 'avg_logprob': -0.10318036939277024, 'compression_ratio': 1.7406015037593985, 'no_speech_prob': 0.0014774681767448783}, {'id': 443, 'seek': 250108, 'start': 2524.44, 'end': 2528.52, 'text': \" derivative that we can't directly evaluate. We can expand that once more with another\", 'tokens': [51532, 13760, 300, 321, 393, 380, 3838, 13059, 13, 492, 393, 5268, 300, 1564, 544, 365, 1071, 51736], 'temperature': 0.0, 'avg_logprob': -0.10318036939277024, 'compression_ratio': 1.7406015037593985, 'no_speech_prob': 0.0014774681767448783}, {'id': 444, 'seek': 252852, 'start': 2528.52, 'end': 2534.04, 'text': ' instantiation of the chain rule. Now, all of these components, we can directly propagate these', 'tokens': [50364, 9836, 6642, 295, 264, 5021, 4978, 13, 823, 11, 439, 295, 613, 6677, 11, 321, 393, 3838, 48256, 613, 50640], 'temperature': 0.0, 'avg_logprob': -0.11140962947498669, 'compression_ratio': 1.7481751824817517, 'no_speech_prob': 0.0004953601164743304}, {'id': 445, 'seek': 252852, 'start': 2534.04, 'end': 2539.8, 'text': \" gradients through the hidden units in our neural network all the way back to the weight that we're\", 'tokens': [50640, 2771, 2448, 807, 264, 7633, 6815, 294, 527, 18161, 3209, 439, 264, 636, 646, 281, 264, 3364, 300, 321, 434, 50928], 'temperature': 0.0, 'avg_logprob': -0.11140962947498669, 'compression_ratio': 1.7481751824817517, 'no_speech_prob': 0.0004953601164743304}, {'id': 446, 'seek': 252852, 'start': 2539.8, 'end': 2543.96, 'text': ' interested in in this example, right? So we first computed the derivative with respect to W2,', 'tokens': [50928, 3102, 294, 294, 341, 1365, 11, 558, 30, 407, 321, 700, 40610, 264, 13760, 365, 3104, 281, 343, 17, 11, 51136], 'temperature': 0.0, 'avg_logprob': -0.11140962947498669, 'compression_ratio': 1.7481751824817517, 'no_speech_prob': 0.0004953601164743304}, {'id': 447, 'seek': 252852, 'start': 2544.6, 'end': 2548.68, 'text': \" then we can back propagate that and use that information also with W1. That's why we really\", 'tokens': [51168, 550, 321, 393, 646, 48256, 300, 293, 764, 300, 1589, 611, 365, 343, 16, 13, 663, 311, 983, 321, 534, 51372], 'temperature': 0.0, 'avg_logprob': -0.11140962947498669, 'compression_ratio': 1.7481751824817517, 'no_speech_prob': 0.0004953601164743304}, {'id': 448, 'seek': 252852, 'start': 2548.68, 'end': 2553.08, 'text': ' call it back propagation because this process occurs from the output all the way back to the input.', 'tokens': [51372, 818, 309, 646, 38377, 570, 341, 1399, 11843, 490, 264, 5598, 439, 264, 636, 646, 281, 264, 4846, 13, 51592], 'temperature': 0.0, 'avg_logprob': -0.11140962947498669, 'compression_ratio': 1.7481751824817517, 'no_speech_prob': 0.0004953601164743304}, {'id': 449, 'seek': 255308, 'start': 2553.72, 'end': 2560.52, 'text': ' Now, we repeat this process essentially many, many times over the course of training by propagating', 'tokens': [50396, 823, 11, 321, 7149, 341, 1399, 4476, 867, 11, 867, 1413, 670, 264, 1164, 295, 3097, 538, 12425, 990, 50736], 'temperature': 0.0, 'avg_logprob': -0.13646847702736079, 'compression_ratio': 1.6794871794871795, 'no_speech_prob': 0.0007205783622339368}, {'id': 450, 'seek': 255308, 'start': 2560.52, 'end': 2565.4, 'text': ' these gradients over and over again through the network all the way from the output to the inputs', 'tokens': [50736, 613, 2771, 2448, 670, 293, 670, 797, 807, 264, 3209, 439, 264, 636, 490, 264, 5598, 281, 264, 15743, 50980], 'temperature': 0.0, 'avg_logprob': -0.13646847702736079, 'compression_ratio': 1.6794871794871795, 'no_speech_prob': 0.0007205783622339368}, {'id': 451, 'seek': 255308, 'start': 2565.4, 'end': 2571.24, 'text': ' to determine for every single weight answering this question, which is how much does a small change', 'tokens': [50980, 281, 6997, 337, 633, 2167, 3364, 13430, 341, 1168, 11, 597, 307, 577, 709, 775, 257, 1359, 1319, 51272], 'temperature': 0.0, 'avg_logprob': -0.13646847702736079, 'compression_ratio': 1.6794871794871795, 'no_speech_prob': 0.0007205783622339368}, {'id': 452, 'seek': 255308, 'start': 2571.24, 'end': 2576.2, 'text': ' in these weights affect our loss function if it increases, it reduces, then how we can use that', 'tokens': [51272, 294, 613, 17443, 3345, 527, 4470, 2445, 498, 309, 8637, 11, 309, 18081, 11, 550, 577, 321, 393, 764, 300, 51520], 'temperature': 0.0, 'avg_logprob': -0.13646847702736079, 'compression_ratio': 1.6794871794871795, 'no_speech_prob': 0.0007205783622339368}, {'id': 453, 'seek': 257620, 'start': 2576.7599999999998, 'end': 2580.4399999999996, 'text': \" improve the loss ultimately because that's our final goal in this class.\", 'tokens': [50392, 3470, 264, 4470, 6284, 570, 300, 311, 527, 2572, 3387, 294, 341, 1508, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.13087794221477744, 'compression_ratio': 1.665158371040724, 'no_speech_prob': 0.0007669563638046384}, {'id': 454, 'seek': 257620, 'start': 2582.68, 'end': 2588.9199999999996, 'text': \" So that's the back propagation algorithm. That's the core of training neural networks. In theory,\", 'tokens': [50688, 407, 300, 311, 264, 646, 38377, 9284, 13, 663, 311, 264, 4965, 295, 3097, 18161, 9590, 13, 682, 5261, 11, 51000], 'temperature': 0.0, 'avg_logprob': -0.13087794221477744, 'compression_ratio': 1.665158371040724, 'no_speech_prob': 0.0007669563638046384}, {'id': 455, 'seek': 257620, 'start': 2588.9199999999996, 'end': 2596.4399999999996, 'text': \" it's very simple. It's really just an instantiation of the chain rule. But let's touch on some insights\", 'tokens': [51000, 309, 311, 588, 2199, 13, 467, 311, 534, 445, 364, 9836, 6642, 295, 264, 5021, 4978, 13, 583, 718, 311, 2557, 322, 512, 14310, 51376], 'temperature': 0.0, 'avg_logprob': -0.13087794221477744, 'compression_ratio': 1.665158371040724, 'no_speech_prob': 0.0007669563638046384}, {'id': 456, 'seek': 257620, 'start': 2596.4399999999996, 'end': 2601.24, 'text': ' that make training neural networks actually extremely complicated in practice even though the', 'tokens': [51376, 300, 652, 3097, 18161, 9590, 767, 4664, 6179, 294, 3124, 754, 1673, 264, 51616], 'temperature': 0.0, 'avg_logprob': -0.13087794221477744, 'compression_ratio': 1.665158371040724, 'no_speech_prob': 0.0007669563638046384}, {'id': 457, 'seek': 260124, 'start': 2601.24, 'end': 2609.16, 'text': ' algorithm of back propagation is simple and many decades old. In practice, though, optimization of', 'tokens': [50364, 9284, 295, 646, 38377, 307, 2199, 293, 867, 7878, 1331, 13, 682, 3124, 11, 1673, 11, 19618, 295, 50760], 'temperature': 0.0, 'avg_logprob': -0.08662291406427772, 'compression_ratio': 1.8407407407407408, 'no_speech_prob': 0.0018951967358589172}, {'id': 458, 'seek': 260124, 'start': 2609.16, 'end': 2613.7999999999997, 'text': ' neural networks looks something like this. It looks nothing like that picture that I showed you before.', 'tokens': [50760, 18161, 9590, 1542, 746, 411, 341, 13, 467, 1542, 1825, 411, 300, 3036, 300, 286, 4712, 291, 949, 13, 50992], 'temperature': 0.0, 'avg_logprob': -0.08662291406427772, 'compression_ratio': 1.8407407407407408, 'no_speech_prob': 0.0018951967358589172}, {'id': 459, 'seek': 260124, 'start': 2613.7999999999997, 'end': 2618.68, 'text': ' There are ways that we can visualize very large deep neural networks and you can think of the', 'tokens': [50992, 821, 366, 2098, 300, 321, 393, 23273, 588, 2416, 2452, 18161, 9590, 293, 291, 393, 519, 295, 264, 51236], 'temperature': 0.0, 'avg_logprob': -0.08662291406427772, 'compression_ratio': 1.8407407407407408, 'no_speech_prob': 0.0018951967358589172}, {'id': 460, 'seek': 260124, 'start': 2618.68, 'end': 2623.08, 'text': ' landscape of these models looking like something like this. This is an illustration from a paper that', 'tokens': [51236, 9661, 295, 613, 5245, 1237, 411, 746, 411, 341, 13, 639, 307, 364, 22645, 490, 257, 3035, 300, 51456], 'temperature': 0.0, 'avg_logprob': -0.08662291406427772, 'compression_ratio': 1.8407407407407408, 'no_speech_prob': 0.0018951967358589172}, {'id': 461, 'seek': 260124, 'start': 2623.08, 'end': 2627.7999999999997, 'text': ' came out several years ago where they tried to actually visualize the landscape of very, very deep', 'tokens': [51456, 1361, 484, 2940, 924, 2057, 689, 436, 3031, 281, 767, 23273, 264, 9661, 295, 588, 11, 588, 2452, 51692], 'temperature': 0.0, 'avg_logprob': -0.08662291406427772, 'compression_ratio': 1.8407407407407408, 'no_speech_prob': 0.0018951967358589172}, {'id': 462, 'seek': 262780, 'start': 2627.8, 'end': 2631.5600000000004, 'text': \" neural networks. That's what this landscape actually looks like. That's what you're trying to\", 'tokens': [50364, 18161, 9590, 13, 663, 311, 437, 341, 9661, 767, 1542, 411, 13, 663, 311, 437, 291, 434, 1382, 281, 50552], 'temperature': 0.0, 'avg_logprob': -0.10597376312528338, 'compression_ratio': 1.6782006920415224, 'no_speech_prob': 0.0029330498073250055}, {'id': 463, 'seek': 262780, 'start': 2631.5600000000004, 'end': 2635.88, 'text': ' deal with and find the minimum in this space. You can imagine the challenges that come with that.', 'tokens': [50552, 2028, 365, 293, 915, 264, 7285, 294, 341, 1901, 13, 509, 393, 3811, 264, 4759, 300, 808, 365, 300, 13, 50768], 'temperature': 0.0, 'avg_logprob': -0.10597376312528338, 'compression_ratio': 1.6782006920415224, 'no_speech_prob': 0.0029330498073250055}, {'id': 464, 'seek': 262780, 'start': 2637.8, 'end': 2643.48, 'text': \" To cover the challenges, let's first think of and recall that update equation defined in gradient\", 'tokens': [50864, 1407, 2060, 264, 4759, 11, 718, 311, 700, 519, 295, 293, 9901, 300, 5623, 5367, 7642, 294, 16235, 51148], 'temperature': 0.0, 'avg_logprob': -0.10597376312528338, 'compression_ratio': 1.6782006920415224, 'no_speech_prob': 0.0029330498073250055}, {'id': 465, 'seek': 262780, 'start': 2643.48, 'end': 2649.6400000000003, 'text': \" descent. I didn't talk too much about this parameter, ADA, but now let's spend a bit of time\", 'tokens': [51148, 23475, 13, 286, 994, 380, 751, 886, 709, 466, 341, 13075, 11, 39354, 11, 457, 586, 718, 311, 3496, 257, 857, 295, 565, 51456], 'temperature': 0.0, 'avg_logprob': -0.10597376312528338, 'compression_ratio': 1.6782006920415224, 'no_speech_prob': 0.0029330498073250055}, {'id': 466, 'seek': 262780, 'start': 2649.6400000000003, 'end': 2654.76, 'text': ' thinking about this. This is called the learning rate, like we saw before. It determines basically how', 'tokens': [51456, 1953, 466, 341, 13, 639, 307, 1219, 264, 2539, 3314, 11, 411, 321, 1866, 949, 13, 467, 24799, 1936, 577, 51712], 'temperature': 0.0, 'avg_logprob': -0.10597376312528338, 'compression_ratio': 1.6782006920415224, 'no_speech_prob': 0.0029330498073250055}, {'id': 467, 'seek': 265476, 'start': 2654.76, 'end': 2659.5600000000004, 'text': ' big of a step we need to take in the direction of our gradient and every single iteration of', 'tokens': [50364, 955, 295, 257, 1823, 321, 643, 281, 747, 294, 264, 3513, 295, 527, 16235, 293, 633, 2167, 24784, 295, 50604], 'temperature': 0.0, 'avg_logprob': -0.08430548831149265, 'compression_ratio': 1.7188612099644127, 'no_speech_prob': 0.0006560128531418741}, {'id': 468, 'seek': 265476, 'start': 2659.5600000000004, 'end': 2665.8, 'text': ' back propagation. In practice, even setting the learning rate can be very challenging. You as', 'tokens': [50604, 646, 38377, 13, 682, 3124, 11, 754, 3287, 264, 2539, 3314, 393, 312, 588, 7595, 13, 509, 382, 50916], 'temperature': 0.0, 'avg_logprob': -0.08430548831149265, 'compression_ratio': 1.7188612099644127, 'no_speech_prob': 0.0006560128531418741}, {'id': 469, 'seek': 265476, 'start': 2665.8, 'end': 2670.5200000000004, 'text': ' the designer of the neural network have to set this value, this learning rate, and how do you pick', 'tokens': [50916, 264, 11795, 295, 264, 18161, 3209, 362, 281, 992, 341, 2158, 11, 341, 2539, 3314, 11, 293, 577, 360, 291, 1888, 51152], 'temperature': 0.0, 'avg_logprob': -0.08430548831149265, 'compression_ratio': 1.7188612099644127, 'no_speech_prob': 0.0006560128531418741}, {'id': 470, 'seek': 265476, 'start': 2670.5200000000004, 'end': 2676.5200000000004, 'text': ' this value? That can actually be quite difficult. It has really large consequences when building a', 'tokens': [51152, 341, 2158, 30, 663, 393, 767, 312, 1596, 2252, 13, 467, 575, 534, 2416, 10098, 562, 2390, 257, 51452], 'temperature': 0.0, 'avg_logprob': -0.08430548831149265, 'compression_ratio': 1.7188612099644127, 'no_speech_prob': 0.0006560128531418741}, {'id': 471, 'seek': 265476, 'start': 2676.5200000000004, 'end': 2684.28, 'text': \" neural network. For example, if we set the learning rate too low, then we learn very slowly. Let's\", 'tokens': [51452, 18161, 3209, 13, 1171, 1365, 11, 498, 321, 992, 264, 2539, 3314, 886, 2295, 11, 550, 321, 1466, 588, 5692, 13, 961, 311, 51840], 'temperature': 0.0, 'avg_logprob': -0.08430548831149265, 'compression_ratio': 1.7188612099644127, 'no_speech_prob': 0.0006560128531418741}, {'id': 472, 'seek': 268428, 'start': 2684.28, 'end': 2688.52, 'text': ' assume we start on the right-hand side here at that initial guess. If our learning rate is not', 'tokens': [50364, 6552, 321, 722, 322, 264, 558, 12, 5543, 1252, 510, 412, 300, 5883, 2041, 13, 759, 527, 2539, 3314, 307, 406, 50576], 'temperature': 0.0, 'avg_logprob': -0.09721362696284741, 'compression_ratio': 1.7610294117647058, 'no_speech_prob': 0.0003248770080972463}, {'id': 473, 'seek': 268428, 'start': 2688.52, 'end': 2693.8, 'text': \" large enough, not only do we converge slowly, we actually don't even converge to the global\", 'tokens': [50576, 2416, 1547, 11, 406, 787, 360, 321, 41881, 5692, 11, 321, 767, 500, 380, 754, 41881, 281, 264, 4338, 50840], 'temperature': 0.0, 'avg_logprob': -0.09721362696284741, 'compression_ratio': 1.7610294117647058, 'no_speech_prob': 0.0003248770080972463}, {'id': 474, 'seek': 268428, 'start': 2693.8, 'end': 2700.0400000000004, 'text': ' minimum, because we kind of get stuck in a local minimum. What if we set our learning rate too high?', 'tokens': [50840, 7285, 11, 570, 321, 733, 295, 483, 5541, 294, 257, 2654, 7285, 13, 708, 498, 321, 992, 527, 2539, 3314, 886, 1090, 30, 51152], 'temperature': 0.0, 'avg_logprob': -0.09721362696284741, 'compression_ratio': 1.7610294117647058, 'no_speech_prob': 0.0003248770080972463}, {'id': 475, 'seek': 268428, 'start': 2700.0400000000004, 'end': 2705.32, 'text': ' What can actually happen is we overshoot and we can actually start to diverge from the solution.', 'tokens': [51152, 708, 393, 767, 1051, 307, 321, 15488, 24467, 293, 321, 393, 767, 722, 281, 18558, 432, 490, 264, 3827, 13, 51416], 'temperature': 0.0, 'avg_logprob': -0.09721362696284741, 'compression_ratio': 1.7610294117647058, 'no_speech_prob': 0.0003248770080972463}, {'id': 476, 'seek': 268428, 'start': 2705.32, 'end': 2709.88, 'text': \" The gradients can actually explode. Very bad things happen and then the neural network doesn't\", 'tokens': [51416, 440, 2771, 2448, 393, 767, 21411, 13, 4372, 1578, 721, 1051, 293, 550, 264, 18161, 3209, 1177, 380, 51644], 'temperature': 0.0, 'avg_logprob': -0.09721362696284741, 'compression_ratio': 1.7610294117647058, 'no_speech_prob': 0.0003248770080972463}, {'id': 477, 'seek': 270988, 'start': 2710.6, 'end': 2715.96, 'text': \" train. That's also not good. In reality, there's a very happy medium between setting a too small,\", 'tokens': [50400, 3847, 13, 663, 311, 611, 406, 665, 13, 682, 4103, 11, 456, 311, 257, 588, 2055, 6399, 1296, 3287, 257, 886, 1359, 11, 50668], 'temperature': 0.0, 'avg_logprob': -0.12919237654087906, 'compression_ratio': 1.7482014388489209, 'no_speech_prob': 0.0022860695607960224}, {'id': 478, 'seek': 270988, 'start': 2715.96, 'end': 2721.2400000000002, 'text': ' setting a too large, where you set it just large enough to kind of overshoot some of these local', 'tokens': [50668, 3287, 257, 886, 2416, 11, 689, 291, 992, 309, 445, 2416, 1547, 281, 733, 295, 15488, 24467, 512, 295, 613, 2654, 50932], 'temperature': 0.0, 'avg_logprob': -0.12919237654087906, 'compression_ratio': 1.7482014388489209, 'no_speech_prob': 0.0022860695607960224}, {'id': 479, 'seek': 270988, 'start': 2721.2400000000002, 'end': 2726.52, 'text': ' minima, put you into a reasonable part of the search space, where then you can actually converge', 'tokens': [50932, 4464, 64, 11, 829, 291, 666, 257, 10585, 644, 295, 264, 3164, 1901, 11, 689, 550, 291, 393, 767, 41881, 51196], 'temperature': 0.0, 'avg_logprob': -0.12919237654087906, 'compression_ratio': 1.7482014388489209, 'no_speech_prob': 0.0022860695607960224}, {'id': 480, 'seek': 270988, 'start': 2726.52, 'end': 2731.7200000000003, 'text': ' on the solutions that you care most about. But actually, how do you set these learning rates in', 'tokens': [51196, 322, 264, 6547, 300, 291, 1127, 881, 466, 13, 583, 767, 11, 577, 360, 291, 992, 613, 2539, 6846, 294, 51456], 'temperature': 0.0, 'avg_logprob': -0.12919237654087906, 'compression_ratio': 1.7482014388489209, 'no_speech_prob': 0.0022860695607960224}, {'id': 481, 'seek': 270988, 'start': 2731.7200000000003, 'end': 2737.0, 'text': ' practice? How do you pick what is the ideal learning rate? One option, and this is actually a very', 'tokens': [51456, 3124, 30, 1012, 360, 291, 1888, 437, 307, 264, 7157, 2539, 3314, 30, 1485, 3614, 11, 293, 341, 307, 767, 257, 588, 51720], 'temperature': 0.0, 'avg_logprob': -0.12919237654087906, 'compression_ratio': 1.7482014388489209, 'no_speech_prob': 0.0022860695607960224}, {'id': 482, 'seek': 273700, 'start': 2737.0, 'end': 2741.88, 'text': ' common option in practices to simply try out a bunch of learning rates and see what works the', 'tokens': [50364, 2689, 3614, 294, 7525, 281, 2935, 853, 484, 257, 3840, 295, 2539, 6846, 293, 536, 437, 1985, 264, 50608], 'temperature': 0.0, 'avg_logprob': -0.10271191596984863, 'compression_ratio': 1.8239700374531835, 'no_speech_prob': 0.0010810522362589836}, {'id': 483, 'seek': 273700, 'start': 2741.88, 'end': 2747.8, 'text': \" best. Let's say a whole grade of different learning rates and train all of these neural networks,\", 'tokens': [50608, 1151, 13, 961, 311, 584, 257, 1379, 7204, 295, 819, 2539, 6846, 293, 3847, 439, 295, 613, 18161, 9590, 11, 50904], 'temperature': 0.0, 'avg_logprob': -0.10271191596984863, 'compression_ratio': 1.8239700374531835, 'no_speech_prob': 0.0010810522362589836}, {'id': 484, 'seek': 273700, 'start': 2747.8, 'end': 2753.32, 'text': ' see which one works the best. But I think we can do something a lot smarter. What are some more', 'tokens': [50904, 536, 597, 472, 1985, 264, 1151, 13, 583, 286, 519, 321, 393, 360, 746, 257, 688, 20294, 13, 708, 366, 512, 544, 51180], 'temperature': 0.0, 'avg_logprob': -0.10271191596984863, 'compression_ratio': 1.8239700374531835, 'no_speech_prob': 0.0010810522362589836}, {'id': 485, 'seek': 273700, 'start': 2753.32, 'end': 2757.4, 'text': ' intelligent ways that we could do this instead of exhaustively trying out a whole bunch of different', 'tokens': [51180, 13232, 2098, 300, 321, 727, 360, 341, 2602, 295, 14687, 3413, 1382, 484, 257, 1379, 3840, 295, 819, 51384], 'temperature': 0.0, 'avg_logprob': -0.10271191596984863, 'compression_ratio': 1.8239700374531835, 'no_speech_prob': 0.0010810522362589836}, {'id': 486, 'seek': 273700, 'start': 2757.4, 'end': 2763.48, 'text': ' learning rates? Can we design a learning rate algorithm that actually adapts to our neural network', 'tokens': [51384, 2539, 6846, 30, 1664, 321, 1715, 257, 2539, 3314, 9284, 300, 767, 23169, 1373, 281, 527, 18161, 3209, 51688], 'temperature': 0.0, 'avg_logprob': -0.10271191596984863, 'compression_ratio': 1.8239700374531835, 'no_speech_prob': 0.0010810522362589836}, {'id': 487, 'seek': 276348, 'start': 2763.48, 'end': 2767.96, 'text': \" and adapts to its landscape so that it's a bit more intelligent than that previous idea?\", 'tokens': [50364, 293, 23169, 1373, 281, 1080, 9661, 370, 300, 309, 311, 257, 857, 544, 13232, 813, 300, 3894, 1558, 30, 50588], 'temperature': 0.0, 'avg_logprob': -0.12169136090225048, 'compression_ratio': 1.7330316742081449, 'no_speech_prob': 0.001031813444569707}, {'id': 488, 'seek': 276348, 'start': 2769.48, 'end': 2776.84, 'text': ' So this really ultimately means that the learning rate, the speed at which the algorithm is trusting', 'tokens': [50664, 407, 341, 534, 6284, 1355, 300, 264, 2539, 3314, 11, 264, 3073, 412, 597, 264, 9284, 307, 28235, 51032], 'temperature': 0.0, 'avg_logprob': -0.12169136090225048, 'compression_ratio': 1.7330316742081449, 'no_speech_prob': 0.001031813444569707}, {'id': 489, 'seek': 276348, 'start': 2776.84, 'end': 2782.28, 'text': ' the gradients that it sees, is going to depend on how large the gradient is in that location', 'tokens': [51032, 264, 2771, 2448, 300, 309, 8194, 11, 307, 516, 281, 5672, 322, 577, 2416, 264, 16235, 307, 294, 300, 4914, 51304], 'temperature': 0.0, 'avg_logprob': -0.12169136090225048, 'compression_ratio': 1.7330316742081449, 'no_speech_prob': 0.001031813444569707}, {'id': 490, 'seek': 276348, 'start': 2783.0, 'end': 2789.88, 'text': \" and how fast we're learning. How many other options, and sorry, and many other options that we might\", 'tokens': [51340, 293, 577, 2370, 321, 434, 2539, 13, 1012, 867, 661, 3956, 11, 293, 2597, 11, 293, 867, 661, 3956, 300, 321, 1062, 51684], 'temperature': 0.0, 'avg_logprob': -0.12169136090225048, 'compression_ratio': 1.7330316742081449, 'no_speech_prob': 0.001031813444569707}, {'id': 491, 'seek': 278988, 'start': 2789.88, 'end': 2794.04, 'text': \" have as part of training and neural networks, right? So it's not only how quickly we're learning,\", 'tokens': [50364, 362, 382, 644, 295, 3097, 293, 18161, 9590, 11, 558, 30, 407, 309, 311, 406, 787, 577, 2661, 321, 434, 2539, 11, 50572], 'temperature': 0.0, 'avg_logprob': -0.11614657864712252, 'compression_ratio': 1.7977941176470589, 'no_speech_prob': 0.00013976301124785095}, {'id': 492, 'seek': 278988, 'start': 2794.04, 'end': 2800.84, 'text': \" you may judge it on many different factors in the learning landscape. In fact, we've all been\", 'tokens': [50572, 291, 815, 6995, 309, 322, 867, 819, 6771, 294, 264, 2539, 9661, 13, 682, 1186, 11, 321, 600, 439, 668, 50912], 'temperature': 0.0, 'avg_logprob': -0.11614657864712252, 'compression_ratio': 1.7977941176470589, 'no_speech_prob': 0.00013976301124785095}, {'id': 493, 'seek': 278988, 'start': 2801.56, 'end': 2806.52, 'text': \" these different algorithms that I'm talking about, these adaptive learning rate algorithms have been\", 'tokens': [50948, 613, 819, 14642, 300, 286, 478, 1417, 466, 11, 613, 27912, 2539, 3314, 14642, 362, 668, 51196], 'temperature': 0.0, 'avg_logprob': -0.11614657864712252, 'compression_ratio': 1.7977941176470589, 'no_speech_prob': 0.00013976301124785095}, {'id': 494, 'seek': 278988, 'start': 2806.52, 'end': 2812.52, 'text': ' very widely studied in practice. There is a very thriving community in the deep learning research', 'tokens': [51196, 588, 13371, 9454, 294, 3124, 13, 821, 307, 257, 588, 30643, 1768, 294, 264, 2452, 2539, 2132, 51496], 'temperature': 0.0, 'avg_logprob': -0.11614657864712252, 'compression_ratio': 1.7977941176470589, 'no_speech_prob': 0.00013976301124785095}, {'id': 495, 'seek': 278988, 'start': 2812.52, 'end': 2819.1600000000003, 'text': ' community that focuses on developing and designing new algorithms for learning rate adaptation and', 'tokens': [51496, 1768, 300, 16109, 322, 6416, 293, 14685, 777, 14642, 337, 2539, 3314, 21549, 293, 51828], 'temperature': 0.0, 'avg_logprob': -0.11614657864712252, 'compression_ratio': 1.7977941176470589, 'no_speech_prob': 0.00013976301124785095}, {'id': 496, 'seek': 281916, 'start': 2819.16, 'end': 2825.0, 'text': \" faster optimization of large neural networks like these. And during your labs, you'll actually get the\", 'tokens': [50364, 4663, 19618, 295, 2416, 18161, 9590, 411, 613, 13, 400, 1830, 428, 20339, 11, 291, 603, 767, 483, 264, 50656], 'temperature': 0.0, 'avg_logprob': -0.07665233783893757, 'compression_ratio': 1.7419354838709677, 'no_speech_prob': 0.0004236377135384828}, {'id': 497, 'seek': 281916, 'start': 2825.0, 'end': 2830.68, 'text': ' opportunity to not only try out a lot of these different adaptive algorithms, which you can see here,', 'tokens': [50656, 2650, 281, 406, 787, 853, 484, 257, 688, 295, 613, 819, 27912, 14642, 11, 597, 291, 393, 536, 510, 11, 50940], 'temperature': 0.0, 'avg_logprob': -0.07665233783893757, 'compression_ratio': 1.7419354838709677, 'no_speech_prob': 0.0004236377135384828}, {'id': 498, 'seek': 281916, 'start': 2830.68, 'end': 2835.3199999999997, 'text': ' but also try to uncover what are kind of the patterns and benefits of one versus the other. And', 'tokens': [50940, 457, 611, 853, 281, 21694, 437, 366, 733, 295, 264, 8294, 293, 5311, 295, 472, 5717, 264, 661, 13, 400, 51172], 'temperature': 0.0, 'avg_logprob': -0.07665233783893757, 'compression_ratio': 1.7419354838709677, 'no_speech_prob': 0.0004236377135384828}, {'id': 499, 'seek': 281916, 'start': 2835.3199999999997, 'end': 2840.2, 'text': \" that's going to be something that I think you'll find very insightful as part of your labs.\", 'tokens': [51172, 300, 311, 516, 281, 312, 746, 300, 286, 519, 291, 603, 915, 588, 46401, 382, 644, 295, 428, 20339, 13, 51416], 'temperature': 0.0, 'avg_logprob': -0.07665233783893757, 'compression_ratio': 1.7419354838709677, 'no_speech_prob': 0.0004236377135384828}, {'id': 500, 'seek': 281916, 'start': 2841.3199999999997, 'end': 2845.64, 'text': \" So another key component of your labs that you'll see is how you can actually put all of this\", 'tokens': [51472, 407, 1071, 2141, 6542, 295, 428, 20339, 300, 291, 603, 536, 307, 577, 291, 393, 767, 829, 439, 295, 341, 51688], 'temperature': 0.0, 'avg_logprob': -0.07665233783893757, 'compression_ratio': 1.7419354838709677, 'no_speech_prob': 0.0004236377135384828}, {'id': 501, 'seek': 284564, 'start': 2845.64, 'end': 2850.52, 'text': \" information that we've covered today into a single picture that looks roughly something like this,\", 'tokens': [50364, 1589, 300, 321, 600, 5343, 965, 666, 257, 2167, 3036, 300, 1542, 9810, 746, 411, 341, 11, 50608], 'temperature': 0.0, 'avg_logprob': -0.09281910711259984, 'compression_ratio': 1.869281045751634, 'no_speech_prob': 0.0012249504216015339}, {'id': 502, 'seek': 284564, 'start': 2850.52, 'end': 2854.92, 'text': \" which defines your model at the first, at the top here. That's where you define your model,\", 'tokens': [50608, 597, 23122, 428, 2316, 412, 264, 700, 11, 412, 264, 1192, 510, 13, 663, 311, 689, 291, 6964, 428, 2316, 11, 50828], 'temperature': 0.0, 'avg_logprob': -0.09281910711259984, 'compression_ratio': 1.869281045751634, 'no_speech_prob': 0.0012249504216015339}, {'id': 503, 'seek': 284564, 'start': 2854.92, 'end': 2859.8799999999997, 'text': ' where you talked about this in the beginning part of the lecture. For every piece in your model,', 'tokens': [50828, 689, 291, 2825, 466, 341, 294, 264, 2863, 644, 295, 264, 7991, 13, 1171, 633, 2522, 294, 428, 2316, 11, 51076], 'temperature': 0.0, 'avg_logprob': -0.09281910711259984, 'compression_ratio': 1.869281045751634, 'no_speech_prob': 0.0012249504216015339}, {'id': 504, 'seek': 284564, 'start': 2859.8799999999997, 'end': 2864.52, 'text': \" you're now going to need to define this optimizer, which we've just talked about. This optimizer is\", 'tokens': [51076, 291, 434, 586, 516, 281, 643, 281, 6964, 341, 5028, 6545, 11, 597, 321, 600, 445, 2825, 466, 13, 639, 5028, 6545, 307, 51308], 'temperature': 0.0, 'avg_logprob': -0.09281910711259984, 'compression_ratio': 1.869281045751634, 'no_speech_prob': 0.0012249504216015339}, {'id': 505, 'seek': 284564, 'start': 2864.52, 'end': 2869.96, 'text': ' defined together with a learning rate, right? How quickly you want to optimize your loss landscape,', 'tokens': [51308, 7642, 1214, 365, 257, 2539, 3314, 11, 558, 30, 1012, 2661, 291, 528, 281, 19719, 428, 4470, 9661, 11, 51580], 'temperature': 0.0, 'avg_logprob': -0.09281910711259984, 'compression_ratio': 1.869281045751634, 'no_speech_prob': 0.0012249504216015339}, {'id': 506, 'seek': 284564, 'start': 2869.96, 'end': 2873.8799999999997, 'text': \" and over many loops, you're going to pass over all of the examples in your data set,\", 'tokens': [51580, 293, 670, 867, 16121, 11, 291, 434, 516, 281, 1320, 670, 439, 295, 264, 5110, 294, 428, 1412, 992, 11, 51776], 'temperature': 0.0, 'avg_logprob': -0.09281910711259984, 'compression_ratio': 1.869281045751634, 'no_speech_prob': 0.0012249504216015339}, {'id': 507, 'seek': 287388, 'start': 2874.6800000000003, 'end': 2879.48, 'text': \" and observe essentially how to improve your network. That's the gradient, and then actually\", 'tokens': [50404, 293, 11441, 4476, 577, 281, 3470, 428, 3209, 13, 663, 311, 264, 16235, 11, 293, 550, 767, 50644], 'temperature': 0.0, 'avg_logprob': -0.12386964291942362, 'compression_ratio': 1.73046875, 'no_speech_prob': 0.000868885894306004}, {'id': 508, 'seek': 287388, 'start': 2879.48, 'end': 2883.08, 'text': ' improve the network in those directions. And keep doing that over and over and over again,', 'tokens': [50644, 3470, 264, 3209, 294, 729, 11095, 13, 400, 1066, 884, 300, 670, 293, 670, 293, 670, 797, 11, 50824], 'temperature': 0.0, 'avg_logprob': -0.12386964291942362, 'compression_ratio': 1.73046875, 'no_speech_prob': 0.000868885894306004}, {'id': 509, 'seek': 287388, 'start': 2883.08, 'end': 2887.08, 'text': ' until eventually your neural network converges to some sort of solution.', 'tokens': [50824, 1826, 4728, 428, 18161, 3209, 9652, 2880, 281, 512, 1333, 295, 3827, 13, 51024], 'temperature': 0.0, 'avg_logprob': -0.12386964291942362, 'compression_ratio': 1.73046875, 'no_speech_prob': 0.000868885894306004}, {'id': 510, 'seek': 287388, 'start': 2889.7200000000003, 'end': 2894.44, 'text': ' So I want to very quickly, briefly, in the remaining time that we have, continue to talk about', 'tokens': [51156, 407, 286, 528, 281, 588, 2661, 11, 10515, 11, 294, 264, 8877, 565, 300, 321, 362, 11, 2354, 281, 751, 466, 51392], 'temperature': 0.0, 'avg_logprob': -0.12386964291942362, 'compression_ratio': 1.73046875, 'no_speech_prob': 0.000868885894306004}, {'id': 511, 'seek': 287388, 'start': 2894.44, 'end': 2901.32, 'text': ' tips for training these neural networks in practice, and focus on this very powerful idea of', 'tokens': [51392, 6082, 337, 3097, 613, 18161, 9590, 294, 3124, 11, 293, 1879, 322, 341, 588, 4005, 1558, 295, 51736], 'temperature': 0.0, 'avg_logprob': -0.12386964291942362, 'compression_ratio': 1.73046875, 'no_speech_prob': 0.000868885894306004}, {'id': 512, 'seek': 290132, 'start': 2901.32, 'end': 2907.6400000000003, 'text': ' batching your data into what are called mini batches of smaller pieces of data.', 'tokens': [50364, 15245, 278, 428, 1412, 666, 437, 366, 1219, 8382, 15245, 279, 295, 4356, 3755, 295, 1412, 13, 50680], 'temperature': 0.0, 'avg_logprob': -0.12424922269933364, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.0006459939759224653}, {'id': 513, 'seek': 290132, 'start': 2908.44, 'end': 2913.48, 'text': \" To do this, let's revisit that gradient descent algorithm, right? So here, this gradient that we\", 'tokens': [50720, 1407, 360, 341, 11, 718, 311, 32676, 300, 16235, 23475, 9284, 11, 558, 30, 407, 510, 11, 341, 16235, 300, 321, 50972], 'temperature': 0.0, 'avg_logprob': -0.12424922269933364, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.0006459939759224653}, {'id': 514, 'seek': 290132, 'start': 2913.48, 'end': 2919.7200000000003, 'text': \" talked about before is actually extraordinarily computationally expensive to compute, because it's\", 'tokens': [50972, 2825, 466, 949, 307, 767, 34557, 24903, 379, 5124, 281, 14722, 11, 570, 309, 311, 51284], 'temperature': 0.0, 'avg_logprob': -0.12424922269933364, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.0006459939759224653}, {'id': 515, 'seek': 290132, 'start': 2919.7200000000003, 'end': 2926.6800000000003, 'text': ' computed as a summation across all of the pieces in your data set, right? And in most real life,', 'tokens': [51284, 40610, 382, 257, 28811, 2108, 439, 295, 264, 3755, 294, 428, 1412, 992, 11, 558, 30, 400, 294, 881, 957, 993, 11, 51632], 'temperature': 0.0, 'avg_logprob': -0.12424922269933364, 'compression_ratio': 1.6533333333333333, 'no_speech_prob': 0.0006459939759224653}, {'id': 516, 'seek': 292668, 'start': 2926.68, 'end': 2932.6, 'text': \" for real world problems, it's simply not feasible to compute a gradient over your entire data set.\", 'tokens': [50364, 337, 957, 1002, 2740, 11, 309, 311, 2935, 406, 26648, 281, 14722, 257, 16235, 670, 428, 2302, 1412, 992, 13, 50660], 'temperature': 0.0, 'avg_logprob': -0.09906122901222923, 'compression_ratio': 1.817490494296578, 'no_speech_prob': 0.0004511528823059052}, {'id': 517, 'seek': 292668, 'start': 2932.6, 'end': 2938.44, 'text': ' Data sets are just too large these days. So there are some alternatives, right? What are the', 'tokens': [50660, 11888, 6352, 366, 445, 886, 2416, 613, 1708, 13, 407, 456, 366, 512, 20478, 11, 558, 30, 708, 366, 264, 50952], 'temperature': 0.0, 'avg_logprob': -0.09906122901222923, 'compression_ratio': 1.817490494296578, 'no_speech_prob': 0.0004511528823059052}, {'id': 518, 'seek': 292668, 'start': 2938.44, 'end': 2944.2, 'text': ' alternatives? Instead of computing the derivative for the gradients across your entire data set,', 'tokens': [50952, 20478, 30, 7156, 295, 15866, 264, 13760, 337, 264, 2771, 2448, 2108, 428, 2302, 1412, 992, 11, 51240], 'temperature': 0.0, 'avg_logprob': -0.09906122901222923, 'compression_ratio': 1.817490494296578, 'no_speech_prob': 0.0004511528823059052}, {'id': 519, 'seek': 292668, 'start': 2944.8399999999997, 'end': 2950.7599999999998, 'text': ' what if you instead computed the gradient over just a single example in your data set? Just one', 'tokens': [51272, 437, 498, 291, 2602, 40610, 264, 16235, 670, 445, 257, 2167, 1365, 294, 428, 1412, 992, 30, 1449, 472, 51568], 'temperature': 0.0, 'avg_logprob': -0.09906122901222923, 'compression_ratio': 1.817490494296578, 'no_speech_prob': 0.0004511528823059052}, {'id': 520, 'seek': 292668, 'start': 2950.7599999999998, 'end': 2956.2799999999997, 'text': \" example. Well, of course, this estimate of your gradient is going to be exactly that. It's an\", 'tokens': [51568, 1365, 13, 1042, 11, 295, 1164, 11, 341, 12539, 295, 428, 16235, 307, 516, 281, 312, 2293, 300, 13, 467, 311, 364, 51844], 'temperature': 0.0, 'avg_logprob': -0.09906122901222923, 'compression_ratio': 1.817490494296578, 'no_speech_prob': 0.0004511528823059052}, {'id': 521, 'seek': 295628, 'start': 2956.28, 'end': 2961.5600000000004, 'text': \" estimate. It's going to be very noisy. It may roughly reflect the trends of your entire data set,\", 'tokens': [50364, 12539, 13, 467, 311, 516, 281, 312, 588, 24518, 13, 467, 815, 9810, 5031, 264, 13892, 295, 428, 2302, 1412, 992, 11, 50628], 'temperature': 0.0, 'avg_logprob': -0.11380751927693684, 'compression_ratio': 1.8517110266159695, 'no_speech_prob': 0.0002736418682616204}, {'id': 522, 'seek': 295628, 'start': 2961.5600000000004, 'end': 2966.6000000000004, 'text': \" but because it's a very, it's only one example. In fact, if your entire data set, it may be very noisy.\", 'tokens': [50628, 457, 570, 309, 311, 257, 588, 11, 309, 311, 787, 472, 1365, 13, 682, 1186, 11, 498, 428, 2302, 1412, 992, 11, 309, 815, 312, 588, 24518, 13, 50880], 'temperature': 0.0, 'avg_logprob': -0.11380751927693684, 'compression_ratio': 1.8517110266159695, 'no_speech_prob': 0.0002736418682616204}, {'id': 523, 'seek': 295628, 'start': 2967.2400000000002, 'end': 2974.52, 'text': \" Right? Well, the advantage of this, though, is that it's much faster to compute, obviously,\", 'tokens': [50912, 1779, 30, 1042, 11, 264, 5002, 295, 341, 11, 1673, 11, 307, 300, 309, 311, 709, 4663, 281, 14722, 11, 2745, 11, 51276], 'temperature': 0.0, 'avg_logprob': -0.11380751927693684, 'compression_ratio': 1.8517110266159695, 'no_speech_prob': 0.0002736418682616204}, {'id': 524, 'seek': 295628, 'start': 2974.52, 'end': 2979.6400000000003, 'text': \" the gradient over a single example, because it's one example. So computationally, this has huge\", 'tokens': [51276, 264, 16235, 670, 257, 2167, 1365, 11, 570, 309, 311, 472, 1365, 13, 407, 24903, 379, 11, 341, 575, 2603, 51532], 'temperature': 0.0, 'avg_logprob': -0.11380751927693684, 'compression_ratio': 1.8517110266159695, 'no_speech_prob': 0.0002736418682616204}, {'id': 525, 'seek': 295628, 'start': 2979.6400000000003, 'end': 2984.44, 'text': \" advantages, but the downside is that it's extremely stochastic, right? That's the reason why this\", 'tokens': [51532, 14906, 11, 457, 264, 25060, 307, 300, 309, 311, 4664, 342, 8997, 2750, 11, 558, 30, 663, 311, 264, 1778, 983, 341, 51772], 'temperature': 0.0, 'avg_logprob': -0.11380751927693684, 'compression_ratio': 1.8517110266159695, 'no_speech_prob': 0.0002736418682616204}, {'id': 526, 'seek': 298444, 'start': 2984.44, 'end': 2988.28, 'text': \" algorithm is not called gradient descent. It's called stochastic gradient descent. Now,\", 'tokens': [50364, 9284, 307, 406, 1219, 16235, 23475, 13, 467, 311, 1219, 342, 8997, 2750, 16235, 23475, 13, 823, 11, 50556], 'temperature': 0.0, 'avg_logprob': -0.09319576723822232, 'compression_ratio': 1.8097014925373134, 'no_speech_prob': 0.0003052910615224391}, {'id': 527, 'seek': 298444, 'start': 2989.48, 'end': 2993.16, 'text': \" now what's the middle ground? Right? Instead of computing it with respect to one example in your\", 'tokens': [50616, 586, 437, 311, 264, 2808, 2727, 30, 1779, 30, 7156, 295, 15866, 309, 365, 3104, 281, 472, 1365, 294, 428, 50800], 'temperature': 0.0, 'avg_logprob': -0.09319576723822232, 'compression_ratio': 1.8097014925373134, 'no_speech_prob': 0.0003052910615224391}, {'id': 528, 'seek': 298444, 'start': 2993.16, 'end': 2999.16, 'text': \" data set, what if we computed what's called a mini batch of examples, a small batch of examples\", 'tokens': [50800, 1412, 992, 11, 437, 498, 321, 40610, 437, 311, 1219, 257, 8382, 15245, 295, 5110, 11, 257, 1359, 15245, 295, 5110, 51100], 'temperature': 0.0, 'avg_logprob': -0.09319576723822232, 'compression_ratio': 1.8097014925373134, 'no_speech_prob': 0.0003052910615224391}, {'id': 529, 'seek': 298444, 'start': 2999.16, 'end': 3005.8, 'text': \" that we can compute the gradients over? And when we take these gradients, they're still computationally\", 'tokens': [51100, 300, 321, 393, 14722, 264, 2771, 2448, 670, 30, 400, 562, 321, 747, 613, 2771, 2448, 11, 436, 434, 920, 24903, 379, 51432], 'temperature': 0.0, 'avg_logprob': -0.09319576723822232, 'compression_ratio': 1.8097014925373134, 'no_speech_prob': 0.0003052910615224391}, {'id': 530, 'seek': 298444, 'start': 3005.8, 'end': 3010.36, 'text': \" efficient to compute because it's a mini batch. It's not too large. Maybe we're talking on the order\", 'tokens': [51432, 7148, 281, 14722, 570, 309, 311, 257, 8382, 15245, 13, 467, 311, 406, 886, 2416, 13, 2704, 321, 434, 1417, 322, 264, 1668, 51660], 'temperature': 0.0, 'avg_logprob': -0.09319576723822232, 'compression_ratio': 1.8097014925373134, 'no_speech_prob': 0.0003052910615224391}, {'id': 531, 'seek': 301036, 'start': 3010.36, 'end': 3018.04, 'text': \" of tens or hundreds of examples in our data set, but more importantly, because we've expanded from\", 'tokens': [50364, 295, 10688, 420, 6779, 295, 5110, 294, 527, 1412, 992, 11, 457, 544, 8906, 11, 570, 321, 600, 14342, 490, 50748], 'temperature': 0.0, 'avg_logprob': -0.11927294731140137, 'compression_ratio': 1.8235294117647058, 'no_speech_prob': 0.001572467153891921}, {'id': 532, 'seek': 301036, 'start': 3018.04, 'end': 3023.2400000000002, 'text': ' a single example to maybe a hundred examples, the stochasticity is significantly reduced, and the', 'tokens': [50748, 257, 2167, 1365, 281, 1310, 257, 3262, 5110, 11, 264, 342, 8997, 2750, 507, 307, 10591, 9212, 11, 293, 264, 51008], 'temperature': 0.0, 'avg_logprob': -0.11927294731140137, 'compression_ratio': 1.8235294117647058, 'no_speech_prob': 0.001572467153891921}, {'id': 533, 'seek': 301036, 'start': 3023.2400000000002, 'end': 3029.56, 'text': \" accuracy of our gradients is much improved. So normally, we're thinking of batch sizes, mini batch\", 'tokens': [51008, 14170, 295, 527, 2771, 2448, 307, 709, 9689, 13, 407, 5646, 11, 321, 434, 1953, 295, 15245, 11602, 11, 8382, 15245, 51324], 'temperature': 0.0, 'avg_logprob': -0.11927294731140137, 'compression_ratio': 1.8235294117647058, 'no_speech_prob': 0.001572467153891921}, {'id': 534, 'seek': 301036, 'start': 3029.56, 'end': 3035.48, 'text': ' sizes roughly on the order of 100 data points, tens or hundreds of data points. This is much faster,', 'tokens': [51324, 11602, 9810, 322, 264, 1668, 295, 2319, 1412, 2793, 11, 10688, 420, 6779, 295, 1412, 2793, 13, 639, 307, 709, 4663, 11, 51620], 'temperature': 0.0, 'avg_logprob': -0.11927294731140137, 'compression_ratio': 1.8235294117647058, 'no_speech_prob': 0.001572467153891921}, {'id': 535, 'seek': 301036, 'start': 3035.48, 'end': 3040.28, 'text': ' obviously, to compute the gradient descent and much more accurate to compute compared to stochastic', 'tokens': [51620, 2745, 11, 281, 14722, 264, 16235, 23475, 293, 709, 544, 8559, 281, 14722, 5347, 281, 342, 8997, 2750, 51860], 'temperature': 0.0, 'avg_logprob': -0.11927294731140137, 'compression_ratio': 1.8235294117647058, 'no_speech_prob': 0.001572467153891921}, {'id': 536, 'seek': 304036, 'start': 3040.36, 'end': 3047.32, 'text': ' gradient descent, which is that single point example. So this increase in gradient accuracy', 'tokens': [50364, 16235, 23475, 11, 597, 307, 300, 2167, 935, 1365, 13, 407, 341, 3488, 294, 16235, 14170, 50712], 'temperature': 0.0, 'avg_logprob': -0.08267357716193566, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.000495404819957912}, {'id': 537, 'seek': 304036, 'start': 3048.28, 'end': 3053.4, 'text': ' allows us to essentially converge to our solution much quicker than it could have been possible', 'tokens': [50760, 4045, 505, 281, 4476, 41881, 281, 527, 3827, 709, 16255, 813, 309, 727, 362, 668, 1944, 51016], 'temperature': 0.0, 'avg_logprob': -0.08267357716193566, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.000495404819957912}, {'id': 538, 'seek': 304036, 'start': 3053.4, 'end': 3058.76, 'text': ' in practice due to gradient descent limitations. It also means that we can increase our learning', 'tokens': [51016, 294, 3124, 3462, 281, 16235, 23475, 15705, 13, 467, 611, 1355, 300, 321, 393, 3488, 527, 2539, 51284], 'temperature': 0.0, 'avg_logprob': -0.08267357716193566, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.000495404819957912}, {'id': 539, 'seek': 304036, 'start': 3058.76, 'end': 3064.1200000000003, 'text': \" rate because we can trust each of those gradients much more efficiently. Right? We're now averaging\", 'tokens': [51284, 3314, 570, 321, 393, 3361, 1184, 295, 729, 2771, 2448, 709, 544, 19621, 13, 1779, 30, 492, 434, 586, 47308, 51552], 'temperature': 0.0, 'avg_logprob': -0.08267357716193566, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.000495404819957912}, {'id': 540, 'seek': 304036, 'start': 3064.1200000000003, 'end': 3068.44, 'text': \" over a batch. It's going to be much more accurate than the stochastic version, so we can increase\", 'tokens': [51552, 670, 257, 15245, 13, 467, 311, 516, 281, 312, 709, 544, 8559, 813, 264, 342, 8997, 2750, 3037, 11, 370, 321, 393, 3488, 51768], 'temperature': 0.0, 'avg_logprob': -0.08267357716193566, 'compression_ratio': 1.740072202166065, 'no_speech_prob': 0.000495404819957912}, {'id': 541, 'seek': 306844, 'start': 3068.44, 'end': 3075.2400000000002, 'text': ' that learning rate and actually learn faster as well. This allows us to also massively parallelize', 'tokens': [50364, 300, 2539, 3314, 293, 767, 1466, 4663, 382, 731, 13, 639, 4045, 505, 281, 611, 29379, 8952, 1125, 50704], 'temperature': 0.0, 'avg_logprob': -0.10860463705929843, 'compression_ratio': 1.5708502024291497, 'no_speech_prob': 0.002082190476357937}, {'id': 542, 'seek': 306844, 'start': 3075.2400000000002, 'end': 3081.08, 'text': ' this entire algorithm and computation. Right? We can split up batches onto separate workers and', 'tokens': [50704, 341, 2302, 9284, 293, 24903, 13, 1779, 30, 492, 393, 7472, 493, 15245, 279, 3911, 4994, 5600, 293, 50996], 'temperature': 0.0, 'avg_logprob': -0.10860463705929843, 'compression_ratio': 1.5708502024291497, 'no_speech_prob': 0.002082190476357937}, {'id': 543, 'seek': 306844, 'start': 3081.08, 'end': 3088.12, 'text': ' achieve even more significant speed ups of this entire problem using GPUs. The last topic that I', 'tokens': [50996, 4584, 754, 544, 4776, 3073, 15497, 295, 341, 2302, 1154, 1228, 18407, 82, 13, 440, 1036, 4829, 300, 286, 51348], 'temperature': 0.0, 'avg_logprob': -0.10860463705929843, 'compression_ratio': 1.5708502024291497, 'no_speech_prob': 0.002082190476357937}, {'id': 544, 'seek': 306844, 'start': 3088.12, 'end': 3094.28, 'text': \" very, very briefly want to cover in today's lecture is this topic of overfitting. Right? When we\", 'tokens': [51348, 588, 11, 588, 10515, 528, 281, 2060, 294, 965, 311, 7991, 307, 341, 4829, 295, 670, 69, 2414, 13, 1779, 30, 1133, 321, 51656], 'temperature': 0.0, 'avg_logprob': -0.10860463705929843, 'compression_ratio': 1.5708502024291497, 'no_speech_prob': 0.002082190476357937}, {'id': 545, 'seek': 309428, 'start': 3094.44, 'end': 3101.0, 'text': \" are optimizing a neural network with stochastic gradient descent, we have this challenge of what's\", 'tokens': [50372, 366, 40425, 257, 18161, 3209, 365, 342, 8997, 2750, 16235, 23475, 11, 321, 362, 341, 3430, 295, 437, 311, 50700], 'temperature': 0.0, 'avg_logprob': -0.09856756802262931, 'compression_ratio': 1.7752808988764044, 'no_speech_prob': 0.00035684375325217843}, {'id': 546, 'seek': 309428, 'start': 3101.0, 'end': 3106.36, 'text': ' called overfitting. Overfitting, I, looks like this roughly, right? So on the left hand side,', 'tokens': [50700, 1219, 670, 69, 2414, 13, 4886, 69, 2414, 11, 286, 11, 1542, 411, 341, 9810, 11, 558, 30, 407, 322, 264, 1411, 1011, 1252, 11, 50968], 'temperature': 0.0, 'avg_logprob': -0.09856756802262931, 'compression_ratio': 1.7752808988764044, 'no_speech_prob': 0.00035684375325217843}, {'id': 547, 'seek': 309428, 'start': 3107.48, 'end': 3111.7200000000003, 'text': \" we want to build a neural network, or let's say in general, we want to build a machine learning\", 'tokens': [51024, 321, 528, 281, 1322, 257, 18161, 3209, 11, 420, 718, 311, 584, 294, 2674, 11, 321, 528, 281, 1322, 257, 3479, 2539, 51236], 'temperature': 0.0, 'avg_logprob': -0.09856756802262931, 'compression_ratio': 1.7752808988764044, 'no_speech_prob': 0.00035684375325217843}, {'id': 548, 'seek': 309428, 'start': 3111.7200000000003, 'end': 3118.1200000000003, 'text': \" model that can accurately describe some patterns in our data, but remember, we're ultimately,\", 'tokens': [51236, 2316, 300, 393, 20095, 6786, 512, 8294, 294, 527, 1412, 11, 457, 1604, 11, 321, 434, 6284, 11, 51556], 'temperature': 0.0, 'avg_logprob': -0.09856756802262931, 'compression_ratio': 1.7752808988764044, 'no_speech_prob': 0.00035684375325217843}, {'id': 549, 'seek': 309428, 'start': 3118.1200000000003, 'end': 3122.36, 'text': \" we don't want to describe the patterns in our training data. Ideally, we want to define the\", 'tokens': [51556, 321, 500, 380, 528, 281, 6786, 264, 8294, 294, 527, 3097, 1412, 13, 40817, 11, 321, 528, 281, 6964, 264, 51768], 'temperature': 0.0, 'avg_logprob': -0.09856756802262931, 'compression_ratio': 1.7752808988764044, 'no_speech_prob': 0.00035684375325217843}, {'id': 550, 'seek': 312236, 'start': 3122.36, 'end': 3127.1600000000003, 'text': \" patterns in our test data. Of course, we don't observe test data. We only observe training data.\", 'tokens': [50364, 8294, 294, 527, 1500, 1412, 13, 2720, 1164, 11, 321, 500, 380, 11441, 1500, 1412, 13, 492, 787, 11441, 3097, 1412, 13, 50604], 'temperature': 0.0, 'avg_logprob': -0.05338571689746998, 'compression_ratio': 1.9268292682926829, 'no_speech_prob': 0.0017535461811348796}, {'id': 551, 'seek': 312236, 'start': 3127.1600000000003, 'end': 3131.56, 'text': ' So we have this challenge of extracting patterns from training data and hoping that they', 'tokens': [50604, 407, 321, 362, 341, 3430, 295, 49844, 8294, 490, 3097, 1412, 293, 7159, 300, 436, 50824], 'temperature': 0.0, 'avg_logprob': -0.05338571689746998, 'compression_ratio': 1.9268292682926829, 'no_speech_prob': 0.0017535461811348796}, {'id': 552, 'seek': 312236, 'start': 3131.56, 'end': 3137.2400000000002, 'text': ' generalize to our test data. So set in one different way, we want to build models that can learn', 'tokens': [50824, 2674, 1125, 281, 527, 1500, 1412, 13, 407, 992, 294, 472, 819, 636, 11, 321, 528, 281, 1322, 5245, 300, 393, 1466, 51108], 'temperature': 0.0, 'avg_logprob': -0.05338571689746998, 'compression_ratio': 1.9268292682926829, 'no_speech_prob': 0.0017535461811348796}, {'id': 553, 'seek': 312236, 'start': 3137.2400000000002, 'end': 3142.92, 'text': ' representations from our training data that can still generalize even when we show them brand new', 'tokens': [51108, 33358, 490, 527, 3097, 1412, 300, 393, 920, 2674, 1125, 754, 562, 321, 855, 552, 3360, 777, 51392], 'temperature': 0.0, 'avg_logprob': -0.05338571689746998, 'compression_ratio': 1.9268292682926829, 'no_speech_prob': 0.0017535461811348796}, {'id': 554, 'seek': 312236, 'start': 3142.92, 'end': 3149.96, 'text': ' unseen pieces of test data. So assume that you want to build a line that can describe or find', 'tokens': [51392, 40608, 3755, 295, 1500, 1412, 13, 407, 6552, 300, 291, 528, 281, 1322, 257, 1622, 300, 393, 6786, 420, 915, 51744], 'temperature': 0.0, 'avg_logprob': -0.05338571689746998, 'compression_ratio': 1.9268292682926829, 'no_speech_prob': 0.0017535461811348796}, {'id': 555, 'seek': 314996, 'start': 3149.96, 'end': 3154.84, 'text': ' the patterns in these points that you can see on the slide. If you have a very simple neural', 'tokens': [50364, 264, 8294, 294, 613, 2793, 300, 291, 393, 536, 322, 264, 4137, 13, 759, 291, 362, 257, 588, 2199, 18161, 50608], 'temperature': 0.0, 'avg_logprob': -0.07768188317616781, 'compression_ratio': 1.73992673992674, 'no_speech_prob': 0.0014545226003974676}, {'id': 556, 'seek': 314996, 'start': 3154.84, 'end': 3163.0, 'text': ' network, which is just a single line, straight line, you can describe this data sub optimally,', 'tokens': [50608, 3209, 11, 597, 307, 445, 257, 2167, 1622, 11, 2997, 1622, 11, 291, 393, 6786, 341, 1412, 1422, 5028, 379, 11, 51016], 'temperature': 0.0, 'avg_logprob': -0.07768188317616781, 'compression_ratio': 1.73992673992674, 'no_speech_prob': 0.0014545226003974676}, {'id': 557, 'seek': 314996, 'start': 3163.0, 'end': 3167.48, 'text': \" because the data here is nonlinear. You're not going to accurately capture all of the nuances\", 'tokens': [51016, 570, 264, 1412, 510, 307, 2107, 28263, 13, 509, 434, 406, 516, 281, 20095, 7983, 439, 295, 264, 38775, 51240], 'temperature': 0.0, 'avg_logprob': -0.07768188317616781, 'compression_ratio': 1.73992673992674, 'no_speech_prob': 0.0014545226003974676}, {'id': 558, 'seek': 314996, 'start': 3167.48, 'end': 3172.68, 'text': \" and subtleties in this data set. That's on the left hand side. If you move to the right hand side,\", 'tokens': [51240, 293, 7257, 2631, 530, 294, 341, 1412, 992, 13, 663, 311, 322, 264, 1411, 1011, 1252, 13, 759, 291, 1286, 281, 264, 558, 1011, 1252, 11, 51500], 'temperature': 0.0, 'avg_logprob': -0.07768188317616781, 'compression_ratio': 1.73992673992674, 'no_speech_prob': 0.0014545226003974676}, {'id': 559, 'seek': 314996, 'start': 3172.68, 'end': 3177.4, 'text': \" you can see a much more complicated model, but here you're actually overexpressive. You're too\", 'tokens': [51500, 291, 393, 536, 257, 709, 544, 6179, 2316, 11, 457, 510, 291, 434, 767, 38657, 87, 11637, 488, 13, 509, 434, 886, 51736], 'temperature': 0.0, 'avg_logprob': -0.07768188317616781, 'compression_ratio': 1.73992673992674, 'no_speech_prob': 0.0014545226003974676}, {'id': 560, 'seek': 317740, 'start': 3177.4, 'end': 3183.56, 'text': \" expressive and you're capturing kind of the nuances, the spurious nuances in your training data\", 'tokens': [50364, 40189, 293, 291, 434, 23384, 733, 295, 264, 38775, 11, 264, 637, 24274, 38775, 294, 428, 3097, 1412, 50672], 'temperature': 0.0, 'avg_logprob': -0.09017700061463473, 'compression_ratio': 1.7526881720430108, 'no_speech_prob': 0.00043718042434193194}, {'id': 561, 'seek': 317740, 'start': 3183.56, 'end': 3189.1600000000003, 'text': ' that are actually not representative of your test data. Ideally, you want to end up with the', 'tokens': [50672, 300, 366, 767, 406, 12424, 295, 428, 1500, 1412, 13, 40817, 11, 291, 528, 281, 917, 493, 365, 264, 50952], 'temperature': 0.0, 'avg_logprob': -0.09017700061463473, 'compression_ratio': 1.7526881720430108, 'no_speech_prob': 0.00043718042434193194}, {'id': 562, 'seek': 317740, 'start': 3189.1600000000003, 'end': 3193.96, 'text': \" model in the middle, which is basically the middle ground, right? It's not too complex and it's not\", 'tokens': [50952, 2316, 294, 264, 2808, 11, 597, 307, 1936, 264, 2808, 2727, 11, 558, 30, 467, 311, 406, 886, 3997, 293, 309, 311, 406, 51192], 'temperature': 0.0, 'avg_logprob': -0.09017700061463473, 'compression_ratio': 1.7526881720430108, 'no_speech_prob': 0.00043718042434193194}, {'id': 563, 'seek': 317740, 'start': 3193.96, 'end': 3199.32, 'text': ' too simple. It still gives you what you want to perform well and even when you give it brand new', 'tokens': [51192, 886, 2199, 13, 467, 920, 2709, 291, 437, 291, 528, 281, 2042, 731, 293, 754, 562, 291, 976, 309, 3360, 777, 51460], 'temperature': 0.0, 'avg_logprob': -0.09017700061463473, 'compression_ratio': 1.7526881720430108, 'no_speech_prob': 0.00043718042434193194}, {'id': 564, 'seek': 317740, 'start': 3199.32, 'end': 3205.88, 'text': \" data. So to address this problem, let's briefly talk about what's called regularization. Regularization\", 'tokens': [51460, 1412, 13, 407, 281, 2985, 341, 1154, 11, 718, 311, 10515, 751, 466, 437, 311, 1219, 3890, 2144, 13, 45659, 2144, 51788], 'temperature': 0.0, 'avg_logprob': -0.09017700061463473, 'compression_ratio': 1.7526881720430108, 'no_speech_prob': 0.00043718042434193194}, {'id': 565, 'seek': 320588, 'start': 3205.88, 'end': 3211.6400000000003, 'text': ' is a technique that you can introduce to your training pipeline to discourage complex models', 'tokens': [50364, 307, 257, 6532, 300, 291, 393, 5366, 281, 428, 3097, 15517, 281, 21497, 609, 3997, 5245, 50652], 'temperature': 0.0, 'avg_logprob': -0.06995791805033781, 'compression_ratio': 1.7345454545454546, 'no_speech_prob': 0.000816263840533793}, {'id': 566, 'seek': 320588, 'start': 3211.6400000000003, 'end': 3217.7200000000003, 'text': \" from being learned. Now, as we've seen before, this is really critical because neural networks are\", 'tokens': [50652, 490, 885, 3264, 13, 823, 11, 382, 321, 600, 1612, 949, 11, 341, 307, 534, 4924, 570, 18161, 9590, 366, 50956], 'temperature': 0.0, 'avg_logprob': -0.06995791805033781, 'compression_ratio': 1.7345454545454546, 'no_speech_prob': 0.000816263840533793}, {'id': 567, 'seek': 320588, 'start': 3217.7200000000003, 'end': 3223.56, 'text': ' extremely large models. They are extremely prone to overfitting, right? So regularization, having', 'tokens': [50956, 4664, 2416, 5245, 13, 814, 366, 4664, 25806, 281, 670, 69, 2414, 11, 558, 30, 407, 3890, 2144, 11, 1419, 51248], 'temperature': 0.0, 'avg_logprob': -0.06995791805033781, 'compression_ratio': 1.7345454545454546, 'no_speech_prob': 0.000816263840533793}, {'id': 568, 'seek': 320588, 'start': 3223.56, 'end': 3229.08, 'text': ' techniques for regularization has extreme implications towards the success of neural networks and', 'tokens': [51248, 7512, 337, 3890, 2144, 575, 8084, 16602, 3030, 264, 2245, 295, 18161, 9590, 293, 51524], 'temperature': 0.0, 'avg_logprob': -0.06995791805033781, 'compression_ratio': 1.7345454545454546, 'no_speech_prob': 0.000816263840533793}, {'id': 569, 'seek': 320588, 'start': 3229.08, 'end': 3234.84, 'text': ' having them generalize beyond training data far into our testing domain. The most popular', 'tokens': [51524, 1419, 552, 2674, 1125, 4399, 3097, 1412, 1400, 666, 527, 4997, 9274, 13, 440, 881, 3743, 51812], 'temperature': 0.0, 'avg_logprob': -0.06995791805033781, 'compression_ratio': 1.7345454545454546, 'no_speech_prob': 0.000816263840533793}, {'id': 570, 'seek': 323484, 'start': 3234.84, 'end': 3240.36, 'text': ' technique for regularization in deep learning is called dropout and the idea of dropout is actually', 'tokens': [50364, 6532, 337, 3890, 2144, 294, 2452, 2539, 307, 1219, 3270, 346, 293, 264, 1558, 295, 3270, 346, 307, 767, 50640], 'temperature': 0.0, 'avg_logprob': -0.08156699267300692, 'compression_ratio': 1.8426966292134832, 'no_speech_prob': 0.002016311278566718}, {'id': 571, 'seek': 323484, 'start': 3240.36, 'end': 3245.08, 'text': \" very simple. It's let's revisit it by drawing this picture of deep neural networks that we saw\", 'tokens': [50640, 588, 2199, 13, 467, 311, 718, 311, 32676, 309, 538, 6316, 341, 3036, 295, 2452, 18161, 9590, 300, 321, 1866, 50876], 'temperature': 0.0, 'avg_logprob': -0.08156699267300692, 'compression_ratio': 1.8426966292134832, 'no_speech_prob': 0.002016311278566718}, {'id': 572, 'seek': 323484, 'start': 3245.08, 'end': 3251.4, 'text': \" earlier in today's lecture. In dropout, during training, we essentially randomly select some subset\", 'tokens': [50876, 3071, 294, 965, 311, 7991, 13, 682, 3270, 346, 11, 1830, 3097, 11, 321, 4476, 16979, 3048, 512, 25993, 51192], 'temperature': 0.0, 'avg_logprob': -0.08156699267300692, 'compression_ratio': 1.8426966292134832, 'no_speech_prob': 0.002016311278566718}, {'id': 573, 'seek': 323484, 'start': 3251.4, 'end': 3258.04, 'text': ' of the neurons in this neural network and we try to prune them out with some random probability.', 'tokens': [51192, 295, 264, 22027, 294, 341, 18161, 3209, 293, 321, 853, 281, 582, 2613, 552, 484, 365, 512, 4974, 8482, 13, 51524], 'temperature': 0.0, 'avg_logprob': -0.08156699267300692, 'compression_ratio': 1.8426966292134832, 'no_speech_prob': 0.002016311278566718}, {'id': 574, 'seek': 323484, 'start': 3258.04, 'end': 3263.4, 'text': ' So for example, we can select this subset of neurons. We can randomly select them with a probability', 'tokens': [51524, 407, 337, 1365, 11, 321, 393, 3048, 341, 25993, 295, 22027, 13, 492, 393, 16979, 3048, 552, 365, 257, 8482, 51792], 'temperature': 0.0, 'avg_logprob': -0.08156699267300692, 'compression_ratio': 1.8426966292134832, 'no_speech_prob': 0.002016311278566718}, {'id': 575, 'seek': 326340, 'start': 3263.4, 'end': 3270.6800000000003, 'text': ' of 50 percent and with that probability, we randomly turn them off or on different iterations of', 'tokens': [50364, 295, 2625, 3043, 293, 365, 300, 8482, 11, 321, 16979, 1261, 552, 766, 420, 322, 819, 36540, 295, 50728], 'temperature': 0.0, 'avg_logprob': -0.09367484312791091, 'compression_ratio': 1.7669172932330828, 'no_speech_prob': 0.00044407587847672403}, {'id': 576, 'seek': 326340, 'start': 3270.6800000000003, 'end': 3277.2400000000002, 'text': ' our training. So this is essentially forcing the neural network to learn you can think of an', 'tokens': [50728, 527, 3097, 13, 407, 341, 307, 4476, 19030, 264, 18161, 3209, 281, 1466, 291, 393, 519, 295, 364, 51056], 'temperature': 0.0, 'avg_logprob': -0.09367484312791091, 'compression_ratio': 1.7669172932330828, 'no_speech_prob': 0.00044407587847672403}, {'id': 577, 'seek': 326340, 'start': 3277.2400000000002, 'end': 3283.32, 'text': \" ensemble of different models. On every iteration, it's going to be exposed to kind of a different\", 'tokens': [51056, 19492, 295, 819, 5245, 13, 1282, 633, 24784, 11, 309, 311, 516, 281, 312, 9495, 281, 733, 295, 257, 819, 51360], 'temperature': 0.0, 'avg_logprob': -0.09367484312791091, 'compression_ratio': 1.7669172932330828, 'no_speech_prob': 0.00044407587847672403}, {'id': 578, 'seek': 326340, 'start': 3283.32, 'end': 3287.88, 'text': ' model internally than the one it had on the last iteration. So it has to learn how to build', 'tokens': [51360, 2316, 19501, 813, 264, 472, 309, 632, 322, 264, 1036, 24784, 13, 407, 309, 575, 281, 1466, 577, 281, 1322, 51588], 'temperature': 0.0, 'avg_logprob': -0.09367484312791091, 'compression_ratio': 1.7669172932330828, 'no_speech_prob': 0.00044407587847672403}, {'id': 579, 'seek': 326340, 'start': 3287.88, 'end': 3293.2400000000002, 'text': \" internal pathways to process the same information and it can't rely on information that it\", 'tokens': [51588, 6920, 22988, 281, 1399, 264, 912, 1589, 293, 309, 393, 380, 10687, 322, 1589, 300, 309, 51856], 'temperature': 0.0, 'avg_logprob': -0.09367484312791091, 'compression_ratio': 1.7669172932330828, 'no_speech_prob': 0.00044407587847672403}, {'id': 580, 'seek': 329324, 'start': 3293.24, 'end': 3298.8399999999997, 'text': ' learned on previous iterations. So it forces it to kind of capture some deeper meaning within the', 'tokens': [50364, 3264, 322, 3894, 36540, 13, 407, 309, 5874, 309, 281, 733, 295, 7983, 512, 7731, 3620, 1951, 264, 50644], 'temperature': 0.0, 'avg_logprob': -0.11302201407296317, 'compression_ratio': 1.7338129496402879, 'no_speech_prob': 7.030349661363289e-05}, {'id': 581, 'seek': 329324, 'start': 3298.8399999999997, 'end': 3302.8399999999997, 'text': ' pathways of the neural network and this can be extremely powerful because the number one, it', 'tokens': [50644, 22988, 295, 264, 18161, 3209, 293, 341, 393, 312, 4664, 4005, 570, 264, 1230, 472, 11, 309, 50844], 'temperature': 0.0, 'avg_logprob': -0.11302201407296317, 'compression_ratio': 1.7338129496402879, 'no_speech_prob': 7.030349661363289e-05}, {'id': 582, 'seek': 329324, 'start': 3302.8399999999997, 'end': 3308.4399999999996, 'text': \" lowers the capacity of the neural network significantly. You're lowering it by roughly 50 percent\", 'tokens': [50844, 44936, 264, 6042, 295, 264, 18161, 3209, 10591, 13, 509, 434, 28124, 309, 538, 9810, 2625, 3043, 51124], 'temperature': 0.0, 'avg_logprob': -0.11302201407296317, 'compression_ratio': 1.7338129496402879, 'no_speech_prob': 7.030349661363289e-05}, {'id': 583, 'seek': 329324, 'start': 3308.4399999999996, 'end': 3314.3599999999997, 'text': ' in this example. But also because it makes it easier to train because the number of weights that', 'tokens': [51124, 294, 341, 1365, 13, 583, 611, 570, 309, 1669, 309, 3571, 281, 3847, 570, 264, 1230, 295, 17443, 300, 51420], 'temperature': 0.0, 'avg_logprob': -0.11302201407296317, 'compression_ratio': 1.7338129496402879, 'no_speech_prob': 7.030349661363289e-05}, {'id': 584, 'seek': 329324, 'start': 3314.3599999999997, 'end': 3318.9199999999996, 'text': \" have gradients in this case is also reduced. So it's actually much faster to train them as well.\", 'tokens': [51420, 362, 2771, 2448, 294, 341, 1389, 307, 611, 9212, 13, 407, 309, 311, 767, 709, 4663, 281, 3847, 552, 382, 731, 13, 51648], 'temperature': 0.0, 'avg_logprob': -0.11302201407296317, 'compression_ratio': 1.7338129496402879, 'no_speech_prob': 7.030349661363289e-05}, {'id': 585, 'seek': 331892, 'start': 3319.2400000000002, 'end': 3326.04, 'text': ' Now, like I mentioned, on every iteration, we randomly drop out a different set of neurons,', 'tokens': [50380, 823, 11, 411, 286, 2835, 11, 322, 633, 24784, 11, 321, 16979, 3270, 484, 257, 819, 992, 295, 22027, 11, 50720], 'temperature': 0.0, 'avg_logprob': -0.14001792814673447, 'compression_ratio': 1.6088888888888888, 'no_speech_prob': 0.0007669660844840109}, {'id': 586, 'seek': 331892, 'start': 3326.04, 'end': 3331.32, 'text': ' right? And that helps the data generalize better. And the second regularization techniques,', 'tokens': [50720, 558, 30, 400, 300, 3665, 264, 1412, 2674, 1125, 1101, 13, 400, 264, 1150, 3890, 2144, 7512, 11, 50984], 'temperature': 0.0, 'avg_logprob': -0.14001792814673447, 'compression_ratio': 1.6088888888888888, 'no_speech_prob': 0.0007669660844840109}, {'id': 587, 'seek': 331892, 'start': 3331.32, 'end': 3335.7200000000003, 'text': ' which is actually a very broad regularization technique far beyond neural networks,', 'tokens': [50984, 597, 307, 767, 257, 588, 4152, 3890, 2144, 6532, 1400, 4399, 18161, 9590, 11, 51204], 'temperature': 0.0, 'avg_logprob': -0.14001792814673447, 'compression_ratio': 1.6088888888888888, 'no_speech_prob': 0.0007669660844840109}, {'id': 588, 'seek': 331892, 'start': 3335.7200000000003, 'end': 3343.56, 'text': ' is simply called early stopping. Now, we know the definition of overfitting is simply when our', 'tokens': [51204, 307, 2935, 1219, 2440, 12767, 13, 823, 11, 321, 458, 264, 7123, 295, 670, 69, 2414, 307, 2935, 562, 527, 51596], 'temperature': 0.0, 'avg_logprob': -0.14001792814673447, 'compression_ratio': 1.6088888888888888, 'no_speech_prob': 0.0007669660844840109}, {'id': 589, 'seek': 334356, 'start': 3343.56, 'end': 3349.0, 'text': \" model starts to represent basically the training data more than the testing data. That's really\", 'tokens': [50364, 2316, 3719, 281, 2906, 1936, 264, 3097, 1412, 544, 813, 264, 4997, 1412, 13, 663, 311, 534, 50636], 'temperature': 0.0, 'avg_logprob': -0.11061049751613451, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0006068918737582862}, {'id': 590, 'seek': 334356, 'start': 3349.0, 'end': 3354.92, 'text': ' what overfitting comes down to its core. If we set aside some of the training data to use separately,', 'tokens': [50636, 437, 670, 69, 2414, 1487, 760, 281, 1080, 4965, 13, 759, 321, 992, 7359, 512, 295, 264, 3097, 1412, 281, 764, 14759, 11, 50932], 'temperature': 0.0, 'avg_logprob': -0.11061049751613451, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0006068918737582862}, {'id': 591, 'seek': 334356, 'start': 3354.92, 'end': 3361.64, 'text': \" that we don't train on it, we can use a kind of a testing data set, synthetic testing data set in\", 'tokens': [50932, 300, 321, 500, 380, 3847, 322, 309, 11, 321, 393, 764, 257, 733, 295, 257, 4997, 1412, 992, 11, 23420, 4997, 1412, 992, 294, 51268], 'temperature': 0.0, 'avg_logprob': -0.11061049751613451, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0006068918737582862}, {'id': 592, 'seek': 334356, 'start': 3361.64, 'end': 3367.7999999999997, 'text': ' some ways. We can monitor how our network is learning on this unseen portion of data. So for', 'tokens': [51268, 512, 2098, 13, 492, 393, 6002, 577, 527, 3209, 307, 2539, 322, 341, 40608, 8044, 295, 1412, 13, 407, 337, 51576], 'temperature': 0.0, 'avg_logprob': -0.11061049751613451, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0006068918737582862}, {'id': 593, 'seek': 334356, 'start': 3367.7999999999997, 'end': 3373.0, 'text': ' example, we can over the course of training, we can basically plot the performance of our network', 'tokens': [51576, 1365, 11, 321, 393, 670, 264, 1164, 295, 3097, 11, 321, 393, 1936, 7542, 264, 3389, 295, 527, 3209, 51836], 'temperature': 0.0, 'avg_logprob': -0.11061049751613451, 'compression_ratio': 1.8837209302325582, 'no_speech_prob': 0.0006068918737582862}, {'id': 594, 'seek': 337300, 'start': 3373.0, 'end': 3378.04, 'text': ' on both the training set as well as our held out test set. And as the network is trained,', 'tokens': [50364, 322, 1293, 264, 3097, 992, 382, 731, 382, 527, 5167, 484, 1500, 992, 13, 400, 382, 264, 3209, 307, 8895, 11, 50616], 'temperature': 0.0, 'avg_logprob': -0.10628882769880624, 'compression_ratio': 1.9747899159663866, 'no_speech_prob': 0.0009247473208233714}, {'id': 595, 'seek': 337300, 'start': 3378.04, 'end': 3381.72, 'text': \" we're going to see that, first of all, these both decrease, but there's going to be a point\", 'tokens': [50616, 321, 434, 516, 281, 536, 300, 11, 700, 295, 439, 11, 613, 1293, 11514, 11, 457, 456, 311, 516, 281, 312, 257, 935, 50800], 'temperature': 0.0, 'avg_logprob': -0.10628882769880624, 'compression_ratio': 1.9747899159663866, 'no_speech_prob': 0.0009247473208233714}, {'id': 596, 'seek': 337300, 'start': 3382.36, 'end': 3388.04, 'text': ' where the loss plateaus and starts to increase. The training loss will actually start to increase.', 'tokens': [50832, 689, 264, 4470, 5924, 8463, 293, 3719, 281, 3488, 13, 440, 3097, 4470, 486, 767, 722, 281, 3488, 13, 51116], 'temperature': 0.0, 'avg_logprob': -0.10628882769880624, 'compression_ratio': 1.9747899159663866, 'no_speech_prob': 0.0009247473208233714}, {'id': 597, 'seek': 337300, 'start': 3388.04, 'end': 3392.44, 'text': \" This is exactly the point where you start to overfit, right? Because now you're starting to have,\", 'tokens': [51116, 639, 307, 2293, 264, 935, 689, 291, 722, 281, 670, 6845, 11, 558, 30, 1436, 586, 291, 434, 2891, 281, 362, 11, 51336], 'temperature': 0.0, 'avg_logprob': -0.10628882769880624, 'compression_ratio': 1.9747899159663866, 'no_speech_prob': 0.0009247473208233714}, {'id': 598, 'seek': 337300, 'start': 3393.32, 'end': 3396.76, 'text': \" sorry, that was the test loss. The test loss actually starts to increase because now you're\", 'tokens': [51380, 2597, 11, 300, 390, 264, 1500, 4470, 13, 440, 1500, 4470, 767, 3719, 281, 3488, 570, 586, 291, 434, 51552], 'temperature': 0.0, 'avg_logprob': -0.10628882769880624, 'compression_ratio': 1.9747899159663866, 'no_speech_prob': 0.0009247473208233714}, {'id': 599, 'seek': 339676, 'start': 3396.76, 'end': 3402.1200000000003, 'text': ' starting to overfit on your training data. This pattern basically continues for the rest of training.', 'tokens': [50364, 2891, 281, 670, 6845, 322, 428, 3097, 1412, 13, 639, 5102, 1936, 6515, 337, 264, 1472, 295, 3097, 13, 50632], 'temperature': 0.0, 'avg_logprob': -0.08945640357764992, 'compression_ratio': 1.7902621722846441, 'no_speech_prob': 0.0006459809956140816}, {'id': 600, 'seek': 339676, 'start': 3402.6800000000003, 'end': 3407.32, 'text': ' And this is the point that I want you to focus on, right? This middle point is where we need to', 'tokens': [50660, 400, 341, 307, 264, 935, 300, 286, 528, 291, 281, 1879, 322, 11, 558, 30, 639, 2808, 935, 307, 689, 321, 643, 281, 50892], 'temperature': 0.0, 'avg_logprob': -0.08945640357764992, 'compression_ratio': 1.7902621722846441, 'no_speech_prob': 0.0006459809956140816}, {'id': 601, 'seek': 339676, 'start': 3407.32, 'end': 3412.6000000000004, 'text': ' stop training because after this point, assuming that this test set is a valid representation', 'tokens': [50892, 1590, 3097, 570, 934, 341, 935, 11, 11926, 300, 341, 1500, 992, 307, 257, 7363, 10290, 51156], 'temperature': 0.0, 'avg_logprob': -0.08945640357764992, 'compression_ratio': 1.7902621722846441, 'no_speech_prob': 0.0006459809956140816}, {'id': 602, 'seek': 339676, 'start': 3413.2400000000002, 'end': 3417.88, 'text': ' of the true test set, this is the place where the accuracy of the model will only get worse,', 'tokens': [51188, 295, 264, 2074, 1500, 992, 11, 341, 307, 264, 1081, 689, 264, 14170, 295, 264, 2316, 486, 787, 483, 5324, 11, 51420], 'temperature': 0.0, 'avg_logprob': -0.08945640357764992, 'compression_ratio': 1.7902621722846441, 'no_speech_prob': 0.0006459809956140816}, {'id': 603, 'seek': 339676, 'start': 3417.88, 'end': 3421.88, 'text': ' right? So this is where we would want to early stop our model and regularize the performance.', 'tokens': [51420, 558, 30, 407, 341, 307, 689, 321, 576, 528, 281, 2440, 1590, 527, 2316, 293, 3890, 1125, 264, 3389, 13, 51620], 'temperature': 0.0, 'avg_logprob': -0.08945640357764992, 'compression_ratio': 1.7902621722846441, 'no_speech_prob': 0.0006459809956140816}, {'id': 604, 'seek': 342188, 'start': 3422.84, 'end': 3427.96, 'text': \" And we can see that stopping any time before this point is also not good. We're going to produce\", 'tokens': [50412, 400, 321, 393, 536, 300, 12767, 604, 565, 949, 341, 935, 307, 611, 406, 665, 13, 492, 434, 516, 281, 5258, 50668], 'temperature': 0.0, 'avg_logprob': -0.09688349529705216, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.0049778469838202}, {'id': 605, 'seek': 342188, 'start': 3427.96, 'end': 3432.6800000000003, 'text': \" an underfit model where we could have had a better model on the test data, but it's this tradeoff,\", 'tokens': [50668, 364, 833, 6845, 2316, 689, 321, 727, 362, 632, 257, 1101, 2316, 322, 264, 1500, 1412, 11, 457, 309, 311, 341, 4923, 4506, 11, 50904], 'temperature': 0.0, 'avg_logprob': -0.09688349529705216, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.0049778469838202}, {'id': 606, 'seek': 342188, 'start': 3432.6800000000003, 'end': 3435.32, 'text': \" right? You can't stop too late and you can't stop too early as well.\", 'tokens': [50904, 558, 30, 509, 393, 380, 1590, 886, 3469, 293, 291, 393, 380, 1590, 886, 2440, 382, 731, 13, 51036], 'temperature': 0.0, 'avg_logprob': -0.09688349529705216, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.0049778469838202}, {'id': 607, 'seek': 342188, 'start': 3437.32, 'end': 3442.36, 'text': \" So I'll conclude this lecture by just summarizing these three key points that we've covered in\", 'tokens': [51136, 407, 286, 603, 16886, 341, 7991, 538, 445, 14611, 3319, 613, 1045, 2141, 2793, 300, 321, 600, 5343, 294, 51388], 'temperature': 0.0, 'avg_logprob': -0.09688349529705216, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.0049778469838202}, {'id': 608, 'seek': 342188, 'start': 3442.36, 'end': 3447.96, 'text': \" today's lecture so far. So we first covered these fundamental building blocks of all neural networks,\", 'tokens': [51388, 965, 311, 7991, 370, 1400, 13, 407, 321, 700, 5343, 613, 8088, 2390, 8474, 295, 439, 18161, 9590, 11, 51668], 'temperature': 0.0, 'avg_logprob': -0.09688349529705216, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.0049778469838202}, {'id': 609, 'seek': 344796, 'start': 3447.96, 'end': 3454.2, 'text': \" which is the single neuron, the perceptron. We've built these up into larger neural layers and then\", 'tokens': [50364, 597, 307, 264, 2167, 34090, 11, 264, 43276, 2044, 13, 492, 600, 3094, 613, 493, 666, 4833, 18161, 7914, 293, 550, 50676], 'temperature': 0.0, 'avg_logprob': -0.1319122102525499, 'compression_ratio': 1.6550218340611353, 'no_speech_prob': 0.002357725752517581}, {'id': 610, 'seek': 344796, 'start': 3454.2, 'end': 3458.68, 'text': \" from their neural networks and deep neural networks. We've learned how we can train these,\", 'tokens': [50676, 490, 641, 18161, 9590, 293, 2452, 18161, 9590, 13, 492, 600, 3264, 577, 321, 393, 3847, 613, 11, 50900], 'temperature': 0.0, 'avg_logprob': -0.1319122102525499, 'compression_ratio': 1.6550218340611353, 'no_speech_prob': 0.002357725752517581}, {'id': 611, 'seek': 344796, 'start': 3458.68, 'end': 3465.08, 'text': \" apply them to data sets, back propagate through them, and we've seen some tips and tricks for\", 'tokens': [50900, 3079, 552, 281, 1412, 6352, 11, 646, 48256, 807, 552, 11, 293, 321, 600, 1612, 512, 6082, 293, 11733, 337, 51220], 'temperature': 0.0, 'avg_logprob': -0.1319122102525499, 'compression_ratio': 1.6550218340611353, 'no_speech_prob': 0.002357725752517581}, {'id': 612, 'seek': 344796, 'start': 3465.08, 'end': 3471.08, 'text': \" optimizing these systems end to end. In the next lecture, we'll hear from AVA on deep sequence\", 'tokens': [51220, 40425, 613, 3652, 917, 281, 917, 13, 682, 264, 958, 7991, 11, 321, 603, 1568, 490, 316, 20914, 322, 2452, 8310, 51520], 'temperature': 0.0, 'avg_logprob': -0.1319122102525499, 'compression_ratio': 1.6550218340611353, 'no_speech_prob': 0.002357725752517581}, {'id': 613, 'seek': 347108, 'start': 3471.08, 'end': 3478.6, 'text': ' modeling using RNNs and specifically this very exciting new type of model called the transformer', 'tokens': [50364, 15983, 1228, 45702, 45, 82, 293, 4682, 341, 588, 4670, 777, 2010, 295, 2316, 1219, 264, 31782, 50740], 'temperature': 0.0, 'avg_logprob': -0.15477435871706172, 'compression_ratio': 1.4946236559139785, 'no_speech_prob': 0.0019522063666954637}, {'id': 614, 'seek': 347108, 'start': 3478.6, 'end': 3484.12, 'text': \" architecture and attention mechanisms. So maybe let's resume the class in about five minutes after\", 'tokens': [50740, 9482, 293, 3202, 15902, 13, 407, 1310, 718, 311, 15358, 264, 1508, 294, 466, 1732, 2077, 934, 51016], 'temperature': 0.0, 'avg_logprob': -0.15477435871706172, 'compression_ratio': 1.4946236559139785, 'no_speech_prob': 0.0019522063666954637}, {'id': 615, 'seek': 347108, 'start': 3484.12, 'end': 3491.24, 'text': ' we have a chance to swap speakers and thank you so much for all of your attention.', 'tokens': [51016, 321, 362, 257, 2931, 281, 18135, 9518, 293, 1309, 291, 370, 709, 337, 439, 295, 428, 3202, 13, 51372], 'temperature': 0.0, 'avg_logprob': -0.15477435871706172, 'compression_ratio': 1.4946236559139785, 'no_speech_prob': 0.0019522063666954637}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdlpSiqyfMxs",
        "outputId": "65249571-e5d2-43e2-8e97-8f03066b8888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 2,\n",
              " 'seek': 3000,\n",
              " 'start': 31.0,\n",
              " 'end': 37.0,\n",
              " 'text': ' This is a course about justice and we begin with a story.',\n",
              " 'tokens': [50414,\n",
              "  639,\n",
              "  307,\n",
              "  257,\n",
              "  1164,\n",
              "  466,\n",
              "  6118,\n",
              "  293,\n",
              "  321,\n",
              "  1841,\n",
              "  365,\n",
              "  257,\n",
              "  1657,\n",
              "  13,\n",
              "  50714],\n",
              " 'temperature': 0.0,\n",
              " 'avg_logprob': -0.13037579329972415,\n",
              " 'compression_ratio': 1.663716814159292,\n",
              " 'no_speech_prob': 0.6196123957633972}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def store_segments(segments):\n",
        "  texts = []\n",
        "  start_times = []\n",
        "\n",
        "  for segment in segments:\n",
        "    text = segment['text']\n",
        "    start = segment['start']\n",
        "\n",
        "    # Convert the starting time to a datetime object\n",
        "    start_datetime = datetime.fromtimestamp(start)\n",
        "\n",
        "    # Format the starting time as a string in the format \"00:00:00\"\n",
        "    formatted_start_time = start_datetime.strftime('%H:%M:%S')\n",
        "\n",
        "    texts.append(\"\".join(text))\n",
        "    start_times.append(formatted_start_time)\n",
        "\n",
        "  return texts, start_times"
      ],
      "metadata": {
        "id": "G43zeNwUiSkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_segments(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaqzfco_iVOr",
        "outputId": "7cb366e7-66a3-4a59-ef9a-b6dd871fd7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([' Good afternoon, everyone. Thank you all for joining today. My name is Alexander Amini,',\n",
              "  \" and I'll be one of your course organizers this year, along with Ava. And together, we're\",\n",
              "  ' super excited to introduce you all to Introduction to Deep Learning. Now, MIT Inter to Deep Learning',\n",
              "  ' is a really, really fun, exciting and fast-paced program here at MIT. And let me start by just',\n",
              "  \" first of all, giving you a little bit of background into what we do and what you're going to\",\n",
              "  \" learn about this year. So this week of Introduction to Deep Learning, we're going to cover a ton\",\n",
              "  \" of material in just one week. You'll learn the foundations of this really, really fascinating\",\n",
              "  \" and exciting field of deep learning and artificial intelligence. And more importantly, you're\",\n",
              "  ' going to get hands-on experience actually reinforcing what you learn in the lectures as part of',\n",
              "  ' hands-on software labs. Now, over the past decade, AI and deep learning have really had a huge',\n",
              "  ' resurgence and had incredible successes. And a lot of problems that even just a decade ago, we',\n",
              "  \" thought we're not really even solvable in the near future. Now, we're solving with deep learning\",\n",
              "  ' with incredible ease. Now, this past year in particular of 2022 has been an incredible year for',\n",
              "  \" deep learning progress. And I'd like to say that actually this past year in particular has been the\",\n",
              "  ' year of generative deep learning, using deep learning to generate brand new types of data that',\n",
              "  ' have never been seen before and never existed in reality. In fact, I want to start this class by',\n",
              "  ' actually showing you how we started this class several years ago, which was by playing this video',\n",
              "  \" that I'll play in a second. Now, this video actually was an introductory video for the class. It was\",\n",
              "  \" and kind of exemplifies this idea that I'm talking about. So let me just start there and play\",\n",
              "  ' this video first of all. Hi everybody and welcome to MIT, six S-191. We are',\n",
              "  \" conducting course on deep learning to talk here at MIT. We've learned to revolutionize\",\n",
              "  \" things so many things from the bodies, the medicine and everything in the field. You'll learn\",\n",
              "  ' upon the levels of this field and how you can build some of these incredible alternatives.',\n",
              "  \" In fact, this is where our students and video are not real and we're created to be deep learning\",\n",
              "  \" and our visual intelligence. And in this class, you'll learn how to have been honored\",\n",
              "  \" with me today and I hope to be in the way that we're seeing.\",\n",
              "  \" So in case you couldn't tell, this video and its entire audio was actually not real. It was\",\n",
              "  ' synthetically generated by a deep learning algorithm and when we introduced this class a few years',\n",
              "  ' ago, this video was created several years ago. But even several years ago,',\n",
              "  ' when we introduced this and put it on YouTube, when some were viral, people really loved this',\n",
              "  ' video. They were intrigued by how real the video and audio felt and looked entirely generated by',\n",
              "  ' an algorithm, by a computer. And people were shocked with the power and the realism of these',\n",
              "  ' types of approaches and this was a few years ago. Now, fast forward to today and the state of deep',\n",
              "  \" learning today, we have seen deep learning accelerating at a rate faster than we've ever seen\",\n",
              "  ' before. In fact, we can use deep learning now to generate not just images of faces, but generate',\n",
              "  ' full synthetic environments where we can train autonomous vehicles entirely in simulation and',\n",
              "  ' deploy them on full scale vehicles in the real world seamlessly. The videos here you see are',\n",
              "  ' actually from a data driven simulator from neural networks generated called Vista that we actually',\n",
              "  ' built here at MIT and have open sourced to the public. So all of you can actually train and build',\n",
              "  ' the future of autonomy and self-driving cars. And of course, it goes far beyond this as well.',\n",
              "  ' Deep learning can be used to generate content directly from how we speak and the language that',\n",
              "  ' we convey to it from prompts that we say. Deep learning can reason about the prompts in natural',\n",
              "  ' language and English, for example, and then guide and control what is generated according to what',\n",
              "  \" we specify. We've seen examples of where we can generate, for example, things that, again,\",\n",
              "  ' have never existed in reality. We can ask a neural network to generate a photo of an astronaut',\n",
              "  ' writing a horse. And it actually can imagine, hallucinate what this might look like even though,',\n",
              "  \" of course, this photo, not only this photo has never occurred before, but I don't think any photo\",\n",
              "  \" of an astronaut writing a horse has ever occurred before. So there's not really even training data\",\n",
              "  ' that you could go off in this case. And my personal favorite is actually how we can not only build',\n",
              "  ' software that can generate images and videos, but build software that can generate software',\n",
              "  ' as well. We can also have algorithms that can take language prompts, for example, a prompt like',\n",
              "  ' this, write code and TensorFlow to generate or to train a neural network. And not only will it',\n",
              "  ' write the code and create that neural network, but it will have the ability to reason about the',\n",
              "  \" code that it's generated and walk you through. Step by step, explaining the process and procedure\",\n",
              "  ' all the way from the ground up to you so that you can actually learn how to do this process as well.',\n",
              "  ' Now, I think some of these examples really just highlight how far deep learning and these',\n",
              "  ' methods have come in the past six years since we started this course. And you saw that example',\n",
              "  \" just a few years ago from that introductory video. But now we're seeing such incredible advances.\",\n",
              "  ' The most amazing part of this course, in my opinion, is actually that within this one week,',\n",
              "  \" we're going to take you through from the ground up, starting from today, all of the foundational\",\n",
              "  ' building blocks that will allow you to understand and make all of this amazing advances possible.',\n",
              "  \" So with that, hopefully now you're all super excited about what this class will teach. And I want to\",\n",
              "  \" basically now just start by taking a step back and introducing some of these terminologies that I've\",\n",
              "  ' kind of been throwing around so far, the deep learning, artificial intelligence, what do these',\n",
              "  ' things actually mean? So first of all, I want to maybe just take a second to speak a little bit',\n",
              "  ' about intelligence and what intelligence means at its core. So to me, intelligence is simply the',\n",
              "  ' ability to process information such that we can use it to inform some future decision or action',\n",
              "  ' that we take. Now, the field of artificial intelligence is simply the ability for us to build',\n",
              "  ' algorithms, artificial algorithms that can do exactly this, process information to inform some',\n",
              "  ' future decision. Now, machine learning is simply a subset of AI, which focuses specifically on how',\n",
              "  ' we can build a machine to or teach a machine how to do this from some experiences or data, for',\n",
              "  ' example. Now, deep learning goes one step beyond this and is a subset of machine learning, which',\n",
              "  ' focuses explicitly on what are called neural networks and how we can build neural networks that',\n",
              "  ' can extract features in the data. These are basically what you can think of as patterns that',\n",
              "  \" occur within the data so that it can learn to complete these tasks as well. Now, that's exactly what\",\n",
              "  \" this class is really all about at its core. We're going to try and teach you and give you the\",\n",
              "  ' foundational understanding and how we can build and teach computers to learn tasks, many different',\n",
              "  \" types of tasks directly from raw data. And that's really what this class boils down to at its\",\n",
              "  \" most simple form. And we'll provide a very solid foundation for you both on the technical side\",\n",
              "  ' through the lectures, which will happen in two parts throughout the class, the first lecture and',\n",
              "  ' the second lecture, each one about one hour long, followed by a software lab, which will immediately',\n",
              "  ' follow the lectures, which will try to reinforce a lot of what we cover in the technical part of',\n",
              "  ' the class and give you hands-on experience implementing those ideas. So this program is split between',\n",
              "  ' these two pieces, the technical lectures and the software labs. We have several new updates this',\n",
              "  ' year in specific, especially in many of the later lectures. The first lecture will cover the foundations',\n",
              "  \" of deep learning, which is going to be right now. And finally, we'll conclude the course with some\",\n",
              "  ' very exciting guest lectures from both academia and industry who are really leading and driving',\n",
              "  ' forward the state of AI and deep learning. And of course, we have many awesome prizes that go with',\n",
              "  ' all of the software labs and the project competition at the end of the course. So maybe quickly to go',\n",
              "  \" through these each day, like I said, we'll have dedicated software labs that couple with the lectures.\",\n",
              "  \" Starting today with Lab 1, you'll actually build a neural network, keeping with the theme of\",\n",
              "  \" generative AI, you'll build a neural network that can learn, listen to a lot of music and actually\",\n",
              "  ' learn how to generate brand new songs in that genre of music. At the end, at the next level of the',\n",
              "  \" class on Friday, we'll host a project pitch competition where either you individually or as part of\",\n",
              "  \" a group can participate and present an idea, a novel deep learning idea to all of us. It'll be\",\n",
              "  ' roughly three minutes in length. And we will focus not as much because this is a one week program.',\n",
              "  ' We are not going to focus so much on the results of your pitch, but rather the invasion and the idea',\n",
              "  \" and the novelty of what you're trying to propose. The prizes here are quite significant already.\",\n",
              "  ' Where first prize is going to get an Nvidia GPU, which is really a key piece of hardware that is',\n",
              "  ' instrumental. If you want to actually build a deep learning project and train these neural',\n",
              "  ' networks, which can be very large and require a lot of compute, these prizes will give you the compute',\n",
              "  \" to do so. And finally, this year we'll be awarding a grand prize for labs two and three combined,\",\n",
              "  ' which will occur on Tuesday and Wednesday, focused on what I believe is actually solving some of the',\n",
              "  ' most exciting problems in this field of deep learning and how specifically how we can build models',\n",
              "  \" that can be robust, not only accurate, but robust and trustworthy and safe when they're deployed\",\n",
              "  \" as well. And you'll actually get experience developing those types of solutions that can actually\",\n",
              "  ' advance the state of the art and AI. Now, all of these labs that I mentioned and competitions here',\n",
              "  \" are going to be due on Thursday night at 11 p.m. right before the last day of class. And we'll be\",\n",
              "  ' helping you all along the way. This prize or this competition in particular has very significant',\n",
              "  ' prizes. So I encourage all of you to really enter this prize and try to try to give a chance to win',\n",
              "  \" the prize. And of course, like I said, we're going to be helping you all along the way who are\",\n",
              "  ' many available resources throughout this class to help you achieve this. Please post a Piazza if',\n",
              "  ' you have any questions. And of course, this program has an incredible team that you can reach out to',\n",
              "  ' at any point in case you have any issues or questions on the materials. Myself and Ava will be your',\n",
              "  \" two main lectures for the first part of the class. We'll also be hearing, like I said, in the\",\n",
              "  ' later part of the class from some guest lectures who will share some really cutting-edge state-of-the-art',\n",
              "  ' developments and deep learning. And of course, I want to give a huge shout out and thanks to all of',\n",
              "  \" our sponsors who without their support, this program wouldn't have been possible for yet again\",\n",
              "  \" another year. So thank you all. Okay, so now with that, let's really dive into the really fun stuff\",\n",
              "  \" of today's lecture, which is, you know, the technical part. And I think I want to start this part by\",\n",
              "  ' asking all of you and having yourselves ask yourself, you know, having you ask yourselves this',\n",
              "  ' question of, you know, why are all of you here, first of all, why do you care about this topic',\n",
              "  ' in the first place? Now, I think to answer this question, we have to take a step back and think',\n",
              "  ' about, you know, the history of machine learning and what machine learning is and what deep learning',\n",
              "  ' brings to the table on top of machine learning. Now, traditional machine learning algorithms',\n",
              "  ' typically define what are called these set of features in the data. You can think of these as',\n",
              "  ' certain patterns in the data and usually these features are hand engineered. So probably a human',\n",
              "  ' will come into the data set and with a lot of domain knowledge and experience can try to uncover',\n",
              "  ' what these features might be. Now, the key idea of deep learning and this is really central to this',\n",
              "  ' class is that instead of having a human define these features, what if we could have a machine',\n",
              "  ' look at all of this data and actually try to extract and uncover what are the core patterns in',\n",
              "  ' the data so that it can use those when it sees new data to make some decisions. So, for example,',\n",
              "  ' if we wanted to detect faces in an image, a deep neural network algorithm might actually learn',\n",
              "  ' that in order to detect a face, it first has to detect things like edges in the image, lines and',\n",
              "  ' edges. And when you combine those lines and edges, you can actually create compositions of features',\n",
              "  ' like corners and curves, which when you combine those, you can create more high level features,',\n",
              "  ' for example, eyes and noses and ears. And then those are the features that allow you to ultimately',\n",
              "  ' detect what you care about detecting, which is the face. But all of these come from what are called',\n",
              "  ' kind of a hierarchical learning of features. And you can actually see some examples of these.',\n",
              "  \" These are real features learned by a neural network and how they're combined defines this progression\",\n",
              "  ' of information. But in fact, what I just described, this underlying and fundamental building block',\n",
              "  ' of neural networks and deep learning have actually existed for decades. Now, why are we studying',\n",
              "  ' all of this now and today in this class with all this great enthusiasm to learn this, right? Well,',\n",
              "  ' for one, there have been several key advances that have occurred in the past decade. Number one is',\n",
              "  ' that data is so much more pervasive than it has ever been before in our lifetimes. These models',\n",
              "  \" are hungry for more data. And we're living in the age of big data. More data is available to these\",\n",
              "  ' models than ever before and they thrive off of that. Secondly, these algorithms are massively',\n",
              "  \" parallelizable. They require a lot of compute. And we're also at a unique time in history where we\",\n",
              "  ' have the ability to train these extremely large scale algorithms and techniques that have existed',\n",
              "  ' for a very long time. But we can now train them due to the hardware advances that have been made.',\n",
              "  ' And finally, due to open source toolboxes and software platforms like TensorFlow, for example,',\n",
              "  ' which all of you will get a lot of experience on in this class, training and building the code',\n",
              "  ' for these neural networks has never been easier. So from the software point of view as well,',\n",
              "  \" there have been incredible advances to open source the underlying fundamentals of what you're going\",\n",
              "  ' to learn. So let me start now with just building up from the ground up, the fundamental building block',\n",
              "  \" of every single neural network that you're going to learn in this class. And that's going to be\",\n",
              "  ' just a single neuron. And in neural network language, a single neuron is called a perceptron.',\n",
              "  \" So what is a perceptron? A perceptron is, like I said, a single neuron. And it's actually, I'm going\",\n",
              "  \" to say it's very, very simple idea. So I want to make sure that everyone in the audience understands\",\n",
              "  \" exactly what a perceptron is and how it works. So let's start by first defining a perceptron as taking\",\n",
              "  ' as input a set of inputs. So on the left hand side, you can see this perceptron takes M different',\n",
              "  \" inputs, 1 to M. These are the blue circles. We're denoting these inputs as x's.\",\n",
              "  ' Each of these numbers, each of these inputs, is then multiplied by a corresponding weight,',\n",
              "  \" which we can call w. So x1 will be multiplied by w1. And we'll add the result of all of these\",\n",
              "  ' multiplications together. Now we take that single number after the addition and we pass it through',\n",
              "  ' this nonlinear, what we call a nonlinear activation function. And that produces our final output',\n",
              "  ' of the perceptron, which we can call y. Now this is actually not entirely accurate of the',\n",
              "  \" picture of a perceptron. There's one step that I forgot to mention here. So in addition to\",\n",
              "  \" multiplying all of these inputs with their corresponding weights, we're also now going to add\",\n",
              "  \" what's called a bias term. Here denoted as this w0, which is just a scalar weight. And you can\",\n",
              "  \" think of it coming with an input of just 1. So that's going to allow the network to basically shift\",\n",
              "  ' its nonlinear activation function nonlinearly as it sees its inputs.',\n",
              "  ' Now on the right hand side, you can see this diagram mathematically formulated. As a single',\n",
              "  ' equation, we can now rewrite this linear this equation with linear algebra terms of vectors',\n",
              "  ' and dot products. So for example, we can define our entire inputs x1 to xm as large vector x.',\n",
              "  ' That large vector x can be multiplied by or take you a dot, excuse me, matrix multiplied',\n",
              "  ' with our weights w. This again, another vector of our weights w1 to wm.',\n",
              "  ' Taking their dot product not only multiplies them, but it also adds the resulting terms together.',\n",
              "  ' Adding a bias, like we said before, and applying this nonlinearity.',\n",
              "  \" Now you might be wondering what is this nonlinear function? I've mentioned it a few times already.\",\n",
              "  ' Well, I said it is a function that we pass the outputs of the neural network through before we',\n",
              "  \" return it to the next neuron in the pipeline. So one common example of a nonlinear function that's\",\n",
              "  ' very popular in deep neural networks is called the sigmoid function. You can think of this as kind',\n",
              "  ' of a continuous version of a threshold function. It goes from 0 to 1 and it can take us input any',\n",
              "  ' real number on the real number line. And you can see an example of it illustrated on the bottom',\n",
              "  ' right hand. Now, in fact, there are many types of nonlinear activation functions that are popular',\n",
              "  \" in deep neural networks. And here are some common ones. And throughout this presentation, you'll\",\n",
              "  \" actually see some examples of these code snippets on the bottom of the slides where we'll try and\",\n",
              "  \" actually tie in some of what you're learning in the lectures to actual software and how you can\",\n",
              "  ' implement these pieces, which will help you a lot for your software labs explicitly. So the sigmoid',\n",
              "  \" activation on the left is very popular since it's a function that outputs between 0 and 1. So\",\n",
              "  ' especially when you want to deal with probability distributions, for example, this is very important',\n",
              "  ' because probabilities live between 0 and 1. In modern deep neural networks, though, the',\n",
              "  ' relu function, which you can see on the far right hand, is a very popular activation function',\n",
              "  \" because it's piecewise linear. It's extremely efficient to compute, especially when\",\n",
              "  \" computing it's derivatives, right? It's derivatives are constants, except for nonlinear, yet 0.\",\n",
              "  ' Now, I hope actually all of you are probably asking this question to yourself of why do we even',\n",
              "  ' need this nonlinear activation function? It seems like it kind of just complicates this whole',\n",
              "  \" picture when we didn't really need it in the first place. And I want to just spend a moment\",\n",
              "  ' on answering this because the point of a nonlinear activation function is, of course, number one is',\n",
              "  ' to introduce nonlinearities to our data, right? If we think about our data, almost all data that we',\n",
              "  ' care about, all real world data is highly nonlinear. Now, this is important because if we want to be',\n",
              "  ' able to deal with those types of data sets, we need models that are also nonlinear so they can capture',\n",
              "  ' those same types of patterns. So imagine I told you to separate, for example, I gave you this',\n",
              "  ' data set, red points from green points and I asked you to try and separate those two types of',\n",
              "  ' data points. Now, you might think that this is easy, but what if I could only, if I told you,',\n",
              "  ' that you could only use a single line to do so? Well, now it becomes a very complicated problem.',\n",
              "  \" In fact, you can't really solve it effectively with a single line. And in fact, if you introduce\",\n",
              "  \" nonlinear activation functions to your solution, that's exactly what allows you to, you know,\",\n",
              "  ' deal with these types of problems. Nonlinear activation functions allow you to deal with',\n",
              "  \" nonlinear types of data. Now, and that's what exactly makes neural networks so powerful at their core.\",\n",
              "  \" So let's understand this maybe with a very simple example, walking through this diagram of a\",\n",
              "  ' perceptron one more time. Imagine I give you this trained neural network with weights now, not',\n",
              "  \" W1, W2. I'm going to actually give you numbers at these locations, right? So the trained weights,\",\n",
              "  ' W0 will be 1 and W will be a vector of 3 and negative 2. So this neural network has two inputs,',\n",
              "  ' like we said before, it has input x1 and has input x2. If we want to get the output of it,',\n",
              "  ' this is also the main thing I want all of you to take away from this lecture today is that',\n",
              "  ' to get the output of a perceptron, there are three steps we need to take, right? From this stage,',\n",
              "  ' we first compute the multiplication of our inputs with our weights.',\n",
              "  \" Sorry, yeah, multiply them together, add their result and compute a nonlinearity. It's these three\",\n",
              "  ' steps that define the forward propagation of information through a perceptron.',\n",
              "  \" So let's take a look at how that exactly works, right? So if we plug in these numbers to those\",\n",
              "  ' equations, we can see that everything inside of our nonlinearity, here the nonlinearity is G,',\n",
              "  ' right? That function G, which could be a sigmoid, we saw a previous slide. That component inside',\n",
              "  ' our nonlinearity is in fact just a two-dimensional line. It has two inputs, and if we consider the',\n",
              "  ' space of all of the possible inputs that this neural network could see, we can actually plot this',\n",
              "  ' on a decision boundary, right? We can plot this two-dimensional line as a decision boundary,',\n",
              "  ' as a plane separating these two components of our space. In fact, not only is it a single plane,',\n",
              "  \" there's a directionality component, depending on which side of the plane that we live on.\",\n",
              "  ' If we see an input, for example, here, negative one, two, we actually know that it lives on one',\n",
              "  ' side of the plane, and it will have a certain type of output. In this case, that output is going to be',\n",
              "  ' positive, right? Because in this case, when we plug those components into our equation,',\n",
              "  \" we'll get a positive number that passes through the nonlinearity component, and that gets propagated\",\n",
              "  \" through as well. Of course, if you're on the other side of the space, you're going to have the\",\n",
              "  ' opposite result, right? That thresholding function is going to essentially live at this decision',\n",
              "  ' boundary. Depending on which side of the space you live on, that thresholding function, that',\n",
              "  ' sigmoid function, is going to then control how you move to one side or the other.',\n",
              "  ' Now, in this particular example, this is very convenient, because we can actually visualize,',\n",
              "  \" and I can draw this exact full space for you on this slide. It's only a two-dimensional space,\",\n",
              "  \" so it's very easy for us to visualize. But, of course, for almost all problems that we care about,\",\n",
              "  ' our data points are not going to be two-dimensional. If you think about an image,',\n",
              "  ' the dimensionality of an image is going to be the number of pixels that you have in the image.',\n",
              "  ' So these are going to be thousands of dimensions, millions of dimensions, or even more.',\n",
              "  \" And then, drawing these types of plots, like you see here, is simply not feasible. We can't always\",\n",
              "  ' do this, but hopefully this gives you some intuition to understand, kind of, as we build up into',\n",
              "  \" more complex models. So now that we have an idea of the perceptron, let's see how we can actually\",\n",
              "  ' take this single neuron and start to build it up into something more complicated, a full neural',\n",
              "  \" network, and build a model from that. So let's revisit, again, this previous diagram of the perceptron.\",\n",
              "  ' If, again, just to reiterate one more time, this core piece of information that I want all of',\n",
              "  ' you to take away from this class is how a perceptron works and how it propagates information to',\n",
              "  ' its decision. There are three steps. First is the dot product, second is the bias, and third is',\n",
              "  ' the non-miniarity. And you keep repeating this process for every single perceptron in your neural',\n",
              "  \" network. Let's simplify the diagram a little bit. I'll get rid of the weights. And you can assume that\",\n",
              "  \" every line here, now basically has an associated weight scalar that's associated with it. Every line\",\n",
              "  \" also has, it corresponds to the input that's coming in. It has a weight that's coming in also at\",\n",
              "  \" on the line itself. And I've also removed the bias just for sake of simplicity, but it's still there.\",\n",
              "  \" So now the result is that Z, which let's call that the result of our dot product plus the bias,\",\n",
              "  \" is going, and that's what we pass into our non-linear function, that piece is going to be applied\",\n",
              "  ' to that activation function. Now the final output here is simply going to be G, which is our',\n",
              "  ' activation function of Z, right? Z is going to be basically what you can think of the state of',\n",
              "  \" this neuron. It's the result of that dot product plus bias. Now if we want to define and build up\",\n",
              "  ' a multi-layered output neural network, if we want two outputs to this function, for example,',\n",
              "  \" it's a very simple procedure. We just have now two neurons, two perceptrons. Each perceptron\",\n",
              "  ' will control the output for its associated piece, right? So now we have two outputs. Each one is a',\n",
              "  ' normal perceptron. It takes all of the inputs, so they both take the same inputs, but amazingly,',\n",
              "  ' now with this mathematical understanding, we can start to build our first neural network entirely',\n",
              "  ' from scratch. So what does that look like? So we can start by firstly initializing these two',\n",
              "  ' components. The first component that we saw was the weight matrix, excuse me, the weight vector.',\n",
              "  \" It's a vector of weights, in this case. And the second component is the bias vector that we're\",\n",
              "  ' going to multiply with the dot product of all of our inputs by our weights, right? So the only',\n",
              "  \" remaining step now after we've defined these parameters of our layer is to now define, you know,\",\n",
              "  \" how this forward propagation of information works. And that's exactly those three main components\",\n",
              "  \" that I've been stressing to you. So we can create this call function to do exactly that, to define\",\n",
              "  \" this forward propagation of information. And the story here is exactly the same as we've been\",\n",
              "  ' seeing it, right? Matrix multiply our inputs with our weights, right? Add a bias and then apply a',\n",
              "  ' non-linearity and return the result, right? And that literally, this code will run. This will define',\n",
              "  ' a full neural network layer that you can then take like this. And of course, actually,',\n",
              "  \" luckily for all of you, all of that code, which wasn't much code, that's been abstracted away by\",\n",
              "  ' these libraries like TensorFlow, you can simply call functions like this, which will actually,',\n",
              "  \" you know, replicate exactly that piece of code. So you don't need to necessarily copy all of that\",\n",
              "  ' code down. You just, you can just call it. And with that understanding, you know, we just saw',\n",
              "  ' how you could build a single layer. But of course, now you can actually start to think about how you',\n",
              "  ' can stack these layers as well. So since we now have this transformation, essentially, from our inputs,',\n",
              "  ' to a hidden output, you can think of this as basically how we can define some way of transforming',\n",
              "  ' those inputs, right, into some new dimensional space, right? Perhaps closer to the value that we',\n",
              "  ' want to predict. And that transformation is going to be eventually learned to know how to transform',\n",
              "  \" those inputs into our desired outputs. And we'll get to that later. But for now, the piece that I want\",\n",
              "  ' to really focus on is if we have these more complex neural networks, I want to really distill down',\n",
              "  \" that this is nothing more complex than what we've already seen. If we focus on just one neuron in\",\n",
              "  \" this diagram, take a, here, for example, Z2, right? Z2 is this neuron that's highlighted in the\",\n",
              "  \" middle layer. It's just the same perceptron that we've been seeing so far in this class. It was\",\n",
              "  \" it's output is obtained by taking a dot product, adding a bias, and then applying that non-linearity\",\n",
              "  ' between all of its inputs. If we look at a different node, for example, Z3, which is the one right below',\n",
              "  \" it, it's the exact same story again. It sees all the same inputs, but it has a different set of\",\n",
              "  \" weight matrix that it's going to apply to those inputs. So we'll have a different output. But the\",\n",
              "  \" mathematically equations are exactly the same. So from now on, I'm just going to kind of simplify\",\n",
              "  ' all of these lines and diagrams just to show these icons in the middle just to demonstrate that',\n",
              "  ' these means everything is going to fully connect it to everything and defined by those mathematical',\n",
              "  \" equations that we've been covering. But there's no extra complexity in these models from what you've\",\n",
              "  ' already seen. Now, if you want to stack these types of solutions on top of each other, these',\n",
              "  ' layers on top of each other, you can not only define one layer very easily, but you can actually',\n",
              "  ' create what are called sequential models. These sequential models, you can define one layer after',\n",
              "  ' another, and they define basically the forward propagation of information, not just from the neuron',\n",
              "  ' level, but now from the layer level. Every layer will be fully connected to the next layer,',\n",
              "  ' and the inputs of the secondary layer will be all of the outputs of the prior layer.',\n",
              "  ' Now, of course, if you want to create a very deep neural network, all the deep neural network is,',\n",
              "  \" is we just keep stacking these layers on top of each other. There's nothing else to this story.\",\n",
              "  \" That's really as simple as it is. Once, so these layers are basically all they are, it's just layers\",\n",
              "  ' where the final output is computed, right, by going deeper and deeper into this progression',\n",
              "  ' of different layers, right, and you just keep stacking them until you get to the last layer,',\n",
              "  \" which is your output layer. It's your final prediction that you want to output.\",\n",
              "  ' Right, we can create a deep neural network to do all of this by stacking these layers and',\n",
              "  \" creating these more hierarchical models, like we saw very early in the beginning of today's lecture.\",\n",
              "  ' One where the final output is really computed by, you know, just going deeper and deeper into this',\n",
              "  \" system. Okay, so that's awesome. So we've now seen how we can go from a single neuron\",\n",
              "  ' to a layer to all the way to a deep neural network, right, building off of these foundational',\n",
              "  \" principles. Let's take a look at how exactly we can use these, you know, principles that we've\",\n",
              "  ' just discussed to solve a very real problem that I think all of you are probably very concerned about',\n",
              "  ' this morning when you when you woke up. So that problem is how we can build a neural network',\n",
              "  ' to answer this question, which is, will I pass this class and if I will or will I not?',\n",
              "  \" So to answer this question, let's see if we can train a neural network to solve this problem.\",\n",
              "  \" Okay, so to do this, let's start with a very simple neural network, right, we'll train this model\",\n",
              "  ' with two inputs, just two inputs. One input is going to be the number of lectures that you attend',\n",
              "  ' over the course of this one week. And the second input is going to be how many hours that you spend',\n",
              "  \" on your final project or your competition. Okay, so what we're going to do is firstly go out and\",\n",
              "  \" collect a lot of data from all of the past years that we've taught this course. And we can plot all\",\n",
              "  \" of this data because it's only two input space, we can plot this data on a two-dimensional\",\n",
              "  ' feature space, right. We can actually look at all of the students before you that have passed the',\n",
              "  ' class and failed the class and see where they lived in this space for the amount of hours that they',\n",
              "  \" spent, the number of lectures that they've attended and so on. Green points are the people who have\",\n",
              "  \" passed, read, or those who have failed. Now, and here's you, right, you're right here. Four or\",\n",
              "  \" five is your coordinate space. You fall right there and you've attended four lectures. You've spent\",\n",
              "  ' five hours on your final project. We want to build a neural network to answer the question of,',\n",
              "  \" will you pass the class or will you fail the class? So let's do it. We have two inputs. One is four,\",\n",
              "  \" one is five. These are two numbers. We can feed them through a neural network that we've just seen\",\n",
              "  ' how we can build that. And we feed that into a single layered neural network. Three hidden units',\n",
              "  ' in this example, but we could make it larger if we want it to be more expressive and more powerful.',\n",
              "  \" And we see here that the probability of you passing those classes point one. It's pretty\",\n",
              "  \" physical. So why would this be the case, right? What did we do wrong? Because I don't think it's\",\n",
              "  ' correct, right? When we looked at this space, it looked like actually you were a good candidate to',\n",
              "  \" pass the class. But why is the neural network saying that there's only 10% likelihood that you should pass?\",\n",
              "  ' Does anyone have any ideas? Exactly. So this neural network is just like it was just born,',\n",
              "  \" right? It has no information about the world or this class. It doesn't know what four and five\",\n",
              "  ' mean or what the notion of passing or failing means, right? So exactly right. This neural network has',\n",
              "  \" not been trained. You can think of it kind of as a baby. It hasn't learned anything yet. So our\",\n",
              "  ' job firstly is to train it. And part of that understanding is we first need to tell the neural',\n",
              "  ' network when it makes mistakes, right? So mathematically, we should now think about how we can answer this',\n",
              "  ' question, which is, did my neural network make a mistake? And if it made a mistake, how can I tell',\n",
              "  ' it? How big of a mistake it was so that the next time it sees this data point, can it do better?',\n",
              "  ' Minimize that mistake. So in neural network language, those mistakes are called losses,',\n",
              "  \" and specifically you want to define what's called a loss function, which is going to take as input\",\n",
              "  ' your prediction and the true prediction, right? And how far away your prediction is from the',\n",
              "  \" true prediction tells you how big of a loss there is, right? So for example, let's say we want to build\",\n",
              "  ' a neural network to do classification of, or sorry, actually even before that, I want to maybe give',\n",
              "  ' you some terminology. So there are multiple different ways of saying the same thing in neural networks',\n",
              "  ' and deep learning. So what I just described as a loss function is also commonly referred to as an',\n",
              "  \" objective function in empirical risk, a cost function. These are all exactly the same thing. They're\",\n",
              "  ' all the way for us to train the neural network, to teach the neural network when it makes mistakes.',\n",
              "  ' And what we really ultimately want to do is over the course of an entire data set,',\n",
              "  ' not just one data point of mistakes, we want to say over the entire data set,',\n",
              "  ' we want to minimize all of the mistakes on average that this neural network makes.',\n",
              "  ' So if we look at the problem, like I said, of binary classification, will I pass this class or',\n",
              "  \" will I not? There's a yes or no answer, that means binary classification. Now we can use what's\",\n",
              "  \" called a loss function of the softmax cross entropy loss. And for those of you who aren't familiar,\",\n",
              "  ' this notion of cross entropy is actually developed here at MIT by Shod Klanit. Shod, excuse me, yes.',\n",
              "  ' Claude Shannon, who is a visionary, he did his masters here over 50 years ago, he introduced this',\n",
              "  ' notion of cross entropy and that was pivotal in the ability for us to train these types of neural',\n",
              "  \" networks even now into the future. So let's start by, instead of predicting a binary cross entropy\",\n",
              "  \" output, what if we wanted to predict a final grade of your class score? For example, that's no\",\n",
              "  \" longer binary output, yes or no, it's actually a continuous variable, it's the grade, let's say out\",\n",
              "  ' of 100 points, what is the value of your score in the class project? For this type of loss, we can',\n",
              "  \" use what's called a mean squared error loss. You can think of this literally as just subtracting your\",\n",
              "  \" predicted grade from the true grade and minimizing that distance apart. So I think now we're ready to\",\n",
              "  ' really put all of this information together and tackle this problem of training a neural network,',\n",
              "  ' right, to not just identify how erroneous it is, how large its loss is, but more importantly,',\n",
              "  ' minimize that loss as a function of seeing all of this training data that is observed.',\n",
              "  ' So we know that we want to find this neural network, like we mentioned before, that minimizes',\n",
              "  ' this empirical risk or this empirical loss averaged across our entire data set. Now this means that',\n",
              "  \" we want to find mathematically these W's, right, that minimize J of W. J of W is our loss function,\",\n",
              "  ' averaged over our entire data set, and W is our weight. So we want to find the set of weights',\n",
              "  ' that on average is going to give us the smallest loss as possible. Now remember that W here is just',\n",
              "  \" a list. Basically it's just a group of all of the weights in our neural network. You may have hundreds\",\n",
              "  \" of the weights and a very, very small neural network, or in today's neural networks, you may have\",\n",
              "  ' billions or trillions of weights, and you want to find what is the value of every single one of these',\n",
              "  ' weights that is going to result in the smallest loss as possible. Now how can you do this?',\n",
              "  ' Remember that our loss function, J of W, is just a function of our weights, right? So for any',\n",
              "  ' instantiation of our weights, we can compute a scalar value of how erroneous would our neural',\n",
              "  \" network be for this instantiation of our weights. So let's try and visualize, for example, in a very\",\n",
              "  ' simple example of a two-dimensional space where we have only two weights, extremely simple neural',\n",
              "  ' network here, very small, two-weight neural network, and we want to find what are the optimal weights',\n",
              "  ' that would train this neural network. We can plot basically the loss, how erroneous the neural',\n",
              "  ' network is for every single instantiation of these two weights, right? This is a huge space,',\n",
              "  \" it's an infinite space, but still we can try to, we can have a function that evaluates at every\",\n",
              "  ' point in this space. Now what we ultimately want to do is, again, we want to find which set of',\n",
              "  \" W's will give us the smallest loss possible. That means basically the lowest point on this\",\n",
              "  \" landscape that you can see here, where is the W's that bring us to that lowest point?\",\n",
              "  ' The way that we do this is actually just by firstly starting at a random place, we have no idea',\n",
              "  \" where to start, so pick a random place to start in this space, and let's start there. At this\",\n",
              "  \" location, let's evaluate our neural network. We can compute the loss at this specific location,\",\n",
              "  ' and on top of that we can actually compute how the loss is changing. We can compute the gradient',\n",
              "  ' of the loss because our loss function is a continuous function, right? So we can actually compute',\n",
              "  ' derivatives of our function across the space of our weights, and the gradient tells us the direction',\n",
              "  ' of the highest point, right? So from where we stand, the gradient tells us where we should go',\n",
              "  \" to increase our loss. Now of course we don't want to increase our loss, we want to decrease our loss,\",\n",
              "  ' so we negate our gradient, and we take a step in the opposite direction of the gradient. That brings',\n",
              "  ' us one step closer to the bottom of the landscape, and we just keep repeating this process, right?',\n",
              "  ' Over and over again, we evaluate the neural network at this new location, compute its gradient,',\n",
              "  ' and step in that new direction. We keep traversing this landscape until we converge to the minimum.',\n",
              "  ' We can really summarize this algorithm, which is known formally as gradient descent, right? So',\n",
              "  ' gradient descent simply can be written like this. We initialize all of our weights, right? This can',\n",
              "  ' be two weights, like you saw in the previous example, it can be billions of weights, like in',\n",
              "  ' real neural networks. We compute this gradient of the partial derivative of our loss with respect',\n",
              "  ' to the weights, and then we can update our weights in the opposite direction of this gradient.',\n",
              "  ' So essentially we just take this small amount, small step, you can think of it, which here is denoted',\n",
              "  \" as eta, and we refer to this small step, right? This is commonly referred to as what's known as\",\n",
              "  \" the learning rate. It's like how much we want to trust that gradient and step in the direction of\",\n",
              "  \" that gradient. We'll talk more about this later, but just to give you some sense of code, this algorithm\",\n",
              "  ' is very well translatable to real code as well. For every line on the pseudo code you can see on the',\n",
              "  ' left, you can see corresponding real code on the right that is runnable and directly implementable',\n",
              "  \" by all of you in your labs. But now let's take a look specifically at this term here. This is the\",\n",
              "  ' gradient. We touched very briefly on this in the visual example. This explains, like I said,',\n",
              "  ' how the loss is changing as a function of the weights, right? So as the weights move around,',\n",
              "  ' will my loss increase or decrease, and that will tell the neural network if it needs to move the',\n",
              "  ' weights in a certain direction or not. But I never actually told you how to compute this, right?',\n",
              "  \" And I think that's an extremely important part because if you don't know that, then you can't\",\n",
              "  \" well, you can't train your neural network, right? This is a critical part of training neural networks,\",\n",
              "  \" and that process of computing this line, this gradient line, is known as back propagation. So let's\",\n",
              "  \" do a very quick intro to back propagation and how it works. So again, let's start with the\",\n",
              "  ' simplest neural network in existence. This neural network has one input, one output, and only one',\n",
              "  ' neuron, right? This is as simple as it gets. We want to compute the gradient of our loss with respect',\n",
              "  \" to our weight. In this case, let's compute it with respect to W2, the second weight.\",\n",
              "  ' So this derivative is going to tell us how much a small change in this weight will affect our loss.',\n",
              "  ' If a small change, if we change our weight a little bit in one direction, will it increase our loss',\n",
              "  ' or decrease our loss? So to compute that, we can write out this derivative. We can start with',\n",
              "  ' applying the chain rule backwards from the loss function through the output. Specifically,',\n",
              "  ' what we can do is we can actually just decompose this derivative into two components. The first',\n",
              "  ' component is the derivative of our loss with respect to our output multiplied by the derivative of our',\n",
              "  ' output with respect to W2, right? This is just a standard instantiation of the chain rule with',\n",
              "  \" this original derivative that we had on the left-hand side. Let's suppose we want to compute the\",\n",
              "  ' gradients of the weight before that, which in this case are not W1, but W, excuse me, not W2, but',\n",
              "  ' W1. Well, all we do is replace W2 with W1 and that chain rule still holds, right? That same',\n",
              "  ' equation holds, but now you can see on the red component, that last component of the chain rule,',\n",
              "  \" we have to, once again, recursively apply one more chain rule because that's again another\",\n",
              "  \" derivative that we can't directly evaluate. We can expand that once more with another\",\n",
              "  ' instantiation of the chain rule. Now, all of these components, we can directly propagate these',\n",
              "  \" gradients through the hidden units in our neural network all the way back to the weight that we're\",\n",
              "  ' interested in in this example, right? So we first computed the derivative with respect to W2,',\n",
              "  \" then we can back propagate that and use that information also with W1. That's why we really\",\n",
              "  ' call it back propagation because this process occurs from the output all the way back to the input.',\n",
              "  ' Now, we repeat this process essentially many, many times over the course of training by propagating',\n",
              "  ' these gradients over and over again through the network all the way from the output to the inputs',\n",
              "  ' to determine for every single weight answering this question, which is how much does a small change',\n",
              "  ' in these weights affect our loss function if it increases, it reduces, then how we can use that',\n",
              "  \" improve the loss ultimately because that's our final goal in this class.\",\n",
              "  \" So that's the back propagation algorithm. That's the core of training neural networks. In theory,\",\n",
              "  \" it's very simple. It's really just an instantiation of the chain rule. But let's touch on some insights\",\n",
              "  ' that make training neural networks actually extremely complicated in practice even though the',\n",
              "  ' algorithm of back propagation is simple and many decades old. In practice, though, optimization of',\n",
              "  ' neural networks looks something like this. It looks nothing like that picture that I showed you before.',\n",
              "  ' There are ways that we can visualize very large deep neural networks and you can think of the',\n",
              "  ' landscape of these models looking like something like this. This is an illustration from a paper that',\n",
              "  ' came out several years ago where they tried to actually visualize the landscape of very, very deep',\n",
              "  \" neural networks. That's what this landscape actually looks like. That's what you're trying to\",\n",
              "  ' deal with and find the minimum in this space. You can imagine the challenges that come with that.',\n",
              "  \" To cover the challenges, let's first think of and recall that update equation defined in gradient\",\n",
              "  \" descent. I didn't talk too much about this parameter, ADA, but now let's spend a bit of time\",\n",
              "  ' thinking about this. This is called the learning rate, like we saw before. It determines basically how',\n",
              "  ' big of a step we need to take in the direction of our gradient and every single iteration of',\n",
              "  ' back propagation. In practice, even setting the learning rate can be very challenging. You as',\n",
              "  ' the designer of the neural network have to set this value, this learning rate, and how do you pick',\n",
              "  ' this value? That can actually be quite difficult. It has really large consequences when building a',\n",
              "  \" neural network. For example, if we set the learning rate too low, then we learn very slowly. Let's\",\n",
              "  ' assume we start on the right-hand side here at that initial guess. If our learning rate is not',\n",
              "  \" large enough, not only do we converge slowly, we actually don't even converge to the global\",\n",
              "  ' minimum, because we kind of get stuck in a local minimum. What if we set our learning rate too high?',\n",
              "  ' What can actually happen is we overshoot and we can actually start to diverge from the solution.',\n",
              "  \" The gradients can actually explode. Very bad things happen and then the neural network doesn't\",\n",
              "  \" train. That's also not good. In reality, there's a very happy medium between setting a too small,\",\n",
              "  ' setting a too large, where you set it just large enough to kind of overshoot some of these local',\n",
              "  ' minima, put you into a reasonable part of the search space, where then you can actually converge',\n",
              "  ' on the solutions that you care most about. But actually, how do you set these learning rates in',\n",
              "  ' practice? How do you pick what is the ideal learning rate? One option, and this is actually a very',\n",
              "  ' common option in practices to simply try out a bunch of learning rates and see what works the',\n",
              "  \" best. Let's say a whole grade of different learning rates and train all of these neural networks,\",\n",
              "  ' see which one works the best. But I think we can do something a lot smarter. What are some more',\n",
              "  ' intelligent ways that we could do this instead of exhaustively trying out a whole bunch of different',\n",
              "  ' learning rates? Can we design a learning rate algorithm that actually adapts to our neural network',\n",
              "  \" and adapts to its landscape so that it's a bit more intelligent than that previous idea?\",\n",
              "  ' So this really ultimately means that the learning rate, the speed at which the algorithm is trusting',\n",
              "  ' the gradients that it sees, is going to depend on how large the gradient is in that location',\n",
              "  \" and how fast we're learning. How many other options, and sorry, and many other options that we might\",\n",
              "  \" have as part of training and neural networks, right? So it's not only how quickly we're learning,\",\n",
              "  \" you may judge it on many different factors in the learning landscape. In fact, we've all been\",\n",
              "  \" these different algorithms that I'm talking about, these adaptive learning rate algorithms have been\",\n",
              "  ' very widely studied in practice. There is a very thriving community in the deep learning research',\n",
              "  ' community that focuses on developing and designing new algorithms for learning rate adaptation and',\n",
              "  \" faster optimization of large neural networks like these. And during your labs, you'll actually get the\",\n",
              "  ' opportunity to not only try out a lot of these different adaptive algorithms, which you can see here,',\n",
              "  ' but also try to uncover what are kind of the patterns and benefits of one versus the other. And',\n",
              "  \" that's going to be something that I think you'll find very insightful as part of your labs.\",\n",
              "  \" So another key component of your labs that you'll see is how you can actually put all of this\",\n",
              "  \" information that we've covered today into a single picture that looks roughly something like this,\",\n",
              "  \" which defines your model at the first, at the top here. That's where you define your model,\",\n",
              "  ' where you talked about this in the beginning part of the lecture. For every piece in your model,',\n",
              "  \" you're now going to need to define this optimizer, which we've just talked about. This optimizer is\",\n",
              "  ' defined together with a learning rate, right? How quickly you want to optimize your loss landscape,',\n",
              "  \" and over many loops, you're going to pass over all of the examples in your data set,\",\n",
              "  \" and observe essentially how to improve your network. That's the gradient, and then actually\",\n",
              "  ' improve the network in those directions. And keep doing that over and over and over again,',\n",
              "  ' until eventually your neural network converges to some sort of solution.',\n",
              "  ' So I want to very quickly, briefly, in the remaining time that we have, continue to talk about',\n",
              "  ' tips for training these neural networks in practice, and focus on this very powerful idea of',\n",
              "  ' batching your data into what are called mini batches of smaller pieces of data.',\n",
              "  \" To do this, let's revisit that gradient descent algorithm, right? So here, this gradient that we\",\n",
              "  \" talked about before is actually extraordinarily computationally expensive to compute, because it's\",\n",
              "  ' computed as a summation across all of the pieces in your data set, right? And in most real life,',\n",
              "  \" for real world problems, it's simply not feasible to compute a gradient over your entire data set.\",\n",
              "  ' Data sets are just too large these days. So there are some alternatives, right? What are the',\n",
              "  ' alternatives? Instead of computing the derivative for the gradients across your entire data set,',\n",
              "  ' what if you instead computed the gradient over just a single example in your data set? Just one',\n",
              "  \" example. Well, of course, this estimate of your gradient is going to be exactly that. It's an\",\n",
              "  \" estimate. It's going to be very noisy. It may roughly reflect the trends of your entire data set,\",\n",
              "  \" but because it's a very, it's only one example. In fact, if your entire data set, it may be very noisy.\",\n",
              "  \" Right? Well, the advantage of this, though, is that it's much faster to compute, obviously,\",\n",
              "  \" the gradient over a single example, because it's one example. So computationally, this has huge\",\n",
              "  \" advantages, but the downside is that it's extremely stochastic, right? That's the reason why this\",\n",
              "  \" algorithm is not called gradient descent. It's called stochastic gradient descent. Now,\",\n",
              "  \" now what's the middle ground? Right? Instead of computing it with respect to one example in your\",\n",
              "  \" data set, what if we computed what's called a mini batch of examples, a small batch of examples\",\n",
              "  \" that we can compute the gradients over? And when we take these gradients, they're still computationally\",\n",
              "  \" efficient to compute because it's a mini batch. It's not too large. Maybe we're talking on the order\",\n",
              "  \" of tens or hundreds of examples in our data set, but more importantly, because we've expanded from\",\n",
              "  ' a single example to maybe a hundred examples, the stochasticity is significantly reduced, and the',\n",
              "  \" accuracy of our gradients is much improved. So normally, we're thinking of batch sizes, mini batch\",\n",
              "  ' sizes roughly on the order of 100 data points, tens or hundreds of data points. This is much faster,',\n",
              "  ' obviously, to compute the gradient descent and much more accurate to compute compared to stochastic',\n",
              "  ' gradient descent, which is that single point example. So this increase in gradient accuracy',\n",
              "  ' allows us to essentially converge to our solution much quicker than it could have been possible',\n",
              "  ' in practice due to gradient descent limitations. It also means that we can increase our learning',\n",
              "  \" rate because we can trust each of those gradients much more efficiently. Right? We're now averaging\",\n",
              "  \" over a batch. It's going to be much more accurate than the stochastic version, so we can increase\",\n",
              "  ' that learning rate and actually learn faster as well. This allows us to also massively parallelize',\n",
              "  ' this entire algorithm and computation. Right? We can split up batches onto separate workers and',\n",
              "  ' achieve even more significant speed ups of this entire problem using GPUs. The last topic that I',\n",
              "  \" very, very briefly want to cover in today's lecture is this topic of overfitting. Right? When we\",\n",
              "  \" are optimizing a neural network with stochastic gradient descent, we have this challenge of what's\",\n",
              "  ' called overfitting. Overfitting, I, looks like this roughly, right? So on the left hand side,',\n",
              "  \" we want to build a neural network, or let's say in general, we want to build a machine learning\",\n",
              "  \" model that can accurately describe some patterns in our data, but remember, we're ultimately,\",\n",
              "  \" we don't want to describe the patterns in our training data. Ideally, we want to define the\",\n",
              "  \" patterns in our test data. Of course, we don't observe test data. We only observe training data.\",\n",
              "  ' So we have this challenge of extracting patterns from training data and hoping that they',\n",
              "  ' generalize to our test data. So set in one different way, we want to build models that can learn',\n",
              "  ' representations from our training data that can still generalize even when we show them brand new',\n",
              "  ' unseen pieces of test data. So assume that you want to build a line that can describe or find',\n",
              "  ' the patterns in these points that you can see on the slide. If you have a very simple neural',\n",
              "  ' network, which is just a single line, straight line, you can describe this data sub optimally,',\n",
              "  \" because the data here is nonlinear. You're not going to accurately capture all of the nuances\",\n",
              "  \" and subtleties in this data set. That's on the left hand side. If you move to the right hand side,\",\n",
              "  \" you can see a much more complicated model, but here you're actually overexpressive. You're too\",\n",
              "  \" expressive and you're capturing kind of the nuances, the spurious nuances in your training data\",\n",
              "  ' that are actually not representative of your test data. Ideally, you want to end up with the',\n",
              "  \" model in the middle, which is basically the middle ground, right? It's not too complex and it's not\",\n",
              "  ' too simple. It still gives you what you want to perform well and even when you give it brand new',\n",
              "  \" data. So to address this problem, let's briefly talk about what's called regularization. Regularization\",\n",
              "  ' is a technique that you can introduce to your training pipeline to discourage complex models',\n",
              "  \" from being learned. Now, as we've seen before, this is really critical because neural networks are\",\n",
              "  ' extremely large models. They are extremely prone to overfitting, right? So regularization, having',\n",
              "  ' techniques for regularization has extreme implications towards the success of neural networks and',\n",
              "  ' having them generalize beyond training data far into our testing domain. The most popular',\n",
              "  ' technique for regularization in deep learning is called dropout and the idea of dropout is actually',\n",
              "  \" very simple. It's let's revisit it by drawing this picture of deep neural networks that we saw\",\n",
              "  \" earlier in today's lecture. In dropout, during training, we essentially randomly select some subset\",\n",
              "  ' of the neurons in this neural network and we try to prune them out with some random probability.',\n",
              "  ' So for example, we can select this subset of neurons. We can randomly select them with a probability',\n",
              "  ' of 50 percent and with that probability, we randomly turn them off or on different iterations of',\n",
              "  ' our training. So this is essentially forcing the neural network to learn you can think of an',\n",
              "  \" ensemble of different models. On every iteration, it's going to be exposed to kind of a different\",\n",
              "  ' model internally than the one it had on the last iteration. So it has to learn how to build',\n",
              "  \" internal pathways to process the same information and it can't rely on information that it\",\n",
              "  ' learned on previous iterations. So it forces it to kind of capture some deeper meaning within the',\n",
              "  ' pathways of the neural network and this can be extremely powerful because the number one, it',\n",
              "  \" lowers the capacity of the neural network significantly. You're lowering it by roughly 50 percent\",\n",
              "  ' in this example. But also because it makes it easier to train because the number of weights that',\n",
              "  \" have gradients in this case is also reduced. So it's actually much faster to train them as well.\",\n",
              "  ' Now, like I mentioned, on every iteration, we randomly drop out a different set of neurons,',\n",
              "  ' right? And that helps the data generalize better. And the second regularization techniques,',\n",
              "  ' which is actually a very broad regularization technique far beyond neural networks,',\n",
              "  ' is simply called early stopping. Now, we know the definition of overfitting is simply when our',\n",
              "  \" model starts to represent basically the training data more than the testing data. That's really\",\n",
              "  ' what overfitting comes down to its core. If we set aside some of the training data to use separately,',\n",
              "  \" that we don't train on it, we can use a kind of a testing data set, synthetic testing data set in\",\n",
              "  ' some ways. We can monitor how our network is learning on this unseen portion of data. So for',\n",
              "  ' example, we can over the course of training, we can basically plot the performance of our network',\n",
              "  ' on both the training set as well as our held out test set. And as the network is trained,',\n",
              "  \" we're going to see that, first of all, these both decrease, but there's going to be a point\",\n",
              "  ' where the loss plateaus and starts to increase. The training loss will actually start to increase.',\n",
              "  \" This is exactly the point where you start to overfit, right? Because now you're starting to have,\",\n",
              "  \" sorry, that was the test loss. The test loss actually starts to increase because now you're\",\n",
              "  ' starting to overfit on your training data. This pattern basically continues for the rest of training.',\n",
              "  ' And this is the point that I want you to focus on, right? This middle point is where we need to',\n",
              "  ' stop training because after this point, assuming that this test set is a valid representation',\n",
              "  ' of the true test set, this is the place where the accuracy of the model will only get worse,',\n",
              "  ' right? So this is where we would want to early stop our model and regularize the performance.',\n",
              "  \" And we can see that stopping any time before this point is also not good. We're going to produce\",\n",
              "  \" an underfit model where we could have had a better model on the test data, but it's this tradeoff,\",\n",
              "  \" right? You can't stop too late and you can't stop too early as well.\",\n",
              "  \" So I'll conclude this lecture by just summarizing these three key points that we've covered in\",\n",
              "  \" today's lecture so far. So we first covered these fundamental building blocks of all neural networks,\",\n",
              "  \" which is the single neuron, the perceptron. We've built these up into larger neural layers and then\",\n",
              "  \" from their neural networks and deep neural networks. We've learned how we can train these,\",\n",
              "  \" apply them to data sets, back propagate through them, and we've seen some tips and tricks for\",\n",
              "  \" optimizing these systems end to end. In the next lecture, we'll hear from AVA on deep sequence\",\n",
              "  ' modeling using RNNs and specifically this very exciting new type of model called the transformer',\n",
              "  \" architecture and attention mechanisms. So maybe let's resume the class in about five minutes after\",\n",
              "  ' we have a chance to swap speakers and thank you so much for all of your attention.'],\n",
              " ['00:00:00',\n",
              "  '00:00:14',\n",
              "  '00:00:19',\n",
              "  '00:00:26',\n",
              "  '00:00:32',\n",
              "  '00:00:37',\n",
              "  '00:00:42',\n",
              "  '00:00:47',\n",
              "  '00:00:53',\n",
              "  '00:00:59',\n",
              "  '00:01:05',\n",
              "  '00:01:11',\n",
              "  '00:01:15',\n",
              "  '00:01:23',\n",
              "  '00:01:28',\n",
              "  '00:01:33',\n",
              "  '00:01:39',\n",
              "  '00:01:44',\n",
              "  '00:01:51',\n",
              "  '00:01:56',\n",
              "  '00:02:08',\n",
              "  '00:02:15',\n",
              "  '00:02:24',\n",
              "  '00:02:31',\n",
              "  '00:02:41',\n",
              "  '00:02:50',\n",
              "  '00:02:53',\n",
              "  '00:03:03',\n",
              "  '00:03:08',\n",
              "  '00:03:13',\n",
              "  '00:03:18',\n",
              "  '00:03:26',\n",
              "  '00:03:31',\n",
              "  '00:03:37',\n",
              "  '00:03:45',\n",
              "  '00:03:52',\n",
              "  '00:03:57',\n",
              "  '00:04:02',\n",
              "  '00:04:07',\n",
              "  '00:04:13',\n",
              "  '00:04:18',\n",
              "  '00:04:24',\n",
              "  '00:04:30',\n",
              "  '00:04:36',\n",
              "  '00:04:43',\n",
              "  '00:04:48',\n",
              "  '00:04:54',\n",
              "  '00:04:59',\n",
              "  '00:05:03',\n",
              "  '00:05:08',\n",
              "  '00:05:14',\n",
              "  '00:05:19',\n",
              "  '00:05:25',\n",
              "  '00:05:31',\n",
              "  '00:05:35',\n",
              "  '00:05:41',\n",
              "  '00:05:47',\n",
              "  '00:05:51',\n",
              "  '00:05:56',\n",
              "  '00:06:01',\n",
              "  '00:06:06',\n",
              "  '00:06:13',\n",
              "  '00:06:19',\n",
              "  '00:06:25',\n",
              "  '00:06:29',\n",
              "  '00:06:36',\n",
              "  '00:06:42',\n",
              "  '00:06:48',\n",
              "  '00:06:54',\n",
              "  '00:06:59',\n",
              "  '00:07:05',\n",
              "  '00:07:13',\n",
              "  '00:07:19',\n",
              "  '00:07:23',\n",
              "  '00:07:27',\n",
              "  '00:07:34',\n",
              "  '00:07:38',\n",
              "  '00:07:44',\n",
              "  '00:07:50',\n",
              "  '00:07:56',\n",
              "  '00:08:00',\n",
              "  '00:08:05',\n",
              "  '00:08:11',\n",
              "  '00:08:18',\n",
              "  '00:08:23',\n",
              "  '00:08:29',\n",
              "  '00:08:34',\n",
              "  '00:08:40',\n",
              "  '00:08:47',\n",
              "  '00:08:52',\n",
              "  '00:08:58',\n",
              "  '00:09:03',\n",
              "  '00:09:08',\n",
              "  '00:09:14',\n",
              "  '00:09:20',\n",
              "  '00:09:27',\n",
              "  '00:09:34',\n",
              "  '00:09:39',\n",
              "  '00:09:44',\n",
              "  '00:09:49',\n",
              "  '00:09:54',\n",
              "  '00:09:58',\n",
              "  '00:10:04',\n",
              "  '00:10:08',\n",
              "  '00:10:15',\n",
              "  '00:10:21',\n",
              "  '00:10:25',\n",
              "  '00:10:32',\n",
              "  '00:10:38',\n",
              "  '00:10:44',\n",
              "  '00:10:50',\n",
              "  '00:10:55',\n",
              "  '00:11:01',\n",
              "  '00:11:06',\n",
              "  '00:11:12',\n",
              "  '00:11:17',\n",
              "  '00:11:22',\n",
              "  '00:11:26',\n",
              "  '00:11:31',\n",
              "  '00:11:38',\n",
              "  '00:11:44',\n",
              "  '00:11:50',\n",
              "  '00:11:55',\n",
              "  '00:12:00',\n",
              "  '00:12:05',\n",
              "  '00:12:10',\n",
              "  '00:12:15',\n",
              "  '00:12:20',\n",
              "  '00:12:25',\n",
              "  '00:12:30',\n",
              "  '00:12:36',\n",
              "  '00:12:41',\n",
              "  '00:12:46',\n",
              "  '00:12:52',\n",
              "  '00:12:57',\n",
              "  '00:13:02',\n",
              "  '00:13:08',\n",
              "  '00:13:13',\n",
              "  '00:13:17',\n",
              "  '00:13:22',\n",
              "  '00:13:27',\n",
              "  '00:13:33',\n",
              "  '00:13:39',\n",
              "  '00:13:45',\n",
              "  '00:13:51',\n",
              "  '00:13:57',\n",
              "  '00:14:04',\n",
              "  '00:14:10',\n",
              "  '00:14:16',\n",
              "  '00:14:22',\n",
              "  '00:14:26',\n",
              "  '00:14:31',\n",
              "  '00:14:37',\n",
              "  '00:14:41',\n",
              "  '00:14:47',\n",
              "  '00:14:53',\n",
              "  '00:14:58',\n",
              "  '00:15:04',\n",
              "  '00:15:12',\n",
              "  '00:15:16',\n",
              "  '00:15:23',\n",
              "  '00:15:29',\n",
              "  '00:15:36',\n",
              "  '00:15:42',\n",
              "  '00:15:49',\n",
              "  '00:15:55',\n",
              "  '00:16:00',\n",
              "  '00:16:07',\n",
              "  '00:16:13',\n",
              "  '00:16:17',\n",
              "  '00:16:23',\n",
              "  '00:16:29',\n",
              "  '00:16:36',\n",
              "  '00:16:42',\n",
              "  '00:16:48',\n",
              "  '00:16:57',\n",
              "  '00:17:03',\n",
              "  '00:17:10',\n",
              "  '00:17:16',\n",
              "  '00:17:22',\n",
              "  '00:17:27',\n",
              "  '00:17:34',\n",
              "  '00:17:40',\n",
              "  '00:17:45',\n",
              "  '00:17:52',\n",
              "  '00:17:57',\n",
              "  '00:18:02',\n",
              "  '00:18:06',\n",
              "  '00:18:11',\n",
              "  '00:18:16',\n",
              "  '00:18:21',\n",
              "  '00:18:26',\n",
              "  '00:18:30',\n",
              "  '00:18:35',\n",
              "  '00:18:39',\n",
              "  '00:18:43',\n",
              "  '00:18:51',\n",
              "  '00:18:55',\n",
              "  '00:18:59',\n",
              "  '00:19:04',\n",
              "  '00:19:09',\n",
              "  '00:19:15',\n",
              "  '00:19:21',\n",
              "  '00:19:25',\n",
              "  '00:19:29',\n",
              "  '00:19:33',\n",
              "  '00:19:38',\n",
              "  '00:19:43',\n",
              "  '00:19:49',\n",
              "  '00:19:54',\n",
              "  '00:19:59',\n",
              "  '00:20:06',\n",
              "  '00:20:10',\n",
              "  '00:20:15',\n",
              "  '00:20:21',\n",
              "  '00:20:29',\n",
              "  '00:20:34',\n",
              "  '00:20:38',\n",
              "  '00:20:43',\n",
              "  '00:20:48',\n",
              "  '00:20:55',\n",
              "  '00:21:01',\n",
              "  '00:21:06',\n",
              "  '00:21:12',\n",
              "  '00:21:19',\n",
              "  '00:21:25',\n",
              "  '00:21:31',\n",
              "  '00:21:38',\n",
              "  '00:21:45',\n",
              "  '00:21:49',\n",
              "  '00:21:55',\n",
              "  '00:22:00',\n",
              "  '00:22:05',\n",
              "  '00:22:11',\n",
              "  '00:22:16',\n",
              "  '00:22:21',\n",
              "  '00:22:25',\n",
              "  '00:22:32',\n",
              "  '00:22:37',\n",
              "  '00:22:42',\n",
              "  '00:22:47',\n",
              "  '00:22:52',\n",
              "  '00:22:56',\n",
              "  '00:23:01',\n",
              "  '00:23:07',\n",
              "  '00:23:12',\n",
              "  '00:23:18',\n",
              "  '00:23:22',\n",
              "  '00:23:28',\n",
              "  '00:23:33',\n",
              "  '00:23:39',\n",
              "  '00:23:45',\n",
              "  '00:23:49',\n",
              "  '00:23:56',\n",
              "  '00:24:01',\n",
              "  '00:24:08',\n",
              "  '00:24:14',\n",
              "  '00:24:21',\n",
              "  '00:24:27',\n",
              "  '00:24:34',\n",
              "  '00:24:39',\n",
              "  '00:24:46',\n",
              "  '00:24:51',\n",
              "  '00:24:55',\n",
              "  '00:25:01',\n",
              "  '00:25:07',\n",
              "  '00:25:12',\n",
              "  '00:25:18',\n",
              "  '00:25:23',\n",
              "  '00:25:30',\n",
              "  '00:25:36',\n",
              "  '00:25:42',\n",
              "  '00:25:47',\n",
              "  '00:25:52',\n",
              "  '00:25:57',\n",
              "  '00:26:05',\n",
              "  '00:26:11',\n",
              "  '00:26:18',\n",
              "  '00:26:22',\n",
              "  '00:26:27',\n",
              "  '00:26:32',\n",
              "  '00:26:39',\n",
              "  '00:26:44',\n",
              "  '00:26:51',\n",
              "  '00:26:59',\n",
              "  '00:27:06',\n",
              "  '00:27:11',\n",
              "  '00:27:16',\n",
              "  '00:27:21',\n",
              "  '00:27:26',\n",
              "  '00:27:33',\n",
              "  '00:27:39',\n",
              "  '00:27:44',\n",
              "  '00:27:49',\n",
              "  '00:27:53',\n",
              "  '00:27:58',\n",
              "  '00:28:03',\n",
              "  '00:28:08',\n",
              "  '00:28:13',\n",
              "  '00:28:18',\n",
              "  '00:28:24',\n",
              "  '00:28:28',\n",
              "  '00:28:33',\n",
              "  '00:28:38',\n",
              "  '00:28:43',\n",
              "  '00:28:50',\n",
              "  '00:28:54',\n",
              "  '00:28:58',\n",
              "  '00:29:04',\n",
              "  '00:29:10',\n",
              "  '00:29:14',\n",
              "  '00:29:18',\n",
              "  '00:29:22',\n",
              "  '00:29:27',\n",
              "  '00:29:32',\n",
              "  '00:29:40',\n",
              "  '00:29:45',\n",
              "  '00:29:52',\n",
              "  '00:29:58',\n",
              "  '00:30:04',\n",
              "  '00:30:10',\n",
              "  '00:30:15',\n",
              "  '00:30:20',\n",
              "  '00:30:25',\n",
              "  '00:30:30',\n",
              "  '00:30:37',\n",
              "  '00:30:41',\n",
              "  '00:30:45',\n",
              "  '00:30:50',\n",
              "  '00:30:55',\n",
              "  '00:30:59',\n",
              "  '00:31:05',\n",
              "  '00:31:10',\n",
              "  '00:31:15',\n",
              "  '00:31:21',\n",
              "  '00:31:25',\n",
              "  '00:31:31',\n",
              "  '00:31:36',\n",
              "  '00:31:40',\n",
              "  '00:31:45',\n",
              "  '00:31:50',\n",
              "  '00:31:56',\n",
              "  '00:32:08',\n",
              "  '00:32:14',\n",
              "  '00:32:21',\n",
              "  '00:32:26',\n",
              "  '00:32:31',\n",
              "  '00:32:36',\n",
              "  '00:32:42',\n",
              "  '00:32:47',\n",
              "  '00:32:54',\n",
              "  '00:32:59',\n",
              "  '00:33:05',\n",
              "  '00:33:13',\n",
              "  '00:33:19',\n",
              "  '00:33:25',\n",
              "  '00:33:30',\n",
              "  '00:33:35',\n",
              "  '00:33:41',\n",
              "  '00:33:46',\n",
              "  '00:33:50',\n",
              "  '00:33:56',\n",
              "  '00:34:01',\n",
              "  '00:34:07',\n",
              "  '00:34:12',\n",
              "  '00:34:21',\n",
              "  '00:34:28',\n",
              "  '00:34:35',\n",
              "  '00:34:43',\n",
              "  '00:34:49',\n",
              "  '00:34:54',\n",
              "  '00:35:01',\n",
              "  '00:35:05',\n",
              "  '00:35:14',\n",
              "  '00:35:20',\n",
              "  '00:35:27',\n",
              "  '00:35:32',\n",
              "  '00:35:38',\n",
              "  '00:35:45',\n",
              "  '00:35:53',\n",
              "  '00:35:58',\n",
              "  '00:36:07',\n",
              "  '00:36:13',\n",
              "  '00:36:17',\n",
              "  '00:36:22',\n",
              "  '00:36:27',\n",
              "  '00:36:34',\n",
              "  '00:36:41',\n",
              "  '00:36:47',\n",
              "  '00:36:54',\n",
              "  '00:36:59',\n",
              "  '00:37:06',\n",
              "  '00:37:12',\n",
              "  '00:37:17',\n",
              "  '00:37:25',\n",
              "  '00:37:31',\n",
              "  '00:37:38',\n",
              "  '00:37:44',\n",
              "  '00:37:49',\n",
              "  '00:37:54',\n",
              "  '00:38:00',\n",
              "  '00:38:05',\n",
              "  '00:38:12',\n",
              "  '00:38:18',\n",
              "  '00:38:23',\n",
              "  '00:38:28',\n",
              "  '00:38:34',\n",
              "  '00:38:39',\n",
              "  '00:38:47',\n",
              "  '00:38:52',\n",
              "  '00:38:57',\n",
              "  '00:39:02',\n",
              "  '00:39:10',\n",
              "  '00:39:15',\n",
              "  '00:39:21',\n",
              "  '00:39:29',\n",
              "  '00:39:33',\n",
              "  '00:39:40',\n",
              "  '00:39:44',\n",
              "  '00:39:49',\n",
              "  '00:39:55',\n",
              "  '00:40:00',\n",
              "  '00:40:05',\n",
              "  '00:40:10',\n",
              "  '00:40:15',\n",
              "  '00:40:19',\n",
              "  '00:40:24',\n",
              "  '00:40:30',\n",
              "  '00:40:37',\n",
              "  '00:40:42',\n",
              "  '00:40:48',\n",
              "  '00:40:54',\n",
              "  '00:41:01',\n",
              "  '00:41:05',\n",
              "  '00:41:11',\n",
              "  '00:41:17',\n",
              "  '00:41:22',\n",
              "  '00:41:27',\n",
              "  '00:41:35',\n",
              "  '00:41:41',\n",
              "  '00:41:47',\n",
              "  '00:41:53',\n",
              "  '00:41:59',\n",
              "  '00:42:04',\n",
              "  '00:42:08',\n",
              "  '00:42:14',\n",
              "  '00:42:19',\n",
              "  '00:42:24',\n",
              "  '00:42:28',\n",
              "  '00:42:33',\n",
              "  '00:42:40',\n",
              "  '00:42:45',\n",
              "  '00:42:51',\n",
              "  '00:42:56',\n",
              "  '00:43:02',\n",
              "  '00:43:08',\n",
              "  '00:43:16',\n",
              "  '00:43:21',\n",
              "  '00:43:29',\n",
              "  '00:43:33',\n",
              "  '00:43:38',\n",
              "  '00:43:43',\n",
              "  '00:43:47',\n",
              "  '00:43:51',\n",
              "  '00:43:57',\n",
              "  '00:44:03',\n",
              "  '00:44:09',\n",
              "  '00:44:14',\n",
              "  '00:44:19',\n",
              "  '00:44:25',\n",
              "  '00:44:30',\n",
              "  '00:44:36',\n",
              "  '00:44:44',\n",
              "  '00:44:48',\n",
              "  '00:44:53',\n",
              "  '00:45:00',\n",
              "  '00:45:05',\n",
              "  '00:45:10',\n",
              "  '00:45:15',\n",
              "  '00:45:21',\n",
              "  '00:45:26',\n",
              "  '00:45:31',\n",
              "  '00:45:37',\n",
              "  '00:45:41',\n",
              "  '00:45:47',\n",
              "  '00:45:53',\n",
              "  '00:45:57',\n",
              "  '00:46:03',\n",
              "  '00:46:09',\n",
              "  '00:46:16',\n",
              "  '00:46:23',\n",
              "  '00:46:29',\n",
              "  '00:46:34',\n",
              "  '00:46:41',\n",
              "  '00:46:46',\n",
              "  '00:46:52',\n",
              "  '00:46:59',\n",
              "  '00:47:05',\n",
              "  '00:47:10',\n",
              "  '00:47:15',\n",
              "  '00:47:21',\n",
              "  '00:47:25',\n",
              "  '00:47:30',\n",
              "  '00:47:34',\n",
              "  '00:47:39',\n",
              "  '00:47:44',\n",
              "  '00:47:49',\n",
              "  '00:47:54',\n",
              "  '00:47:59',\n",
              "  '00:48:03',\n",
              "  '00:48:09',\n",
              "  '00:48:14',\n",
              "  '00:48:21',\n",
              "  '00:48:28',\n",
              "  '00:48:33',\n",
              "  '00:48:39',\n",
              "  '00:48:46',\n",
              "  '00:48:52',\n",
              "  '00:48:58',\n",
              "  '00:49:04',\n",
              "  '00:49:10',\n",
              "  '00:49:16',\n",
              "  '00:49:21',\n",
              "  '00:49:27',\n",
              "  '00:49:34',\n",
              "  '00:49:39',\n",
              "  '00:49:44',\n",
              "  '00:49:49',\n",
              "  '00:49:53',\n",
              "  '00:49:59',\n",
              "  '00:50:05',\n",
              "  '00:50:10',\n",
              "  '00:50:18',\n",
              "  '00:50:23',\n",
              "  '00:50:29',\n",
              "  '00:50:35',\n",
              "  '00:50:40',\n",
              "  '00:50:48',\n",
              "  '00:50:53',\n",
              "  '00:50:58',\n",
              "  '00:51:04',\n",
              "  '00:51:08',\n",
              "  '00:51:15',\n",
              "  '00:51:21',\n",
              "  '00:51:28',\n",
              "  '00:51:34',\n",
              "  '00:51:41',\n",
              "  '00:51:47',\n",
              "  '00:51:51',\n",
              "  '00:51:58',\n",
              "  '00:52:02',\n",
              "  '00:52:07',\n",
              "  '00:52:11',\n",
              "  '00:52:17',\n",
              "  '00:52:22',\n",
              "  '00:52:29',\n",
              "  '00:52:34',\n",
              "  '00:52:43',\n",
              "  '00:52:47',\n",
              "  '00:52:52',\n",
              "  '00:52:57',\n",
              "  '00:53:03',\n",
              "  '00:53:09',\n",
              "  '00:53:13',\n",
              "  '00:53:19',\n",
              "  '00:53:25',\n",
              "  '00:53:31',\n",
              "  '00:53:37',\n",
              "  '00:53:43',\n",
              "  '00:53:49',\n",
              "  '00:53:54',\n",
              "  '00:54:00',\n",
              "  '00:54:05',\n",
              "  '00:54:11',\n",
              "  '00:54:18',\n",
              "  '00:54:23',\n",
              "  '00:54:30',\n",
              "  '00:54:37',\n",
              "  '00:54:43',\n",
              "  '00:54:47',\n",
              "  '00:54:53',\n",
              "  '00:54:58',\n",
              "  '00:55:02',\n",
              "  '00:55:08',\n",
              "  '00:55:14',\n",
              "  '00:55:19',\n",
              "  '00:55:26',\n",
              "  '00:55:31',\n",
              "  '00:55:35',\n",
              "  '00:55:43',\n",
              "  '00:55:49',\n",
              "  '00:55:54',\n",
              "  '00:56:01',\n",
              "  '00:56:07',\n",
              "  '00:56:13',\n",
              "  '00:56:18',\n",
              "  '00:56:22',\n",
              "  '00:56:28',\n",
              "  '00:56:33',\n",
              "  '00:56:36',\n",
              "  '00:56:42',\n",
              "  '00:56:47',\n",
              "  '00:56:53',\n",
              "  '00:56:57',\n",
              "  '00:57:02',\n",
              "  '00:57:07',\n",
              "  '00:57:12',\n",
              "  '00:57:17',\n",
              "  '00:57:22',\n",
              "  '00:57:27',\n",
              "  '00:57:34',\n",
              "  '00:57:38',\n",
              "  '00:57:45',\n",
              "  '00:57:51',\n",
              "  '00:57:58',\n",
              "  '00:58:04'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts, start_times = store_segments(res)"
      ],
      "metadata": {
        "id": "-ZYECiqNgZ8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "qBBaaahmqCWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "DLAEaZSYqInM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-gpu"
      ],
      "metadata": {
        "id": "AbdW-kuUqM2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.chains import VectorDBQAWithSourcesChain\n",
        "from langchain import OpenAI\n",
        "import openai\n",
        "import faiss"
      ],
      "metadata": {
        "id": "lqQNXEvAqT62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "vgN9CeSaqhUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
        "docs = []\n",
        "metadatas = []\n",
        "for i, d in enumerate(texts):\n",
        "    splits = text_splitter.split_text(d)\n",
        "    docs.extend(splits)\n",
        "    metadatas.extend([{\"source\": start_times[i]}] * len(splits))\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "tjMXHVwRqqTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = FAISS.from_texts(docs, embeddings, metadatas=metadatas)\n",
        "faiss.write_index(store.index, \"docs.index\")"
      ],
      "metadata": {
        "id": "iFri6vBTqutR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain"
      ],
      "metadata": {
        "id": "bUOIpXCFYg_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = VectorDBQAWithSourcesChain.from_llm(llm=OpenAI(temperature=0.9), vectorstore=store)"
      ],
      "metadata": {
        "id": "dZObHNoxqx5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890b4648-f59c-468a-d7aa-a30224ff1ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/qa_with_sources/vector_db.py:67: UserWarning: `VectorDBQAWithSourcesChain` is deprecated - please use `from langchain.chains import RetrievalQAWithSourcesChain`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain({\"question\": \"What is Backpropagation?\"})"
      ],
      "metadata": {
        "id": "j9i9whWUYlJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Answer: {result['answer']}  Sources: {result['sources']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tKd-k8aa_xZ",
        "outputId": "70718e65-e99c-466f-edc4-89f56e2fbcd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  Backpropagation is an algorithm used for training neural networks by propagating errors from the output back to the input.\n",
            "  Sources: 00:42:28, 00:43:02, 00:43:21\n"
          ]
        }
      ]
    }
  ]
}